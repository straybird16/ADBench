{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-step Guidence on How to Install and Use ADBench**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install ADBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T16:06:57.104897Z",
     "start_time": "2023-07-19T16:06:57.092930Z"
    }
   },
   "outputs": [],
   "source": [
    "# download datasets in ADBench from the remote github repo\n",
    "from adbench.myutils import Utils\n",
    "utils = Utils()\n",
    "# we recommend jihulab for China mainland user and github otherwise\n",
    "#utils.download_datasets(repo='github')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run ADBench "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:33:13.848925Z",
     "start_time": "2023-07-17T15:33:13.833498Z"
    }
   },
   "source": [
    "## Run ADBench experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T14:44:03.627289Z",
     "start_time": "2023-07-19T14:26:41.551958Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nParams:\\nsuffix: file name suffix;\\n\\nparallel: running either 'unsupervise', 'semi-supervise', or 'supervise' (AD) algorithms,\\ncorresponding to the Angle I: Availability of Ground Truth Labels (Supervision);\\n\\nrealistic_synthetic_mode: testing on 'local', 'global', 'dependency', and 'cluster' anomalies, \\ncorresponding to the Angle II: Types of Anomalies;\\n\\nnoise type: evaluating algorithms on 'duplicated_anomalies', 'irrelevant_features' and 'label_contamination',\\ncorresponding to the Angle III: Model Robustness with Noisy and Corrupted Data.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adbench.run import RunPipeline\n",
    "\n",
    "'''\n",
    "Params:\n",
    "suffix: file name suffix;\n",
    "\n",
    "parallel: running either 'unsupervise', 'semi-supervise', or 'supervise' (AD) algorithms,\n",
    "corresponding to the Angle I: Availability of Ground Truth Labels (Supervision);\n",
    "\n",
    "realistic_synthetic_mode: testing on 'local', 'global', 'dependency', and 'cluster' anomalies, \n",
    "corresponding to the Angle II: Types of Anomalies;\n",
    "\n",
    "noise type: evaluating algorithms on 'duplicated_anomalies', 'irrelevant_features' and 'label_contamination',\n",
    "corresponding to the Angle III: Model Robustness with Noisy and Corrupted Data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your customized algorithm on ADBench datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized model on ADBench's datasets\n",
    "from adbench.run import RunPipeline\n",
    "from adbench.baseline.SimpleAE.run import SimpleAE\n",
    "from adbench.baseline.VAE.run import VAE\n",
    "from adbench.baseline.DOCAE.run import DOCAE\n",
    "from adbench.baseline.AADOCAE.run import AADOCAE\n",
    "from adbench.baseline.CVStack.run import CVStack\n",
    "from adbench.baseline.SS.run import SS\n",
    "\n",
    "# notice that you should specify the corresponding category of your customized AD algorithm\n",
    "# for example, here we use Logistic Regression as customized clf, which belongs to the supervised algorithm\n",
    "# for your own algorithm, you can realize the same usage as other baselines by modifying the fit.py, model.py, and run.py files in the adbench/baseline/Customized\n",
    "# pipeline = RunPipeline(suffix='ADBench', parallel='unsupervise', realistic_synthetic_mode=None, noise_type=None, num_seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ADBench algos on all datasets with every kinds of synthetic anomalies (unsupervised) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_type = 'duplicated_anomalies'\n",
    "#noise_type = 'irrelevant_features'\n",
    "for mode in [\n",
    "                #None,\n",
    "                'dependency',\n",
    "                'cluster',\n",
    "                None,\n",
    "                'local',\n",
    "                'global',\n",
    "                #'dependency',\n",
    "            ]:\n",
    "    pipeline = RunPipeline(suffix='Unsupervised_models', parallel='unsupervise', realistic_synthetic_mode=mode, noise_type=noise_type, num_seed=3)\n",
    "    # 'duplicated_anomalies', 'irrelevant_features' and 'label_contamination'\n",
    "    results = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 97, 'Anomalies Ratio(%)': 0.97}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 116, 'Anomalies Ratio(%)': 1.16}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 95, 'Anomalies Ratio(%)': 0.95}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 578, 'Anomalies Ratio(%)': 5.78}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 597, 'Anomalies Ratio(%)': 5.97}\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 13, 'Anomalies Ratio(%)': 0.13}\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 34, 'Anomalies Ratio(%)': 3.4}\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 170, 'Anomalies Ratio(%)': 17.0}\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 139, 'Anomalies Ratio(%)': 13.9}\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 46, 'Anomalies Ratio(%)': 0.46}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 35, 'Anomalies Ratio(%)': 0.35}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 34, 'Anomalies Ratio(%)': 0.34}\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 369, 'Anomalies Ratio(%)': 36.9}\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 374, 'Anomalies Ratio(%)': 37.4}\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 349, 'Anomalies Ratio(%)': 34.9}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 315, 'Anomalies Ratio(%)': 3.15}\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 334, 'Anomalies Ratio(%)': 3.34}\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 332, 'Anomalies Ratio(%)': 3.32}\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 38, 'Anomalies Ratio(%)': 3.8}\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 43, 'Anomalies Ratio(%)': 4.3}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3548, 'Anomalies Ratio(%)': 35.48}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3533, 'Anomalies Ratio(%)': 35.33}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3500, 'Anomalies Ratio(%)': 35.0}\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 230, 'Anomalies Ratio(%)': 2.3}\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 241, 'Anomalies Ratio(%)': 2.41}\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 227, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 367, 'Anomalies Ratio(%)': 36.7}\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 346, 'Anomalies Ratio(%)': 34.6}\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 669, 'Anomalies Ratio(%)': 6.69}\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 697, 'Anomalies Ratio(%)': 6.97}\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 714, 'Anomalies Ratio(%)': 7.14}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2081, 'Anomalies Ratio(%)': 20.81}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2082, 'Anomalies Ratio(%)': 20.82}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2066, 'Anomalies Ratio(%)': 20.66}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 5, 'Anomalies Ratio(%)': 0.05}\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 90, 'Anomalies Ratio(%)': 9.0}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 95, 'Anomalies Ratio(%)': 9.5}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 100, 'Anomalies Ratio(%)': 10.0}\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 123, 'Anomalies Ratio(%)': 12.3}\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 133, 'Anomalies Ratio(%)': 13.3}\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 252, 'Anomalies Ratio(%)': 2.52}\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 246, 'Anomalies Ratio(%)': 2.46}\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 256, 'Anomalies Ratio(%)': 2.56}\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 35, 'Anomalies Ratio(%)': 3.5}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 45, 'Anomalies Ratio(%)': 4.5}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 48, 'Anomalies Ratio(%)': 4.8}\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 26, 'Anomalies Ratio(%)': 2.6}\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 29, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 27, 'Anomalies Ratio(%)': 2.7}\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 75, 'Anomalies Ratio(%)': 7.5}\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 80, 'Anomalies Ratio(%)': 8.0}\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 88, 'Anomalies Ratio(%)': 8.8}\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 239, 'Anomalies Ratio(%)': 23.9}\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 224, 'Anomalies Ratio(%)': 22.4}\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 225, 'Anomalies Ratio(%)': 22.5}\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 348, 'Anomalies Ratio(%)': 34.8}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 336, 'Anomalies Ratio(%)': 33.6}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 364, 'Anomalies Ratio(%)': 36.4}\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1134, 'Anomalies Ratio(%)': 11.34}\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1129, 'Anomalies Ratio(%)': 11.29}\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1129, 'Anomalies Ratio(%)': 11.29}\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 222, 'Anomalies Ratio(%)': 2.22}\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 238, 'Anomalies Ratio(%)': 2.38}\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 216, 'Anomalies Ratio(%)': 2.16}\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 628, 'Anomalies Ratio(%)': 6.28}\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 652, 'Anomalies Ratio(%)': 6.52}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 215, 'Anomalies Ratio(%)': 21.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 215, 'Anomalies Ratio(%)': 21.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 198, 'Anomalies Ratio(%)': 19.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 251, 'Anomalies Ratio(%)': 25.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 242, 'Anomalies Ratio(%)': 24.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 242, 'Anomalies Ratio(%)': 24.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 304, 'Anomalies Ratio(%)': 30.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 29.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 254, 'Anomalies Ratio(%)': 25.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 227, 'Anomalies Ratio(%)': 22.7}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 213, 'Anomalies Ratio(%)': 21.3}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 180, 'Anomalies Ratio(%)': 18.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 174, 'Anomalies Ratio(%)': 17.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 160, 'Anomalies Ratio(%)': 16.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 142, 'Anomalies Ratio(%)': 14.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 259, 'Anomalies Ratio(%)': 25.9}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 273, 'Anomalies Ratio(%)': 27.3}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 246, 'Anomalies Ratio(%)': 24.6}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 284, 'Anomalies Ratio(%)': 28.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 283, 'Anomalies Ratio(%)': 28.3}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 281, 'Anomalies Ratio(%)': 28.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 314, 'Anomalies Ratio(%)': 31.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 300, 'Anomalies Ratio(%)': 30.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 326, 'Anomalies Ratio(%)': 32.6}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 240, 'Anomalies Ratio(%)': 24.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 244, 'Anomalies Ratio(%)': 24.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 266, 'Anomalies Ratio(%)': 26.6}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 227, 'Anomalies Ratio(%)': 22.7}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 231, 'Anomalies Ratio(%)': 23.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 252, 'Anomalies Ratio(%)': 25.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 271, 'Anomalies Ratio(%)': 27.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 285, 'Anomalies Ratio(%)': 28.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 288, 'Anomalies Ratio(%)': 28.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 115, 'Anomalies Ratio(%)': 11.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 135, 'Anomalies Ratio(%)': 13.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 114, 'Anomalies Ratio(%)': 11.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 184, 'Anomalies Ratio(%)': 18.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 189, 'Anomalies Ratio(%)': 18.9}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 175, 'Anomalies Ratio(%)': 17.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 305, 'Anomalies Ratio(%)': 30.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 304, 'Anomalies Ratio(%)': 30.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 320, 'Anomalies Ratio(%)': 32.0}\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 40, 'Anomalies Ratio(%)': 4.0}\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 178, 'Anomalies Ratio(%)': 8.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.035803\n",
      "epoch : 1000/5000, loss = 0.029260\n",
      "epoch : 2000/5000, loss = 0.024170\n",
      "epoch : 3000/5000, loss = 0.013771\n",
      "epoch : 4000/5000, loss = 0.013370\n",
      "epoch : 5000/5000, loss = 0.011093\n",
      "self.error_median=0.0048, self.error_range=0.0081, self.latent_error_median_=0.0009, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 68.078895\n",
      "epoch : 1000/5000, loss = 0.004044\n",
      "epoch : 2000/5000, loss = 0.003081\n",
      "epoch : 3000/5000, loss = 0.002947\n",
      "epoch : 4000/5000, loss = 0.003017\n",
      "epoch : 5000/5000, loss = 0.002719\n",
      "self.error_median=0.0090, self.error_range=0.0183, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6132968068122864; loose: 0.2600677013397217\n",
      "Test data rec score: tight: 3.5295162200927734; loose: 1.6647491455078125\n",
      "Test data average rec_m: 2.4228198528289795; average compct_m: -0.09833044558763504\n",
      "Dependency: 0.48762041330337524; Cluster: 0.1612301915884018; Local: 0.16375423967838287; Global: 0.18739517033100128;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:45, 105.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('37_Stamps', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8703735642095823, 'aucpr': 0.4272509346460309}, fitting time: 104.6012351512909, inference time: 0.008975744247436523\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 195, 'Anomalies Ratio(%)': 9.75}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.804744\n",
      "epoch : 1000/5000, loss = 0.021236\n",
      "epoch : 2000/5000, loss = 0.013389\n",
      "epoch : 3000/5000, loss = 0.014296\n",
      "epoch : 4000/5000, loss = 0.011001\n",
      "epoch : 5000/5000, loss = 0.008346\n",
      "self.error_median=0.0037, self.error_range=0.0068, self.latent_error_median_=0.0009, self.latent_error_range_=0.0015\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.341692\n",
      "epoch : 1000/5000, loss = 0.003085\n",
      "epoch : 2000/5000, loss = 0.003083\n",
      "epoch : 3000/5000, loss = 0.002416\n",
      "epoch : 4000/5000, loss = 0.002925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [03:29, 104.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002570\n",
      "self.error_median=0.0081, self.error_range=0.0190, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5241495966911316; loose: 0.25724413990974426\n",
      "Test data rec score: tight: 3.618764638900757; loose: 1.252694845199585\n",
      "Test data average rec_m: 2.0575640201568604; average compct_m: 8.227365493774414\n",
      "Dependency: 0.5491772890090942; Cluster: 0.11670511960983276; Local: 0.17690438032150269; Global: 0.15721319615840912;\n",
      "Current experiment parameters: ('37_Stamps', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8617826695508335, 'aucpr': 0.41671228718873266}, fitting time: 104.47099685668945, inference time: 0.006957292556762695\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 188, 'Anomalies Ratio(%)': 9.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.527491\n",
      "epoch : 1000/5000, loss = 0.018647\n",
      "epoch : 2000/5000, loss = 0.015124\n",
      "epoch : 3000/5000, loss = 0.012434\n",
      "epoch : 4000/5000, loss = 0.012365\n",
      "epoch : 5000/5000, loss = 0.008327\n",
      "self.error_median=0.0036, self.error_range=0.0068, self.latent_error_median_=0.0006, self.latent_error_range_=0.0011\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.565983\n",
      "epoch : 1000/5000, loss = 0.002820\n",
      "epoch : 2000/5000, loss = 0.003444\n",
      "epoch : 3000/5000, loss = 0.002947\n",
      "epoch : 4000/5000, loss = 0.002060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [05:15, 105.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001930\n",
      "self.error_median=0.0058, self.error_range=0.0124, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.53733229637146; loose: 0.28657498955726624\n",
      "Test data rec score: tight: 2.327200174331665; loose: 1.5743443965911865\n",
      "Test data average rec_m: 0.2602071464061737; average compct_m: 1.1886719465255737\n",
      "Dependency: 0.40263617038726807; Cluster: 0.19755719602108002; Local: 0.18225382268428802; Global: 0.21755285561084747;\n",
      "Current experiment parameters: ('37_Stamps', 0.0, 3), model: Customized, metrics: {'aucroc': 0.877954306722689, 'aucpr': 0.36283432351411304}, fitting time: 105.26661086082458, inference time: 0.00698089599609375\n",
      "{'Samples': 2000, 'Features': 30, 'Anomalies': 50, 'Anomalies Ratio(%)': 2.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 24.207351\n",
      "epoch : 1000/5000, loss = 0.054480\n",
      "epoch : 2000/5000, loss = 0.029349\n",
      "epoch : 3000/5000, loss = 0.020436\n",
      "epoch : 4000/5000, loss = 0.014214\n",
      "epoch : 5000/5000, loss = 0.019035\n",
      "self.error_median=0.0043, self.error_range=0.0114, self.latent_error_median_=0.0003, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 58.326606\n",
      "epoch : 1000/5000, loss = 0.012092\n",
      "epoch : 2000/5000, loss = 0.005465\n",
      "epoch : 3000/5000, loss = 0.004012\n",
      "epoch : 4000/5000, loss = 0.005070\n",
      "epoch : 5000/5000, loss = 0.002811\n",
      "self.error_median=0.0098, self.error_range=0.0339, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 15. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.46150243282318115; loose: 0.39798563718795776\n",
      "Test data rec score: tight: 4.240017890930176; loose: 1.485182285308838\n",
      "Test data average rec_m: -1.5125495195388794; average compct_m: 1.7415499687194824\n",
      "Dependency: 0.4698069393634796; Cluster: 0.16714628040790558; Local: 0.19240489602088928; Global: 0.17064186930656433;\n",
      "Current experiment parameters: ('43_WDBC', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9996581196581197, 'aucpr': 0.9866269841269841}, fitting time: 104.3962013721466, inference time: 0.006980419158935547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [07:00, 104.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 30, 'Anomalies': 60, 'Anomalies Ratio(%)': 3.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 24.373426\n",
      "epoch : 1000/5000, loss = 0.035747\n",
      "epoch : 2000/5000, loss = 0.022750\n",
      "epoch : 3000/5000, loss = 0.016919\n",
      "epoch : 4000/5000, loss = 0.015111\n",
      "epoch : 5000/5000, loss = 0.011501\n",
      "self.error_median=0.0041, self.error_range=0.0104, self.latent_error_median_=0.0005, self.latent_error_range_=0.0010\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 75.237873\n",
      "epoch : 1000/5000, loss = 0.013650\n",
      "epoch : 2000/5000, loss = 0.004559\n",
      "epoch : 3000/5000, loss = 0.003881\n",
      "epoch : 4000/5000, loss = 0.003166\n",
      "epoch : 5000/5000, loss = 0.002866\n",
      "self.error_median=0.0095, self.error_range=0.0284, self.latent_error_median_=0.0002, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 15. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.8746787905693054; loose: 0.5410793423652649\n",
      "Test data rec score: tight: 4.948266506195068; loose: 1.699843168258667\n",
      "Test data average rec_m: -4.1377272605896; average compct_m: 0.07000157237052917\n",
      "Dependency: 0.44776856899261475; Cluster: 0.17545810341835022; Local: 0.1988845020532608; Global: 0.17788882553577423;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [08:44, 104.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('43_WDBC', 0.0, 2), model: Customized, metrics: {'aucroc': 0.999331806032837, 'aucpr': 0.9820747520976354}, fitting time: 104.12810826301575, inference time: 0.006980180740356445\n",
      "{'Samples': 2000, 'Features': 30, 'Anomalies': 53, 'Anomalies Ratio(%)': 2.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 23.797167\n",
      "epoch : 1000/5000, loss = 0.050266\n",
      "epoch : 2000/5000, loss = 0.025957\n",
      "epoch : 3000/5000, loss = 0.022240\n",
      "epoch : 4000/5000, loss = 0.018712\n",
      "epoch : 5000/5000, loss = 0.013241\n",
      "self.error_median=0.0051, self.error_range=0.0129, self.latent_error_median_=0.0006, self.latent_error_range_=0.0012\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 60.242377\n",
      "epoch : 1000/5000, loss = 0.006344\n",
      "epoch : 2000/5000, loss = 0.004775\n",
      "epoch : 3000/5000, loss = 0.004273\n",
      "epoch : 4000/5000, loss = 0.004223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [10:28, 104.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003091\n",
      "self.error_median=0.0129, self.error_range=0.0398, self.latent_error_median_=0.0002, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 15. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.8304233551025391; loose: 0.29490479826927185\n",
      "Test data rec score: tight: 3.629208564758301; loose: 1.0750724077224731\n",
      "Test data average rec_m: 1.8951119184494019; average compct_m: -0.2913346290588379\n",
      "Dependency: 0.3835344612598419; Cluster: 0.2226976603269577; Local: 0.17465883493423462; Global: 0.21910907328128815;\n",
      "Current experiment parameters: ('43_WDBC', 0.0, 3), model: Customized, metrics: {'aucroc': 0.997431506849315, 'aucpr': 0.904908990111196}, fitting time: 103.91477584838867, inference time: 0.006986379623413086\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 83, 'Anomalies Ratio(%)': 4.15}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.171850\n",
      "epoch : 1000/5000, loss = 0.018040\n",
      "epoch : 2000/5000, loss = 0.009999\n",
      "epoch : 3000/5000, loss = 0.007866\n",
      "epoch : 4000/5000, loss = 0.005628\n",
      "epoch : 5000/5000, loss = 0.005106\n",
      "self.error_median=0.0016, self.error_range=0.0037, self.latent_error_median_=0.0003, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 85.106997\n",
      "epoch : 1000/5000, loss = 0.003775\n",
      "epoch : 2000/5000, loss = 0.004229\n",
      "epoch : 3000/5000, loss = 0.002583\n",
      "epoch : 4000/5000, loss = 0.006086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [12:13, 104.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001790\n",
      "self.error_median=0.0059, self.error_range=0.0162, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.5955464839935303; loose: 0.5024247169494629\n",
      "Test data rec score: tight: 4.50384521484375; loose: 1.3163591623306274\n",
      "Test data average rec_m: -3.2789597511291504; average compct_m: 0.8365523219108582\n",
      "Dependency: 0.3985339403152466; Cluster: 0.21295934915542603; Local: 0.181412011384964; Global: 0.20709474384784698;\n",
      "Current experiment parameters: ('42_WBC', 0.0, 1), model: Customized, metrics: {'aucroc': 0.851895652173913, 'aucpr': 0.1366219077766364}, fitting time: 104.37135410308838, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 91, 'Anomalies Ratio(%)': 4.55}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.127126\n",
      "epoch : 1000/5000, loss = 0.019992\n",
      "epoch : 2000/5000, loss = 0.008905\n",
      "epoch : 3000/5000, loss = 0.011259\n",
      "epoch : 4000/5000, loss = 0.005541\n",
      "epoch : 5000/5000, loss = 0.005314\n",
      "self.error_median=0.0013, self.error_range=0.0039, self.latent_error_median_=0.0003, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 101.092376\n",
      "epoch : 1000/5000, loss = 0.006592\n",
      "epoch : 2000/5000, loss = 0.002494\n",
      "epoch : 3000/5000, loss = 0.001694\n",
      "epoch : 4000/5000, loss = 0.004261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [13:57, 104.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002007\n",
      "self.error_median=0.0049, self.error_range=0.0200, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.7378361225128174; loose: 0.617345929145813\n",
      "Test data rec score: tight: 3.3747901916503906; loose: 0.9395461082458496\n",
      "Test data average rec_m: 1.5370358228683472; average compct_m: -0.2275727093219757\n",
      "Dependency: 0.3729563057422638; Cluster: 0.24276790022850037; Local: 0.18072627484798431; Global: 0.2035495489835739;\n",
      "Current experiment parameters: ('42_WBC', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8683989399521685, 'aucpr': 0.19750895129761284}, fitting time: 104.50267672538757, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 93, 'Anomalies Ratio(%)': 4.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.027246\n",
      "epoch : 1000/5000, loss = 0.014039\n",
      "epoch : 2000/5000, loss = 0.009270\n",
      "epoch : 3000/5000, loss = 0.008637\n",
      "epoch : 4000/5000, loss = 0.007422\n",
      "epoch : 5000/5000, loss = 0.006733\n",
      "self.error_median=0.0020, self.error_range=0.0051, self.latent_error_median_=0.0004, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 78.709523\n",
      "epoch : 1000/5000, loss = 0.003429\n",
      "epoch : 2000/5000, loss = 0.002979\n",
      "epoch : 3000/5000, loss = 0.001994\n",
      "epoch : 4000/5000, loss = 0.002159\n",
      "epoch : 5000/5000, loss = 0.001921\n",
      "self.error_median=0.0048, self.error_range=0.0152, self.latent_error_median_=0.0002, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6911206245422363; loose: 0.4797859787940979\n",
      "Test data rec score: tight: 3.4294543266296387; loose: 1.1898891925811768\n",
      "Test data average rec_m: -1.7809524536132812; average compct_m: 4.3748884201049805\n",
      "Dependency: 0.42289167642593384; Cluster: 0.1825016587972641; Local: 0.20640738308429718; Global: 0.18819929659366608;\n",
      "Current experiment parameters: ('42_WBC', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8694430569430569, 'aucpr': 0.22034142842261226}, fitting time: 105.1767349243164, inference time: 0.006981611251831055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [15:43, 104.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 33, 'Anomalies': 467, 'Anomalies Ratio(%)': 23.35}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 23.154876\n",
      "epoch : 1000/5000, loss = 0.114086\n",
      "epoch : 2000/5000, loss = 0.067144\n",
      "epoch : 3000/5000, loss = 0.050375\n",
      "epoch : 4000/5000, loss = 0.046341\n",
      "epoch : 5000/5000, loss = 0.041094\n",
      "self.error_median=0.0231, self.error_range=0.0282, self.latent_error_median_=0.0023, self.latent_error_range_=0.0037\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.395262\n",
      "epoch : 1000/5000, loss = 0.019016\n",
      "epoch : 2000/5000, loss = 0.016406\n",
      "epoch : 3000/5000, loss = 0.013356\n",
      "epoch : 4000/5000, loss = 0.011937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [17:27, 104.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.011660\n",
      "self.error_median=0.0544, self.error_range=0.1416, self.latent_error_median_=0.0005, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.745867133140564; loose: 0.13650734722614288\n",
      "Test data rec score: tight: 8.834290504455566; loose: 0.9113188982009888\n",
      "Test data average rec_m: 2.348231554031372; average compct_m: -0.8631178736686707\n",
      "Dependency: 0.6865378618240356; Cluster: 0.09893167018890381; Local: 0.12510527670383453; Global: 0.089425228536129;\n",
      "Current experiment parameters: ('46_WPBC', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9819099378881988, 'aucpr': 0.9166118204594539}, fitting time: 103.68883013725281, inference time: 0.005982875823974609\n",
      "{'Samples': 2000, 'Features': 33, 'Anomalies': 450, 'Anomalies Ratio(%)': 22.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 23.225597\n",
      "epoch : 1000/5000, loss = 0.093083\n",
      "epoch : 2000/5000, loss = 0.058259\n",
      "epoch : 3000/5000, loss = 0.041647\n",
      "epoch : 4000/5000, loss = 0.035110\n",
      "epoch : 5000/5000, loss = 0.029642\n",
      "self.error_median=0.0149, self.error_range=0.0382, self.latent_error_median_=0.0017, self.latent_error_range_=0.0041\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.846350\n",
      "epoch : 1000/5000, loss = 0.018482\n",
      "epoch : 2000/5000, loss = 0.014133\n",
      "epoch : 3000/5000, loss = 0.012733\n",
      "epoch : 4000/5000, loss = 0.010148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [19:11, 104.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.010622\n",
      "self.error_median=0.0317, self.error_range=0.1508, self.latent_error_median_=0.0004, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.40581831336021423; loose: 0.30892232060432434\n",
      "Test data rec score: tight: 6.599714279174805; loose: 1.0548624992370605\n",
      "Test data average rec_m: -1.1591876745224; average compct_m: 2.1051583290100098\n",
      "Dependency: 0.5482913851737976; Cluster: 0.12576302886009216; Local: 0.19097845256328583; Global: 0.13496717810630798;\n",
      "Current experiment parameters: ('46_WPBC', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9973715651135006, 'aucpr': 0.9886223309178417}, fitting time: 103.74451684951782, inference time: 0.00698089599609375\n",
      "{'Samples': 2000, 'Features': 33, 'Anomalies': 443, 'Anomalies Ratio(%)': 22.15}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 23.731345\n",
      "epoch : 1000/5000, loss = 0.124764\n",
      "epoch : 2000/5000, loss = 0.082237\n",
      "epoch : 3000/5000, loss = 0.065752\n",
      "epoch : 4000/5000, loss = 0.051807\n",
      "epoch : 5000/5000, loss = 0.044413\n",
      "self.error_median=0.0247, self.error_range=0.0402, self.latent_error_median_=0.0022, self.latent_error_range_=0.0042\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 71.431946\n",
      "epoch : 1000/5000, loss = 0.023440\n",
      "epoch : 2000/5000, loss = 0.015773\n",
      "epoch : 3000/5000, loss = 0.013960\n",
      "epoch : 4000/5000, loss = 0.012146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [20:55, 104.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.010752\n",
      "self.error_median=0.0515, self.error_range=0.1114, self.latent_error_median_=0.0007, self.latent_error_range_=0.0016\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3939456641674042; loose: 0.21649856865406036\n",
      "Test data rec score: tight: 5.360288143157959; loose: 1.5683023929595947\n",
      "Test data average rec_m: 0.3626477122306824; average compct_m: -0.21675090491771698\n",
      "Dependency: 0.6116003394126892; Cluster: 0.10618993639945984; Local: 0.161032035946846; Global: 0.12117772549390793;\n",
      "Current experiment parameters: ('46_WPBC', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9856386147381301, 'aucpr': 0.9445432256728967}, fitting time: 103.89068388938904, inference time: 0.005982637405395508\n",
      "{'Samples': 2000, 'Features': 18, 'Anomalies': 74, 'Anomalies Ratio(%)': 3.7}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.840971\n",
      "epoch : 1000/5000, loss = 0.040442\n",
      "epoch : 2000/5000, loss = 0.018054\n",
      "epoch : 3000/5000, loss = 0.018141\n",
      "epoch : 4000/5000, loss = 0.014296\n",
      "epoch : 5000/5000, loss = 0.013800\n",
      "self.error_median=0.0065, self.error_range=0.0073, self.latent_error_median_=0.0011, self.latent_error_range_=0.0015\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 60.423124\n",
      "epoch : 1000/5000, loss = 0.008926\n",
      "epoch : 2000/5000, loss = 0.007407\n",
      "epoch : 3000/5000, loss = 0.005675\n",
      "epoch : 4000/5000, loss = 0.004982\n",
      "epoch : 5000/5000, loss = 0.003581\n",
      "self.error_median=0.0162, self.error_range=0.0281, self.latent_error_median_=0.0004, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.42117854952812195; loose: 0.4492534399032593\n",
      "Test data rec score: tight: 7.263687610626221; loose: 1.9975677728652954\n",
      "Test data average rec_m: -2.1939847469329834; average compct_m: 1.4510891437530518\n",
      "Dependency: 0.6359020471572876; Cluster: 0.07449032366275787; Local: 0.1266748458147049; Global: 0.16293276846408844;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [22:39, 104.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('21_Lymphography', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9248191255111671, 'aucpr': 0.40844287147170655}, fitting time: 103.73426747322083, inference time: 0.008975505828857422\n",
      "{'Samples': 2000, 'Features': 18, 'Anomalies': 74, 'Anomalies Ratio(%)': 3.7}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.086984\n",
      "epoch : 1000/5000, loss = 0.028647\n",
      "epoch : 2000/5000, loss = 0.019999\n",
      "epoch : 3000/5000, loss = 0.014315\n",
      "epoch : 4000/5000, loss = 0.011833\n",
      "epoch : 5000/5000, loss = 0.012187\n",
      "self.error_median=0.0043, self.error_range=0.0068, self.latent_error_median_=0.0008, self.latent_error_range_=0.0015\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 84.510185\n",
      "epoch : 1000/5000, loss = 0.008449\n",
      "epoch : 2000/5000, loss = 0.005041\n",
      "epoch : 3000/5000, loss = 0.004291\n",
      "epoch : 4000/5000, loss = 0.006138\n",
      "epoch : 5000/5000, loss = 0.003615\n",
      "self.error_median=0.0113, self.error_range=0.0208, self.latent_error_median_=0.0002, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.41865041851997375; loose: 0.5008553266525269\n",
      "Test data rec score: tight: 5.769158363342285; loose: 1.9416508674621582\n",
      "Test data average rec_m: -1.3679271936416626; average compct_m: -0.11131685972213745\n",
      "Dependency: 0.5343334078788757; Cluster: 0.1412830501794815; Local: 0.14938023686408997; Global: 0.1750032901763916;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [24:24, 104.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('21_Lymphography', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9594212016357345, 'aucpr': 0.36873782681762124}, fitting time: 104.47705960273743, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 18, 'Anomalies': 80, 'Anomalies Ratio(%)': 4.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.364766\n",
      "epoch : 1000/5000, loss = 0.026886\n",
      "epoch : 2000/5000, loss = 0.018448\n",
      "epoch : 3000/5000, loss = 0.016502\n",
      "epoch : 4000/5000, loss = 0.012297\n",
      "epoch : 5000/5000, loss = 0.009913\n",
      "self.error_median=0.0036, self.error_range=0.0065, self.latent_error_median_=0.0007, self.latent_error_range_=0.0012\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 74.455419\n",
      "epoch : 1000/5000, loss = 0.006017\n",
      "epoch : 2000/5000, loss = 0.004673\n",
      "epoch : 3000/5000, loss = 0.003949\n",
      "epoch : 4000/5000, loss = 0.005761\n",
      "epoch : 5000/5000, loss = 0.003104\n",
      "self.error_median=0.0076, self.error_range=0.0176, self.latent_error_median_=0.0002, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.8834284543991089; loose: 0.6107929348945618\n",
      "Test data rec score: tight: 7.809388160705566; loose: 2.6357882022857666\n",
      "Test data average rec_m: 12.959887504577637; average compct_m: 0.4134412407875061\n",
      "Dependency: 0.5591934323310852; Cluster: 0.11484512686729431; Local: 0.15209777653217316; Global: 0.17386369407176971;\n",
      "Current experiment parameters: ('21_Lymphography', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9637586805555555, 'aucpr': 0.4923189020241656}, fitting time: 104.01872277259827, inference time: 0.0069806575775146484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [26:08, 104.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 691, 'Anomalies Ratio(%)': 34.55}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.730864\n",
      "epoch : 1000/5000, loss = 0.017766\n",
      "epoch : 2000/5000, loss = 0.008806\n",
      "epoch : 3000/5000, loss = 0.008296\n",
      "epoch : 4000/5000, loss = 0.007347\n",
      "epoch : 5000/5000, loss = 0.006120\n",
      "self.error_median=0.0023, self.error_range=0.0051, self.latent_error_median_=0.0005, self.latent_error_range_=0.0010\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 72.686530\n",
      "epoch : 1000/5000, loss = 0.003169\n",
      "epoch : 2000/5000, loss = 0.003007\n",
      "epoch : 3000/5000, loss = 0.002153\n",
      "epoch : 4000/5000, loss = 0.002033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [27:53, 104.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002317\n",
      "self.error_median=0.0036, self.error_range=0.0131, self.latent_error_median_=0.0003, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.8453996181488037; loose: 0.5629851222038269\n",
      "Test data rec score: tight: 3.466135025024414; loose: 1.671244740486145\n",
      "Test data average rec_m: 1.0710335969924927; average compct_m: 0.5160380005836487\n",
      "Dependency: 0.3558659851551056; Cluster: 0.22856473922729492; Local: 0.1655811369419098; Global: 0.2499881535768509;\n",
      "Current experiment parameters: ('4_breastw', 0.0, 1), model: Customized, metrics: {'aucroc': 0.7469852859829628, 'aucpr': 0.5308395266014054}, fitting time: 104.66050577163696, inference time: 0.005983829498291016\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 680, 'Anomalies Ratio(%)': 34.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.744117\n",
      "epoch : 1000/5000, loss = 0.017253\n",
      "epoch : 2000/5000, loss = 0.012938\n",
      "epoch : 3000/5000, loss = 0.010894\n",
      "epoch : 4000/5000, loss = 0.025827\n",
      "epoch : 5000/5000, loss = 0.007308\n",
      "self.error_median=0.0024, self.error_range=0.0062, self.latent_error_median_=0.0004, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 85.590045\n",
      "epoch : 1000/5000, loss = 0.003940\n",
      "epoch : 2000/5000, loss = 0.002660\n",
      "epoch : 3000/5000, loss = 0.002761\n",
      "epoch : 4000/5000, loss = 0.001806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [29:38, 104.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001557\n",
      "self.error_median=0.0051, self.error_range=0.0184, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 3.1470916271209717; loose: 0.4947795867919922\n",
      "Test data rec score: tight: 2.8537192344665527; loose: 1.3118953704833984\n",
      "Test data average rec_m: -12.117615699768066; average compct_m: -3.2655184268951416\n",
      "Dependency: 0.4085239768028259; Cluster: 0.20728063583374023; Local: 0.19252744317054749; Global: 0.19166797399520874;\n",
      "Current experiment parameters: ('4_breastw', 0.0, 2), model: Customized, metrics: {'aucroc': 0.7445905129728659, 'aucpr': 0.4920878692132259}, fitting time: 104.89355373382568, inference time: 0.006981849670410156\n",
      "{'Samples': 2000, 'Features': 9, 'Anomalies': 700, 'Anomalies Ratio(%)': 35.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 13.561247\n",
      "epoch : 1000/5000, loss = 0.019795\n",
      "epoch : 2000/5000, loss = 0.010651\n",
      "epoch : 3000/5000, loss = 0.010661\n",
      "epoch : 4000/5000, loss = 0.008185\n",
      "epoch : 5000/5000, loss = 0.009388\n",
      "self.error_median=0.0036, self.error_range=0.0085, self.latent_error_median_=0.0010, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 69.379294\n",
      "epoch : 1000/5000, loss = 0.009502\n",
      "epoch : 2000/5000, loss = 0.002785\n",
      "epoch : 3000/5000, loss = 0.008314\n",
      "epoch : 4000/5000, loss = 0.005174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [31:23, 104.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001717\n",
      "self.error_median=0.0052, self.error_range=0.0162, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.4395272731781006; loose: 0.8119408488273621\n",
      "Test data rec score: tight: 2.3957104682922363; loose: 1.2318062782287598\n",
      "Test data average rec_m: -0.12712952494621277; average compct_m: 0.13444560766220093\n",
      "Dependency: 0.3158624470233917; Cluster: 0.25918638706207275; Local: 0.16123637557029724; Global: 0.26371482014656067;\n",
      "Current experiment parameters: ('4_breastw', 0.0, 3), model: Customized, metrics: {'aucroc': 0.7580830280830282, 'aucpr': 0.5188851260287517}, fitting time: 104.98295617103577, inference time: 0.006982326507568359\n",
      "{'Samples': 2000, 'Features': 13, 'Anomalies': 147, 'Anomalies Ratio(%)': 7.35}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.045394\n",
      "epoch : 1000/5000, loss = 0.039203\n",
      "epoch : 2000/5000, loss = 0.020450\n",
      "epoch : 3000/5000, loss = 0.015887\n",
      "epoch : 4000/5000, loss = 0.013832\n",
      "epoch : 5000/5000, loss = 0.014394\n",
      "self.error_median=0.0068, self.error_range=0.0088, self.latent_error_median_=0.0009, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 68.054280\n",
      "epoch : 1000/5000, loss = 0.005955\n",
      "epoch : 2000/5000, loss = 0.006105\n",
      "epoch : 3000/5000, loss = 0.004672\n",
      "epoch : 4000/5000, loss = 0.003904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [33:07, 104.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004242\n",
      "self.error_median=0.0172, self.error_range=0.0259, self.latent_error_median_=0.0009, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.13183647394180298; loose: 0.18593847751617432\n",
      "Test data rec score: tight: 5.491599082946777; loose: 1.6815608739852905\n",
      "Test data average rec_m: 3.909217119216919; average compct_m: -1.3481311798095703\n",
      "Dependency: 0.5919418931007385; Cluster: 0.0959756076335907; Local: 0.11127248406410217; Global: 0.200810045003891;\n",
      "Current experiment parameters: ('45_wine', 0.0, 1), model: Customized, metrics: {'aucroc': 0.985775016350556, 'aucpr': 0.8977713652050578}, fitting time: 104.07558846473694, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 13, 'Anomalies': 150, 'Anomalies Ratio(%)': 7.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 13.364776\n",
      "epoch : 1000/5000, loss = 0.047574\n",
      "epoch : 2000/5000, loss = 0.022545\n",
      "epoch : 3000/5000, loss = 0.018979\n",
      "epoch : 4000/5000, loss = 0.015320\n",
      "epoch : 5000/5000, loss = 0.013656\n",
      "self.error_median=0.0084, self.error_range=0.0094, self.latent_error_median_=0.0011, self.latent_error_range_=0.0017\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 73.556472\n",
      "epoch : 1000/5000, loss = 0.005888\n",
      "epoch : 2000/5000, loss = 0.004842\n",
      "epoch : 3000/5000, loss = 0.004819\n",
      "epoch : 4000/5000, loss = 0.004272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [34:51, 104.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004056\n",
      "self.error_median=0.0190, self.error_range=0.0312, self.latent_error_median_=0.0004, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.28119394183158875; loose: 0.20960633456707\n",
      "Test data rec score: tight: 6.334110736846924; loose: 1.8014522790908813\n",
      "Test data average rec_m: 8.4498929977417; average compct_m: 0.8038308620452881\n",
      "Dependency: 0.6789659857749939; Cluster: 0.06541835516691208; Local: 0.1340460181236267; Global: 0.12156970053911209;\n",
      "Current experiment parameters: ('45_wine', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9782182182182182, 'aucpr': 0.7369875431337893}, fitting time: 103.77543330192566, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 13, 'Anomalies': 162, 'Anomalies Ratio(%)': 8.1}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.418650\n",
      "epoch : 1000/5000, loss = 0.035083\n",
      "epoch : 2000/5000, loss = 0.025052\n",
      "epoch : 3000/5000, loss = 0.021567\n",
      "epoch : 4000/5000, loss = 0.017756\n",
      "epoch : 5000/5000, loss = 0.016076\n",
      "self.error_median=0.0078, self.error_range=0.0105, self.latent_error_median_=0.0013, self.latent_error_range_=0.0019\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 89.372990\n",
      "epoch : 1000/5000, loss = 0.006053\n",
      "epoch : 2000/5000, loss = 0.006133\n",
      "epoch : 3000/5000, loss = 0.005348\n",
      "epoch : 4000/5000, loss = 0.004102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [36:35, 104.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003338\n",
      "self.error_median=0.0190, self.error_range=0.0278, self.latent_error_median_=0.0004, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2742592394351959; loose: 0.16555051505565643\n",
      "Test data rec score: tight: 4.504104137420654; loose: 1.6421701908111572\n",
      "Test data average rec_m: -6.855332851409912; average compct_m: -0.10998187214136124\n",
      "Dependency: 0.6159021854400635; Cluster: 0.08898880332708359; Local: 0.17535032331943512; Global: 0.11975876241922379;\n",
      "Current experiment parameters: ('45_wine', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9810733730878921, 'aucpr': 0.7735025212995552}, fitting time: 103.68916177749634, inference time: 0.005992889404296875\n",
      "{'Samples': 2000, 'Features': 32, 'Anomalies': 716, 'Anomalies Ratio(%)': 35.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 29.142503\n",
      "epoch : 1000/5000, loss = 0.175428\n",
      "epoch : 2000/5000, loss = 0.109508\n",
      "epoch : 3000/5000, loss = 0.085276\n",
      "epoch : 4000/5000, loss = 0.067253\n",
      "epoch : 5000/5000, loss = 0.062538\n",
      "self.error_median=0.0358, self.error_range=0.0720, self.latent_error_median_=0.0025, self.latent_error_range_=0.0049\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.212787\n",
      "epoch : 1000/5000, loss = 0.024795\n",
      "epoch : 2000/5000, loss = 0.019848\n",
      "epoch : 3000/5000, loss = 0.016600\n",
      "epoch : 4000/5000, loss = 0.015979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [38:19, 104.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.012634\n",
      "self.error_median=0.0731, self.error_range=0.1551, self.latent_error_median_=0.0008, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.20520932972431183; loose: 0.03887796401977539\n",
      "Test data rec score: tight: 6.572898864746094; loose: 1.810075283050537\n",
      "Test data average rec_m: -1.6519078016281128; average compct_m: -1.0461375713348389\n",
      "Dependency: 0.566753625869751; Cluster: 0.16370786726474762; Local: 0.12217240035533905; Global: 0.14736612141132355;\n",
      "Current experiment parameters: ('18_Ionosphere', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9805496828752642, 'aucpr': 0.9314471490747698}, fitting time: 103.71603083610535, inference time: 0.00798177719116211\n",
      "{'Samples': 2000, 'Features': 32, 'Anomalies': 748, 'Anomalies Ratio(%)': 37.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 28.446844\n",
      "epoch : 1000/5000, loss = 0.166395\n",
      "epoch : 2000/5000, loss = 0.106256\n",
      "epoch : 3000/5000, loss = 0.084265\n",
      "epoch : 4000/5000, loss = 0.065638\n",
      "epoch : 5000/5000, loss = 0.059183\n",
      "self.error_median=0.0355, self.error_range=0.0639, self.latent_error_median_=0.0030, self.latent_error_range_=0.0052\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 79.363657\n",
      "epoch : 1000/5000, loss = 0.085585\n",
      "epoch : 2000/5000, loss = 0.021066\n",
      "epoch : 3000/5000, loss = 0.016930\n",
      "epoch : 4000/5000, loss = 0.015831\n",
      "epoch : 5000/5000, loss = 0.012834\n",
      "self.error_median=0.0938, self.error_range=0.2168, self.latent_error_median_=0.0011, self.latent_error_range_=0.0026\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.792982816696167; loose: 0.03377525508403778\n",
      "Test data rec score: tight: 7.330968379974365; loose: 1.3600192070007324\n",
      "Test data average rec_m: -3.250607490539551; average compct_m: -1.0623232126235962\n",
      "Dependency: 0.5313504338264465; Cluster: 0.23220305144786835; Local: 0.09667947888374329; Global: 0.13976706564426422;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [40:03, 104.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('18_Ionosphere', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9673489741641337, 'aucpr': 0.9138922413448093}, fitting time: 103.65781950950623, inference time: 0.006981611251831055\n",
      "{'Samples': 2000, 'Features': 32, 'Anomalies': 691, 'Anomalies Ratio(%)': 34.55}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 27.283175\n",
      "epoch : 1000/5000, loss = 0.207856\n",
      "epoch : 2000/5000, loss = 0.144366\n",
      "epoch : 3000/5000, loss = 0.101209\n",
      "epoch : 4000/5000, loss = 0.080903\n",
      "epoch : 5000/5000, loss = 0.074220\n",
      "self.error_median=0.0445, self.error_range=0.0886, self.latent_error_median_=0.0016, self.latent_error_range_=0.0033\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 61.129980\n",
      "epoch : 1000/5000, loss = 0.027022\n",
      "epoch : 2000/5000, loss = 0.020750\n",
      "epoch : 3000/5000, loss = 0.015696\n",
      "epoch : 4000/5000, loss = 0.013658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [41:47, 104.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.011909\n",
      "self.error_median=0.0839, self.error_range=0.1657, self.latent_error_median_=0.0010, self.latent_error_range_=0.0016\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.31092971563339233; loose: 0.27161261439323425\n",
      "Test data rec score: tight: 4.8115949630737305; loose: 1.9904974699020386\n",
      "Test data average rec_m: 1.1266701221466064; average compct_m: -0.1505829393863678\n",
      "Dependency: 0.5338608622550964; Cluster: 0.16553977131843567; Local: 0.16223376989364624; Global: 0.13836564123630524;\n",
      "Current experiment parameters: ('18_Ionosphere', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9759314575112783, 'aucpr': 0.9309931053531932}, fitting time: 103.74417090415955, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 8, 'Anomalies': 732, 'Anomalies Ratio(%)': 36.6}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.025010\n",
      "epoch : 1000/5000, loss = 0.032145\n",
      "epoch : 2000/5000, loss = 0.020209\n",
      "epoch : 3000/5000, loss = 0.019553\n",
      "epoch : 4000/5000, loss = 0.021185\n",
      "epoch : 5000/5000, loss = 0.012410\n",
      "self.error_median=0.0074, self.error_range=0.0102, self.latent_error_median_=0.0011, self.latent_error_range_=0.0019\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 69.350876\n",
      "epoch : 1000/5000, loss = 0.004295\n",
      "epoch : 2000/5000, loss = 0.003860\n",
      "epoch : 3000/5000, loss = 0.003121\n",
      "epoch : 4000/5000, loss = 0.003172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [43:32, 104.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003382\n",
      "self.error_median=0.0133, self.error_range=0.0225, self.latent_error_median_=0.0003, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.30131709575653076; loose: 0.2395879179239273\n",
      "Test data rec score: tight: 3.75506591796875; loose: 1.8337657451629639\n",
      "Test data average rec_m: -2.6043434143066406; average compct_m: 1.5289642810821533\n",
      "Dependency: 0.5797907710075378; Cluster: 0.10755512863397598; Local: 0.15397611260414124; Global: 0.15867802500724792;\n",
      "Current experiment parameters: ('29_Pima', 0.0, 1), model: Customized, metrics: {'aucroc': 0.7932177033492822, 'aucpr': 0.6539123549397295}, fitting time: 104.67940163612366, inference time: 0.006981372833251953\n",
      "{'Samples': 2000, 'Features': 8, 'Anomalies': 682, 'Anomalies Ratio(%)': 34.1}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.238513\n",
      "epoch : 1000/5000, loss = 0.029533\n",
      "epoch : 2000/5000, loss = 0.021063\n",
      "epoch : 3000/5000, loss = 0.016151\n",
      "epoch : 4000/5000, loss = 0.011185\n",
      "epoch : 5000/5000, loss = 0.012268\n",
      "self.error_median=0.0059, self.error_range=0.0083, self.latent_error_median_=0.0010, self.latent_error_range_=0.0024\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 87.045519\n",
      "epoch : 1000/5000, loss = 0.008302\n",
      "epoch : 2000/5000, loss = 0.015364\n",
      "epoch : 3000/5000, loss = 0.003522\n",
      "epoch : 4000/5000, loss = 0.002922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [45:17, 104.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003035\n",
      "self.error_median=0.0140, self.error_range=0.0251, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3488505482673645; loose: 0.23861435055732727\n",
      "Test data rec score: tight: 4.009634017944336; loose: 1.431601643562317\n",
      "Test data average rec_m: -9.382713317871094; average compct_m: 0.9186245799064636\n",
      "Dependency: 0.6354129910469055; Cluster: 0.07163050770759583; Local: 0.16513191163539886; Global: 0.1278245598077774;\n",
      "Current experiment parameters: ('29_Pima', 0.0, 2), model: Customized, metrics: {'aucroc': 0.7893300401358444, 'aucpr': 0.6348920005816704}, fitting time: 104.50858497619629, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 8, 'Anomalies': 653, 'Anomalies Ratio(%)': 32.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.899413\n",
      "epoch : 1000/5000, loss = 0.031946\n",
      "epoch : 2000/5000, loss = 0.022269\n",
      "epoch : 3000/5000, loss = 0.014973\n",
      "epoch : 4000/5000, loss = 0.013819\n",
      "epoch : 5000/5000, loss = 0.010072\n",
      "self.error_median=0.0057, self.error_range=0.0089, self.latent_error_median_=0.0011, self.latent_error_range_=0.0023\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 86.856340\n",
      "epoch : 1000/5000, loss = 0.004895\n",
      "epoch : 2000/5000, loss = 0.005092\n",
      "epoch : 3000/5000, loss = 0.004006\n",
      "epoch : 4000/5000, loss = 0.006426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [47:02, 104.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002810\n",
      "self.error_median=0.0174, self.error_range=0.0325, self.latent_error_median_=0.0002, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4912436008453369; loose: 0.3296318054199219\n",
      "Test data rec score: tight: 4.784512042999268; loose: 1.0608220100402832\n",
      "Test data average rec_m: -1.4288033246994019; average compct_m: -0.6132885217666626\n",
      "Dependency: 0.6248098015785217; Cluster: 0.0977247804403305; Local: 0.16193018853664398; Global: 0.11553525924682617;\n",
      "Current experiment parameters: ('29_Pima', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8107319660537482, 'aucpr': 0.6715833061763092}, fitting time: 104.68428993225098, inference time: 0.008976221084594727\n",
      "{'Samples': 2000, 'Features': 6, 'Anomalies': 263, 'Anomalies Ratio(%)': 13.15}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.861007\n",
      "epoch : 1000/5000, loss = 0.024615\n",
      "epoch : 2000/5000, loss = 0.010526\n",
      "epoch : 3000/5000, loss = 0.010712\n",
      "epoch : 4000/5000, loss = 0.013016\n",
      "epoch : 5000/5000, loss = 0.007570\n",
      "self.error_median=0.0028, self.error_range=0.0071, self.latent_error_median_=0.0006, self.latent_error_range_=0.0010\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 58.968118\n",
      "epoch : 1000/5000, loss = 0.005287\n",
      "epoch : 2000/5000, loss = 0.002287\n",
      "epoch : 3000/5000, loss = 0.001806\n",
      "epoch : 4000/5000, loss = 0.001552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [48:47, 104.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001443\n",
      "self.error_median=0.0083, self.error_range=0.0163, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3814220726490021; loose: 0.2510187327861786\n",
      "Test data rec score: tight: 2.5760014057159424; loose: 0.9174489378929138\n",
      "Test data average rec_m: -3.3837778568267822; average compct_m: 8.44218921661377\n",
      "Dependency: 0.457317978143692; Cluster: 0.171536386013031; Local: 0.21137556433677673; Global: 0.15977008640766144;\n",
      "Current experiment parameters: ('39_vertebral', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8176340533054739, 'aucpr': 0.457637763348687}, fitting time: 104.6565351486206, inference time: 0.007978200912475586\n",
      "{'Samples': 2000, 'Features': 6, 'Anomalies': 260, 'Anomalies Ratio(%)': 13.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.497002\n",
      "epoch : 1000/5000, loss = 0.022683\n",
      "epoch : 2000/5000, loss = 0.017232\n",
      "epoch : 3000/5000, loss = 0.011459\n",
      "epoch : 4000/5000, loss = 0.009709\n",
      "epoch : 5000/5000, loss = 0.007082\n",
      "self.error_median=0.0037, self.error_range=0.0077, self.latent_error_median_=0.0006, self.latent_error_range_=0.0010\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 48.117608\n",
      "epoch : 1000/5000, loss = 0.001801\n",
      "epoch : 2000/5000, loss = 0.002250\n",
      "epoch : 3000/5000, loss = 0.002485\n",
      "epoch : 4000/5000, loss = 0.001521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [50:32, 104.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001353\n",
      "self.error_median=0.0053, self.error_range=0.0121, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4194525182247162; loose: 0.2558622360229492\n",
      "Test data rec score: tight: 2.1414482593536377; loose: 1.6812553405761719\n",
      "Test data average rec_m: 1.2033072710037231; average compct_m: 0.2618388831615448\n",
      "Dependency: 0.42744386196136475; Cluster: 0.19967256486415863; Local: 0.15650859475135803; Global: 0.21637499332427979;\n",
      "Current experiment parameters: ('39_vertebral', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8848855486786521, 'aucpr': 0.6105322957039439}, fitting time: 104.99710869789124, inference time: 0.007978439331054688\n",
      "{'Samples': 2000, 'Features': 6, 'Anomalies': 259, 'Anomalies Ratio(%)': 12.95}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.585868\n",
      "epoch : 1000/5000, loss = 0.025450\n",
      "epoch : 2000/5000, loss = 0.019048\n",
      "epoch : 3000/5000, loss = 0.011828\n",
      "epoch : 4000/5000, loss = 0.010589\n",
      "epoch : 5000/5000, loss = 0.008402\n",
      "self.error_median=0.0033, self.error_range=0.0073, self.latent_error_median_=0.0008, self.latent_error_range_=0.0011\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 86.727163\n",
      "epoch : 1000/5000, loss = 0.003449\n",
      "epoch : 2000/5000, loss = 0.003337\n",
      "epoch : 3000/5000, loss = 0.002740\n",
      "epoch : 4000/5000, loss = 0.002217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [52:17, 104.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002271\n",
      "self.error_median=0.0073, self.error_range=0.0166, self.latent_error_median_=0.0003, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2524840831756592; loose: 0.1575450301170349\n",
      "Test data rec score: tight: 3.151034355163574; loose: 1.1847262382507324\n",
      "Test data average rec_m: -2.033752918243408; average compct_m: 2.139592409133911\n",
      "Dependency: 0.5395454168319702; Cluster: 0.13570362329483032; Local: 0.17217275500297546; Global: 0.15257824957370758;\n",
      "Current experiment parameters: ('39_vertebral', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8861381275174378, 'aucpr': 0.5700241436220651}, fitting time: 104.94369888305664, inference time: 0.006966590881347656\n",
      "{'Samples': 2000, 'Features': 19, 'Anomalies': 342, 'Anomalies Ratio(%)': 17.1}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 17.365707\n",
      "epoch : 1000/5000, loss = 0.076004\n",
      "epoch : 2000/5000, loss = 0.044984\n",
      "epoch : 3000/5000, loss = 0.043093\n",
      "epoch : 4000/5000, loss = 0.029285\n",
      "epoch : 5000/5000, loss = 0.027126\n",
      "self.error_median=0.0152, self.error_range=0.0134, self.latent_error_median_=0.0018, self.latent_error_range_=0.0026\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 63.082219\n",
      "epoch : 1000/5000, loss = 0.015853\n",
      "epoch : 2000/5000, loss = 0.010671\n",
      "epoch : 3000/5000, loss = 0.011771\n",
      "epoch : 4000/5000, loss = 0.008067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [54:01, 104.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.007646\n",
      "self.error_median=0.0355, self.error_range=0.0509, self.latent_error_median_=0.0008, self.latent_error_range_=0.0018\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3535231649875641; loose: 0.2635679244995117\n",
      "Test data rec score: tight: 10.903032302856445; loose: 2.7675085067749023\n",
      "Test data average rec_m: 18.073396682739258; average compct_m: -0.008272393606603146\n",
      "Dependency: 0.7924162149429321; Cluster: 0.028519758954644203; Local: 0.0936695784330368; Global: 0.0853944793343544;\n",
      "Current experiment parameters: ('15_Hepatitis', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8957824617608564, 'aucpr': 0.6808688055254029}, fitting time: 103.8684561252594, inference time: 0.00698089599609375\n",
      "{'Samples': 2000, 'Features': 19, 'Anomalies': 296, 'Anomalies Ratio(%)': 14.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 20.312331\n",
      "epoch : 1000/5000, loss = 0.081535\n",
      "epoch : 2000/5000, loss = 0.044700\n",
      "epoch : 3000/5000, loss = 0.035418\n",
      "epoch : 4000/5000, loss = 0.026969\n",
      "epoch : 5000/5000, loss = 0.026205\n",
      "self.error_median=0.0153, self.error_range=0.0151, self.latent_error_median_=0.0018, self.latent_error_range_=0.0028\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 84.218112\n",
      "epoch : 1000/5000, loss = 0.014468\n",
      "epoch : 2000/5000, loss = 0.016890\n",
      "epoch : 3000/5000, loss = 0.010246\n",
      "epoch : 4000/5000, loss = 0.008675\n",
      "epoch : 5000/5000, loss = 0.007481\n",
      "self.error_median=0.0367, self.error_range=0.0527, self.latent_error_median_=0.0010, self.latent_error_range_=0.0020\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4003392159938812; loose: 0.46203452348709106\n",
      "Test data rec score: tight: 10.80550765991211; loose: 3.825312614440918\n",
      "Test data average rec_m: 465.607666015625; average compct_m: -0.0696633905172348\n",
      "Dependency: 0.8068951964378357; Cluster: 0.020290829241275787; Local: 0.06411530077457428; Global: 0.10869869589805603;\n",
      "Current experiment parameters: ('15_Hepatitis', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8441258602871655, 'aucpr': 0.5441475514298495}, fitting time: 105.40908622741699, inference time: 0.006981849670410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [55:46, 104.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 19, 'Anomalies': 298, 'Anomalies Ratio(%)': 14.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.621817\n",
      "epoch : 1000/5000, loss = 0.056635\n",
      "epoch : 2000/5000, loss = 0.039731\n",
      "epoch : 3000/5000, loss = 0.029046\n",
      "epoch : 4000/5000, loss = 0.023005\n",
      "epoch : 5000/5000, loss = 0.020879\n",
      "self.error_median=0.0112, self.error_range=0.0106, self.latent_error_median_=0.0016, self.latent_error_range_=0.0021\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 69.206650\n",
      "epoch : 1000/5000, loss = 0.016929\n",
      "epoch : 2000/5000, loss = 0.009750\n",
      "epoch : 3000/5000, loss = 0.010170\n",
      "epoch : 4000/5000, loss = 0.006645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [57:32, 105.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005853\n",
      "self.error_median=0.0358, self.error_range=0.0490, self.latent_error_median_=0.0006, self.latent_error_range_=0.0013\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5330667495727539; loose: 0.3141467273235321\n",
      "Test data rec score: tight: 13.600956916809082; loose: 2.6390295028686523\n",
      "Test data average rec_m: -16.079355239868164; average compct_m: -0.3636929988861084\n",
      "Dependency: 0.8574890494346619; Cluster: 0.016830554232001305; Local: 0.05383066460490227; Global: 0.07184980809688568;\n",
      "Current experiment parameters: ('15_Hepatitis', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8842102948613646, 'aucpr': 0.5148159531915962}, fitting time: 105.97385001182556, inference time: 0.0069806575775146484\n",
      "{'Samples': 2000, 'Features': 7, 'Anomalies': 85, 'Anomalies Ratio(%)': 4.25}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.912670\n",
      "epoch : 1000/5000, loss = 0.016410\n",
      "epoch : 2000/5000, loss = 0.012592\n",
      "epoch : 3000/5000, loss = 0.010146\n",
      "epoch : 4000/5000, loss = 0.009699\n",
      "epoch : 5000/5000, loss = 0.007553\n",
      "self.error_median=0.0029, self.error_range=0.0062, self.latent_error_median_=0.0005, self.latent_error_range_=0.0013\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 64.922811\n",
      "epoch : 1000/5000, loss = 0.002885\n",
      "epoch : 2000/5000, loss = 0.002545\n",
      "epoch : 3000/5000, loss = 0.001961\n",
      "epoch : 4000/5000, loss = 0.001708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [59:18, 105.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003336\n",
      "self.error_median=0.0071, self.error_range=0.0163, self.latent_error_median_=0.0002, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5299950242042542; loose: 0.30007848143577576\n",
      "Test data rec score: tight: 2.818857192993164; loose: 1.201974868774414\n",
      "Test data average rec_m: -117.36797332763672; average compct_m: 0.5838231444358826\n",
      "Dependency: 0.404339075088501; Cluster: 0.21851858496665955; Local: 0.16809850931167603; Global: 0.20904384553432465;\n",
      "Current experiment parameters: ('14_glass', 0.0, 1), model: Customized, metrics: {'aucroc': 0.7890782608695652, 'aucpr': 0.15942458820481356}, fitting time: 106.09988069534302, inference time: 0.006977558135986328\n",
      "{'Samples': 2000, 'Features': 7, 'Anomalies': 86, 'Anomalies Ratio(%)': 4.3}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.525381\n",
      "epoch : 1000/5000, loss = 0.016005\n",
      "epoch : 2000/5000, loss = 0.012866\n",
      "epoch : 3000/5000, loss = 0.007535\n",
      "epoch : 4000/5000, loss = 0.008369\n",
      "epoch : 5000/5000, loss = 0.009342\n",
      "self.error_median=0.0027, self.error_range=0.0060, self.latent_error_median_=0.0004, self.latent_error_range_=0.0011\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.554957\n",
      "epoch : 1000/5000, loss = 0.004361\n",
      "epoch : 2000/5000, loss = 0.003597\n",
      "epoch : 3000/5000, loss = 0.012931\n",
      "epoch : 4000/5000, loss = 0.001662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [1:01:05, 105.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.010395\n",
      "self.error_median=0.0075, self.error_range=0.0184, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 4.909270286560059; loose: 0.37076568603515625\n",
      "Test data rec score: tight: 2.3550968170166016; loose: 1.0658859014511108\n",
      "Test data average rec_m: 5.533585548400879; average compct_m: -0.14595326781272888\n",
      "Dependency: 0.39959678053855896; Cluster: 0.20962239801883698; Local: 0.20559759438037872; Global: 0.18518325686454773;\n",
      "Current experiment parameters: ('14_glass', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8354998659876709, 'aucpr': 0.14202592094474725}, fitting time: 106.25882077217102, inference time: 0.005997180938720703\n",
      "{'Samples': 2000, 'Features': 7, 'Anomalies': 76, 'Anomalies Ratio(%)': 3.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 13.289289\n",
      "epoch : 1000/5000, loss = 0.017006\n",
      "epoch : 2000/5000, loss = 0.012922\n",
      "epoch : 3000/5000, loss = 0.009238\n",
      "epoch : 4000/5000, loss = 0.007644\n",
      "epoch : 5000/5000, loss = 0.009920\n",
      "self.error_median=0.0033, self.error_range=0.0064, self.latent_error_median_=0.0006, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.414285\n",
      "epoch : 1000/5000, loss = 0.009233\n",
      "epoch : 2000/5000, loss = 0.005081\n",
      "epoch : 3000/5000, loss = 0.003019\n",
      "epoch : 4000/5000, loss = 0.001734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [1:02:51, 105.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001575\n",
      "self.error_median=0.0058, self.error_range=0.0134, self.latent_error_median_=0.0001, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6112192869186401; loose: 0.37016934156417847\n",
      "Test data rec score: tight: 1.889570713043213; loose: 1.1184614896774292\n",
      "Test data average rec_m: -21.282428741455078; average compct_m: 0.5648052096366882\n",
      "Dependency: 0.41917884349823; Cluster: 0.18256749212741852; Local: 0.23849284648895264; Global: 0.15976087749004364;\n",
      "Current experiment parameters: ('14_glass', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8003164795418583, 'aucpr': 0.1601820203370251}, fitting time: 106.2513861656189, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 12, 'Anomalies': 61, 'Anomalies Ratio(%)': 3.05}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 13.596956\n",
      "epoch : 1000/5000, loss = 0.033242\n",
      "epoch : 2000/5000, loss = 0.023792\n",
      "epoch : 3000/5000, loss = 0.017143\n",
      "epoch : 4000/5000, loss = 0.013989\n",
      "epoch : 5000/5000, loss = 0.012263\n",
      "self.error_median=0.0053, self.error_range=0.0094, self.latent_error_median_=0.0008, self.latent_error_range_=0.0012\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 65.798323\n",
      "epoch : 1000/5000, loss = 0.005674\n",
      "epoch : 2000/5000, loss = 0.003917\n",
      "epoch : 3000/5000, loss = 0.003748\n",
      "epoch : 4000/5000, loss = 0.002906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [1:04:36, 105.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002785\n",
      "self.error_median=0.0135, self.error_range=0.0287, self.latent_error_median_=0.0003, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3684801459312439; loose: 0.21950572729110718\n",
      "Test data rec score: tight: 3.135870933532715; loose: 1.169392466545105\n",
      "Test data average rec_m: -4.727739334106445; average compct_m: 1.7411296367645264\n",
      "Dependency: 0.5086280703544617; Cluster: 0.1536780744791031; Local: 0.1618468463420868; Global: 0.17584702372550964;\n",
      "Current experiment parameters: ('40_vowels', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9450171821305842, 'aucpr': 0.37691371998348017}, fitting time: 104.80354571342468, inference time: 0.00797891616821289\n",
      "{'Samples': 2000, 'Features': 12, 'Anomalies': 67, 'Anomalies Ratio(%)': 3.35}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.508004\n",
      "epoch : 1000/5000, loss = 0.031273\n",
      "epoch : 2000/5000, loss = 0.020548\n",
      "epoch : 3000/5000, loss = 0.015440\n",
      "epoch : 4000/5000, loss = 0.011649\n",
      "epoch : 5000/5000, loss = 0.010203\n",
      "self.error_median=0.0047, self.error_range=0.0067, self.latent_error_median_=0.0009, self.latent_error_range_=0.0015\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.755954\n",
      "epoch : 1000/5000, loss = 0.005369\n",
      "epoch : 2000/5000, loss = 0.003411\n",
      "epoch : 3000/5000, loss = 0.002914\n",
      "epoch : 4000/5000, loss = 0.002915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [1:06:21, 105.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004211\n",
      "self.error_median=0.0187, self.error_range=0.0356, self.latent_error_median_=0.0007, self.latent_error_range_=0.0022\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.36734938621520996; loose: 0.2583010792732239\n",
      "Test data rec score: tight: 4.241464138031006; loose: 0.7023172378540039\n",
      "Test data average rec_m: -3.807858943939209; average compct_m: 2.122352123260498\n",
      "Dependency: 0.5916064977645874; Cluster: 0.11891227960586548; Local: 0.1653137505054474; Global: 0.12416749447584152;\n",
      "Current experiment parameters: ('40_vowels', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9922413793103448, 'aucpr': 0.8262302825170471}, fitting time: 105.0558009147644, inference time: 0.00698089599609375\n",
      "{'Samples': 2000, 'Features': 12, 'Anomalies': 73, 'Anomalies Ratio(%)': 3.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.939335\n",
      "epoch : 1000/5000, loss = 0.031444\n",
      "epoch : 2000/5000, loss = 0.021163\n",
      "epoch : 3000/5000, loss = 0.015050\n",
      "epoch : 4000/5000, loss = 0.009472\n",
      "epoch : 5000/5000, loss = 0.009012\n",
      "self.error_median=0.0038, self.error_range=0.0054, self.latent_error_median_=0.0009, self.latent_error_range_=0.0013\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 72.981880\n",
      "epoch : 1000/5000, loss = 0.007037\n",
      "epoch : 2000/5000, loss = 0.004463\n",
      "epoch : 3000/5000, loss = 0.003352\n",
      "epoch : 4000/5000, loss = 0.003083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [1:08:08, 105.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003330\n",
      "self.error_median=0.0104, self.error_range=0.0203, self.latent_error_median_=0.0004, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3240417540073395; loose: 0.2512895166873932\n",
      "Test data rec score: tight: 5.95294713973999; loose: 1.542593240737915\n",
      "Test data average rec_m: -3.2561113834381104; average compct_m: 0.3639722168445587\n",
      "Dependency: 0.6558783054351807; Cluster: 0.07272239774465561; Local: 0.10721027851104736; Global: 0.16418905556201935;\n",
      "Current experiment parameters: ('40_vowels', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9887543252595156, 'aucpr': 0.854949828668073}, fitting time: 106.08573341369629, inference time: 0.005983829498291016\n",
      "{'Samples': 2000, 'Features': 8, 'Anomalies': 709, 'Anomalies Ratio(%)': 35.45}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.807872\n",
      "epoch : 1000/5000, loss = 0.015932\n",
      "epoch : 2000/5000, loss = 0.012940\n",
      "epoch : 3000/5000, loss = 0.006634\n",
      "epoch : 4000/5000, loss = 0.004861\n",
      "epoch : 5000/5000, loss = 0.006052\n",
      "self.error_median=0.0020, self.error_range=0.0054, self.latent_error_median_=0.0006, self.latent_error_range_=0.0013\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.095461\n",
      "epoch : 1000/5000, loss = 0.003795\n",
      "epoch : 2000/5000, loss = 0.003054\n",
      "epoch : 3000/5000, loss = 0.001810\n",
      "epoch : 4000/5000, loss = 0.001988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [1:09:55, 106.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001393\n",
      "self.error_median=0.0092, self.error_range=0.0206, self.latent_error_median_=0.0003, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.8661725521087646; loose: 0.466983437538147\n",
      "Test data rec score: tight: 2.2216553688049316; loose: 1.0833327770233154\n",
      "Test data average rec_m: 0.18505646288394928; average compct_m: -0.06728096306324005\n",
      "Dependency: 0.44038718938827515; Cluster: 0.17999204993247986; Local: 0.19446884095668793; Global: 0.18515199422836304;\n",
      "Current experiment parameters: ('47_yeast', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8709830039669542, 'aucpr': 0.7245330941847967}, fitting time: 106.62173366546631, inference time: 0.006985187530517578\n",
      "{'Samples': 2000, 'Features': 8, 'Anomalies': 685, 'Anomalies Ratio(%)': 34.25}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.060289\n",
      "epoch : 1000/5000, loss = 0.012714\n",
      "epoch : 2000/5000, loss = 0.012462\n",
      "epoch : 3000/5000, loss = 0.006632\n",
      "epoch : 4000/5000, loss = 0.011272\n",
      "epoch : 5000/5000, loss = 0.008101\n",
      "self.error_median=0.0025, self.error_range=0.0052, self.latent_error_median_=0.0007, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 76.688566\n",
      "epoch : 1000/5000, loss = 0.003639\n",
      "epoch : 2000/5000, loss = 0.001723\n",
      "epoch : 3000/5000, loss = 0.002252\n",
      "epoch : 4000/5000, loss = 0.002410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [1:11:41, 106.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001157\n",
      "self.error_median=0.0033, self.error_range=0.0103, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7149328589439392; loose: 0.4951230585575104\n",
      "Test data rec score: tight: 2.730027437210083; loose: 2.057813882827759\n",
      "Test data average rec_m: -2.11962890625; average compct_m: -1.642691731452942\n",
      "Dependency: 0.4797547459602356; Cluster: 0.131728395819664; Local: 0.21820282936096191; Global: 0.17031407356262207;\n",
      "Current experiment parameters: ('47_yeast', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8802099413399198, 'aucpr': 0.7553985760019989}, fitting time: 105.84130501747131, inference time: 0.0069811344146728516\n",
      "{'Samples': 2000, 'Features': 8, 'Anomalies': 676, 'Anomalies Ratio(%)': 33.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.237380\n",
      "epoch : 1000/5000, loss = 0.016550\n",
      "epoch : 2000/5000, loss = 0.011856\n",
      "epoch : 3000/5000, loss = 0.008246\n",
      "epoch : 4000/5000, loss = 0.006671\n",
      "epoch : 5000/5000, loss = 0.007010\n",
      "self.error_median=0.0020, self.error_range=0.0050, self.latent_error_median_=0.0006, self.latent_error_range_=0.0011\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 78.929185\n",
      "epoch : 1000/5000, loss = 0.004129\n",
      "epoch : 2000/5000, loss = 0.001872\n",
      "epoch : 3000/5000, loss = 0.001555\n",
      "epoch : 4000/5000, loss = 0.001257\n",
      "epoch : 5000/5000, loss = 0.001629\n",
      "self.error_median=0.0028, self.error_range=0.0078, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6545882821083069; loose: 0.542528510093689\n",
      "Test data rec score: tight: 2.1050972938537598; loose: 1.901190996170044\n",
      "Test data average rec_m: -1.2363131046295166; average compct_m: 0.09877928346395493\n",
      "Dependency: 0.38358694314956665; Cluster: 0.2064710557460785; Local: 0.1728484332561493; Global: 0.23709358274936676;\n",
      "Current experiment parameters: ('47_yeast', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8714620739288506, 'aucpr': 0.7292951793637061}, fitting time: 105.66196703910828, inference time: 0.00698089599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [1:13:26, 106.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 32, 'Anomalies': 127, 'Anomalies Ratio(%)': 6.35}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 28.516150\n",
      "epoch : 1000/5000, loss = 0.086426\n",
      "epoch : 2000/5000, loss = 0.055147\n",
      "epoch : 3000/5000, loss = 0.039032\n",
      "epoch : 4000/5000, loss = 0.027844\n",
      "epoch : 5000/5000, loss = 0.026484\n",
      "self.error_median=0.0117, self.error_range=0.0279, self.latent_error_median_=0.0007, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 65.131349\n",
      "epoch : 1000/5000, loss = 0.013601\n",
      "epoch : 2000/5000, loss = 0.009311\n",
      "epoch : 3000/5000, loss = 0.007015\n",
      "epoch : 4000/5000, loss = 0.006082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [1:15:11, 105.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005617\n",
      "self.error_median=0.0201, self.error_range=0.0663, self.latent_error_median_=0.0003, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.38578101992607117; loose: 0.4172951877117157\n",
      "Test data rec score: tight: 4.35703706741333; loose: 1.3979566097259521\n",
      "Test data average rec_m: -1.0400205850601196; average compct_m: 0.3169309198856354\n",
      "Dependency: 0.4371487498283386; Cluster: 0.18839608132839203; Local: 0.1783571094274521; Global: 0.19609810411930084;\n",
      "Current experiment parameters: ('20_letter', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9997190485109572, 'aucpr': 0.995892451155609}, fitting time: 104.67546844482422, inference time: 0.006980419158935547\n",
      "{'Samples': 2000, 'Features': 32, 'Anomalies': 139, 'Anomalies Ratio(%)': 6.95}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 25.846277\n",
      "epoch : 1000/5000, loss = 0.098122\n",
      "epoch : 2000/5000, loss = 0.071199\n",
      "epoch : 3000/5000, loss = 0.056325\n",
      "epoch : 4000/5000, loss = 0.043091\n",
      "epoch : 5000/5000, loss = 0.039928\n",
      "self.error_median=0.0140, self.error_range=0.0418, self.latent_error_median_=0.0007, self.latent_error_range_=0.0023\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 76.170149\n",
      "epoch : 1000/5000, loss = 0.011351\n",
      "epoch : 2000/5000, loss = 0.010310\n",
      "epoch : 3000/5000, loss = 0.007364\n",
      "epoch : 4000/5000, loss = 0.005670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [1:16:57, 105.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005125\n",
      "self.error_median=0.0222, self.error_range=0.0682, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4916178584098816; loose: 0.35627830028533936\n",
      "Test data rec score: tight: 2.8701138496398926; loose: 1.6938761472702026\n",
      "Test data average rec_m: 0.2171197086572647; average compct_m: -0.1938229650259018\n",
      "Dependency: 0.39063209295272827; Cluster: 0.19307449460029602; Local: 0.19893524050712585; Global: 0.21735820174217224;\n",
      "Current experiment parameters: ('20_letter', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9986772486772486, 'aucpr': 0.9857899151942593}, fitting time: 104.92404508590698, inference time: 0.006955385208129883\n",
      "{'Samples': 2000, 'Features': 32, 'Anomalies': 128, 'Anomalies Ratio(%)': 6.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 26.632693\n",
      "epoch : 1000/5000, loss = 0.085749\n",
      "epoch : 2000/5000, loss = 0.056811\n",
      "epoch : 3000/5000, loss = 0.041769\n",
      "epoch : 4000/5000, loss = 0.031109\n",
      "epoch : 5000/5000, loss = 0.026971\n",
      "self.error_median=0.0107, self.error_range=0.0328, self.latent_error_median_=0.0009, self.latent_error_range_=0.0027\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 73.868834\n",
      "epoch : 1000/5000, loss = 0.010996\n",
      "epoch : 2000/5000, loss = 0.008235\n",
      "epoch : 3000/5000, loss = 0.006609\n",
      "epoch : 4000/5000, loss = 0.005389\n",
      "epoch : 5000/5000, loss = 0.007194\n",
      "self.error_median=0.0187, self.error_range=0.0617, self.latent_error_median_=0.0005, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6012845039367676; loose: 0.34503960609436035\n",
      "Test data rec score: tight: 3.3105883598327637; loose: 1.5465421676635742\n",
      "Test data average rec_m: 0.951187252998352; average compct_m: -5.484005451202393\n",
      "Dependency: 0.37148144841194153; Cluster: 0.22206032276153564; Local: 0.14246711134910583; Global: 0.26399117708206177;\n",
      "Current experiment parameters: ('20_letter', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9962071548979209, 'aucpr': 0.9471265831982025}, fitting time: 104.56467628479004, inference time: 0.006982326507568359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [1:18:41, 105.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 16.153211\n",
      "epoch : 1000/5000, loss = 0.047638\n",
      "epoch : 2000/5000, loss = 0.030047\n",
      "epoch : 3000/5000, loss = 0.024743\n",
      "epoch : 4000/5000, loss = 0.023121\n",
      "epoch : 5000/5000, loss = 0.024276\n",
      "self.error_median=0.0122, self.error_range=0.0170, self.latent_error_median_=0.0014, self.latent_error_range_=0.0018\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 64.655779\n",
      "epoch : 1000/5000, loss = 0.008903\n",
      "epoch : 2000/5000, loss = 0.007262\n",
      "epoch : 3000/5000, loss = 0.005838\n",
      "epoch : 4000/5000, loss = 0.005855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [1:20:26, 105.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004746\n",
      "self.error_median=0.0207, self.error_range=0.0447, self.latent_error_median_=0.0003, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5780735015869141; loose: 0.25003746151924133\n",
      "Test data rec score: tight: 5.550131320953369; loose: 1.6400010585784912\n",
      "Test data average rec_m: -4.592945098876953; average compct_m: 0.5581919550895691\n",
      "Dependency: 0.6107404828071594; Cluster: 0.09619487822055817; Local: 0.15277548134326935; Global: 0.14028917253017426;\n",
      "Current experiment parameters: ('6_cardio', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9727421130556926, 'aucpr': 0.7614702415764769}, fitting time: 104.49190902709961, inference time: 0.0069811344146728516\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.627882\n",
      "epoch : 1000/5000, loss = 0.054479\n",
      "epoch : 2000/5000, loss = 0.033326\n",
      "epoch : 3000/5000, loss = 0.025974\n",
      "epoch : 4000/5000, loss = 0.026842\n",
      "epoch : 5000/5000, loss = 0.016822\n",
      "self.error_median=0.0099, self.error_range=0.0137, self.latent_error_median_=0.0015, self.latent_error_range_=0.0022\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 87.273317\n",
      "epoch : 1000/5000, loss = 0.009390\n",
      "epoch : 2000/5000, loss = 0.008590\n",
      "epoch : 3000/5000, loss = 0.005633\n",
      "epoch : 4000/5000, loss = 0.005542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [1:22:11, 105.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004986\n",
      "self.error_median=0.0213, self.error_range=0.0392, self.latent_error_median_=0.0005, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3758067786693573; loose: 0.3377431333065033\n",
      "Test data rec score: tight: 6.551292896270752; loose: 2.042492151260376\n",
      "Test data average rec_m: -1.463071346282959; average compct_m: 2.5720036029815674\n",
      "Dependency: 0.6481863856315613; Cluster: 0.08336751908063889; Local: 0.13599556684494019; Global: 0.13245059549808502;\n",
      "Current experiment parameters: ('6_cardio', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9741847310276756, 'aucpr': 0.7177525790676236}, fitting time: 104.9586250782013, inference time: 0.0069866180419921875\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 19.474744\n",
      "epoch : 1000/5000, loss = 0.054453\n",
      "epoch : 2000/5000, loss = 0.033799\n",
      "epoch : 3000/5000, loss = 0.025725\n",
      "epoch : 4000/5000, loss = 0.022420\n",
      "epoch : 5000/5000, loss = 0.016162\n",
      "self.error_median=0.0099, self.error_range=0.0141, self.latent_error_median_=0.0015, self.latent_error_range_=0.0021\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 74.838584\n",
      "epoch : 1000/5000, loss = 0.009886\n",
      "epoch : 2000/5000, loss = 0.006939\n",
      "epoch : 3000/5000, loss = 0.005666\n",
      "epoch : 4000/5000, loss = 0.005175\n",
      "epoch : 5000/5000, loss = 0.004578\n",
      "self.error_median=0.0187, self.error_range=0.0361, self.latent_error_median_=0.0004, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2982453405857086; loose: 0.27497267723083496\n",
      "Test data rec score: tight: 5.641843795776367; loose: 1.8936878442764282\n",
      "Test data average rec_m: 9.820494651794434; average compct_m: -0.4716132879257202\n",
      "Dependency: 0.6173617839813232; Cluster: 0.09602855890989304; Local: 0.12130193412303925; Global: 0.1653076857328415;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [1:23:56, 104.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('6_cardio', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9706920769902434, 'aucpr': 0.7316516258690869}, fitting time: 104.24586272239685, inference time: 0.006968975067138672\n",
      "{'Samples': 2000, 'Features': 27, 'Anomalies': 693, 'Anomalies Ratio(%)': 34.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 20.135812\n",
      "epoch : 1000/5000, loss = 0.071128\n",
      "epoch : 2000/5000, loss = 0.042873\n",
      "epoch : 3000/5000, loss = 0.034319\n",
      "epoch : 4000/5000, loss = 0.030193\n",
      "epoch : 5000/5000, loss = 0.024881\n",
      "self.error_median=0.0092, self.error_range=0.0210, self.latent_error_median_=0.0019, self.latent_error_range_=0.0045\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 63.133679\n",
      "epoch : 1000/5000, loss = 0.017236\n",
      "epoch : 2000/5000, loss = 0.014320\n",
      "epoch : 3000/5000, loss = 0.008458\n",
      "epoch : 4000/5000, loss = 0.008226\n",
      "epoch : 5000/5000, loss = 0.007998\n",
      "self.error_median=0.0280, self.error_range=0.0824, self.latent_error_median_=0.0009, self.latent_error_range_=0.0028\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 13. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2844103276729584; loose: 0.12314709275960922\n",
      "Test data rec score: tight: 12.207032203674316; loose: 4.196322441101074\n",
      "Test data average rec_m: -0.8284035325050354; average compct_m: -0.2132001519203186\n",
      "Dependency: 0.6253238916397095; Cluster: 0.09238229691982269; Local: 0.1545901596546173; Global: 0.12770365178585052;\n",
      "Current experiment parameters: ('12_fault', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9933035714285714, 'aucpr': 0.981898714088404}, fitting time: 105.63455557823181, inference time: 0.005984067916870117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [1:25:42, 105.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 2000, 'Features': 27, 'Anomalies': 698, 'Anomalies Ratio(%)': 34.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 19.969231\n",
      "epoch : 1000/5000, loss = 0.086755\n",
      "epoch : 2000/5000, loss = 0.055236\n",
      "epoch : 3000/5000, loss = 0.038765\n",
      "epoch : 4000/5000, loss = 0.031296\n",
      "epoch : 5000/5000, loss = 0.026684\n",
      "self.error_median=0.0118, self.error_range=0.0293, self.latent_error_median_=0.0022, self.latent_error_range_=0.0045\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 80.139494\n",
      "epoch : 1000/5000, loss = 0.018705\n",
      "epoch : 2000/5000, loss = 0.012496\n",
      "epoch : 3000/5000, loss = 0.010263\n",
      "epoch : 4000/5000, loss = 0.008804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [1:27:27, 105.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.007227\n",
      "self.error_median=0.0263, self.error_range=0.0779, self.latent_error_median_=0.0010, self.latent_error_range_=0.0033\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 13. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.18895766139030457; loose: 0.06923206895589828\n",
      "Test data rec score: tight: 11.417959213256836; loose: 5.964738368988037\n",
      "Test data average rec_m: -1.541263222694397; average compct_m: -0.6294134855270386\n",
      "Dependency: 0.6395977139472961; Cluster: 0.08382958918809891; Local: 0.132036954164505; Global: 0.14453577995300293;\n",
      "Current experiment parameters: ('12_fault', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9903327255595394, 'aucpr': 0.9718382677027023}, fitting time: 104.93017077445984, inference time: 0.007979869842529297\n",
      "{'Samples': 2000, 'Features': 27, 'Anomalies': 699, 'Anomalies Ratio(%)': 34.95}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 22.698598\n",
      "epoch : 1000/5000, loss = 0.086572\n",
      "epoch : 2000/5000, loss = 0.045526\n",
      "epoch : 3000/5000, loss = 0.031462\n",
      "epoch : 4000/5000, loss = 0.025802\n",
      "epoch : 5000/5000, loss = 0.022056\n",
      "self.error_median=0.0088, self.error_range=0.0223, self.latent_error_median_=0.0019, self.latent_error_range_=0.0043\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 72.153881\n",
      "epoch : 1000/5000, loss = 0.013161\n",
      "epoch : 2000/5000, loss = 0.010337\n",
      "epoch : 3000/5000, loss = 0.009967\n",
      "epoch : 4000/5000, loss = 0.006866\n",
      "epoch : 5000/5000, loss = 0.006165\n",
      "self.error_median=0.0228, self.error_range=0.0644, self.latent_error_median_=0.0007, self.latent_error_range_=0.0025\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 13. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2928481996059418; loose: 0.10763245820999146\n",
      "Test data rec score: tight: 9.567503929138184; loose: 5.371357440948486\n",
      "Test data average rec_m: -0.4356730282306671; average compct_m: -0.6908952593803406\n",
      "Dependency: 0.5363501906394958; Cluster: 0.14233019948005676; Local: 0.11330652236938477; Global: 0.208013117313385;\n",
      "Current experiment parameters: ('12_fault', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9948473748473748, 'aucpr': 0.9890122295695257}, fitting time: 105.6003348827362, inference time: 0.006982326507568359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [1:29:13, 105.40s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "52it [1:29:15, 74.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [1:29:18, 53.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [1:29:21, 38.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 15.390527\n",
      "epoch : 1000/5000, loss = 0.043954\n",
      "epoch : 2000/5000, loss = 0.031359\n",
      "epoch : 3000/5000, loss = 0.025397\n",
      "epoch : 4000/5000, loss = 0.018059\n",
      "epoch : 5000/5000, loss = 0.016859\n",
      "self.error_median=0.0071, self.error_range=0.0116, self.latent_error_median_=0.0011, self.latent_error_range_=0.0016\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 60.960594\n",
      "epoch : 1000/5000, loss = 0.009008\n",
      "epoch : 2000/5000, loss = 0.011458\n",
      "epoch : 3000/5000, loss = 0.005621\n",
      "epoch : 4000/5000, loss = 0.006489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [1:31:04, 57.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004622\n",
      "self.error_median=0.0188, self.error_range=0.0463, self.latent_error_median_=0.0003, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.34635716676712036; loose: 0.3734953999519348\n",
      "Test data rec score: tight: 7.718465328216553; loose: 1.7329849004745483\n",
      "Test data average rec_m: -4.143494129180908; average compct_m: 0.6743492484092712\n",
      "Dependency: 0.6883659958839417; Cluster: 0.07362312823534012; Local: 0.10058034211397171; Global: 0.13743042945861816;\n",
      "Current experiment parameters: ('7_Cardiotocography', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9686868686868686, 'aucpr': 0.8916095490757896}, fitting time: 103.13123369216919, inference time: 0.005983829498291016\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 16.660681\n",
      "epoch : 1000/5000, loss = 0.056748\n",
      "epoch : 2000/5000, loss = 0.034528\n",
      "epoch : 3000/5000, loss = 0.024767\n",
      "epoch : 4000/5000, loss = 0.021465\n",
      "epoch : 5000/5000, loss = 0.020820\n",
      "self.error_median=0.0094, self.error_range=0.0130, self.latent_error_median_=0.0015, self.latent_error_range_=0.0023\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.958780\n",
      "epoch : 1000/5000, loss = 0.014925\n",
      "epoch : 2000/5000, loss = 0.008553\n",
      "epoch : 3000/5000, loss = 0.007264\n",
      "epoch : 4000/5000, loss = 0.007532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [1:32:48, 71.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005287\n",
      "self.error_median=0.0248, self.error_range=0.0565, self.latent_error_median_=0.0004, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.36956238746643066; loose: 0.38932734727859497\n",
      "Test data rec score: tight: 8.785122871398926; loose: 1.493497371673584\n",
      "Test data average rec_m: 0.6606810688972473; average compct_m: -1.1476185321807861\n",
      "Dependency: 0.7531422972679138; Cluster: 0.048485979437828064; Local: 0.10999492555856705; Global: 0.08837682753801346;\n",
      "Current experiment parameters: ('7_Cardiotocography', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9558441558441558, 'aucpr': 0.8089914638424796}, fitting time: 103.40831542015076, inference time: 0.00698089599609375\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.547127\n",
      "epoch : 1000/5000, loss = 0.059194\n",
      "epoch : 2000/5000, loss = 0.036112\n",
      "epoch : 3000/5000, loss = 0.030027\n",
      "epoch : 4000/5000, loss = 0.021695\n",
      "epoch : 5000/5000, loss = 0.018179\n",
      "self.error_median=0.0098, self.error_range=0.0133, self.latent_error_median_=0.0014, self.latent_error_range_=0.0021\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 75.497764\n",
      "epoch : 1000/5000, loss = 0.010590\n",
      "epoch : 2000/5000, loss = 0.009387\n",
      "epoch : 3000/5000, loss = 0.006358\n",
      "epoch : 4000/5000, loss = 0.006043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [1:34:34, 81.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005435\n",
      "self.error_median=0.0218, self.error_range=0.0560, self.latent_error_median_=0.0003, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.36874154210090637; loose: 0.267922967672348\n",
      "Test data rec score: tight: 8.388352394104004; loose: 1.5641072988510132\n",
      "Test data average rec_m: -4.965719223022461; average compct_m: 0.7955623269081116\n",
      "Dependency: 0.7350776195526123; Cluster: 0.04723679646849632; Local: 0.1141272559762001; Global: 0.10355827957391739;\n",
      "Current experiment parameters: ('7_Cardiotocography', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9537806637806638, 'aucpr': 0.7845050413638041}, fitting time: 105.72396206855774, inference time: 0.006980419158935547\n",
      "{'Samples': 3062, 'Features': 50, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 36.855658\n",
      "epoch : 1000/5000, loss = 0.217361\n",
      "epoch : 2000/5000, loss = 0.189432\n",
      "epoch : 3000/5000, loss = 0.140788\n",
      "epoch : 4000/5000, loss = 0.108451\n",
      "epoch : 5000/5000, loss = 0.091607\n",
      "self.error_median=0.0308, self.error_range=0.1189, self.latent_error_median_=0.0003, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 65.499609\n",
      "epoch : 1000/5000, loss = 0.021485\n",
      "epoch : 2000/5000, loss = 0.016614\n",
      "epoch : 3000/5000, loss = 0.014633\n",
      "epoch : 4000/5000, loss = 0.012790\n",
      "epoch : 5000/5000, loss = 0.011652\n",
      "self.error_median=0.0433, self.error_range=0.1566, self.latent_error_median_=0.0002, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2847776412963867; loose: 0.5856178998947144\n",
      "Test data rec score: tight: 1.460766077041626; loose: 0.9089470505714417\n",
      "Test data average rec_m: 0.7632178664207458; average compct_m: -0.12460047751665115\n",
      "Dependency: 0.2276736944913864; Cluster: 0.3334802985191345; Local: 0.10945184528827667; Global: 0.32939413189888;\n",
      "Current experiment parameters: ('25_musk', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9970166602092213, 'aucpr': 0.8649759391348694}, fitting time: 106.98760676383972, inference time: 0.007946968078613281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [1:36:21, 89.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 3062, 'Features': 50, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 34.835808\n",
      "epoch : 1000/5000, loss = 0.224430\n",
      "epoch : 2000/5000, loss = 0.166303\n",
      "epoch : 3000/5000, loss = 0.137990\n",
      "epoch : 4000/5000, loss = 0.111357\n",
      "epoch : 5000/5000, loss = 0.094888\n",
      "self.error_median=0.0362, self.error_range=0.1201, self.latent_error_median_=0.0008, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 74.005777\n",
      "epoch : 1000/5000, loss = 0.027499\n",
      "epoch : 2000/5000, loss = 0.021067\n",
      "epoch : 3000/5000, loss = 0.018836\n",
      "epoch : 4000/5000, loss = 0.016361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [1:38:08, 94.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.015138\n",
      "self.error_median=0.0558, self.error_range=0.1767, self.latent_error_median_=0.0003, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.1053193807601929; loose: 0.4747779965400696\n",
      "Test data rec score: tight: 1.324363350868225; loose: 0.7923601269721985\n",
      "Test data average rec_m: -0.2150319665670395; average compct_m: 0.3069981038570404\n",
      "Dependency: 0.28820672631263733; Cluster: 0.3056495785713196; Local: 0.14151917397975922; Global: 0.2646245062351227;\n",
      "Current experiment parameters: ('25_musk', 0.0, 2), model: Customized, metrics: {'aucroc': 0.980123982952344, 'aucpr': 0.624241560693646}, fitting time: 106.32653522491455, inference time: 0.007973194122314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "60it [1:49:43, 274.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 16.689860\n",
      "epoch : 1000/5000, loss = 0.060905\n",
      "epoch : 2000/5000, loss = 0.047682\n",
      "epoch : 3000/5000, loss = 0.041757\n",
      "epoch : 4000/5000, loss = 0.036834\n",
      "epoch : 5000/5000, loss = 0.031597\n",
      "self.error_median=0.0182, self.error_range=0.0251, self.latent_error_median_=0.0010, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 72.921763\n",
      "epoch : 1000/5000, loss = 0.008532\n",
      "epoch : 2000/5000, loss = 0.010766\n",
      "epoch : 3000/5000, loss = 0.006384\n",
      "epoch : 4000/5000, loss = 0.005724\n",
      "epoch : 5000/5000, loss = 0.005259\n",
      "self.error_median=0.0304, self.error_range=0.0468, self.latent_error_median_=0.0003, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.23310023546218872; loose: 0.2987908124923706\n",
      "Test data rec score: tight: 1.6684045791625977; loose: 0.6387073397636414\n",
      "Test data average rec_m: -2.624964475631714; average compct_m: 2.2060911655426025\n",
      "Dependency: 0.4128555357456207; Cluster: 0.18641982972621918; Local: 0.155804842710495; Global: 0.2449198216199875;\n",
      "Current experiment parameters: ('41_Waveform', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9941176470588236, 'aucpr': 0.8240966087392493}, fitting time: 102.91794872283936, inference time: 0.006968975067138672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [1:51:26, 223.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 19.649006\n",
      "epoch : 1000/5000, loss = 0.062315\n",
      "epoch : 2000/5000, loss = 0.044498\n",
      "epoch : 3000/5000, loss = 0.037706\n",
      "epoch : 4000/5000, loss = 0.031503\n",
      "epoch : 5000/5000, loss = 0.029070\n",
      "self.error_median=0.0163, self.error_range=0.0235, self.latent_error_median_=0.0010, self.latent_error_range_=0.0012\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 86.475489\n",
      "epoch : 1000/5000, loss = 0.007422\n",
      "epoch : 2000/5000, loss = 0.007193\n",
      "epoch : 3000/5000, loss = 0.006496\n",
      "epoch : 4000/5000, loss = 0.005229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [1:53:09, 187.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005098\n",
      "self.error_median=0.0270, self.error_range=0.0446, self.latent_error_median_=0.0003, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.1785905659198761; loose: 0.3004939556121826\n",
      "Test data rec score: tight: 2.001235246658325; loose: 0.658213198184967\n",
      "Test data average rec_m: -2.975741147994995; average compct_m: -0.045697104185819626\n",
      "Dependency: 0.4692609906196594; Cluster: 0.14430905878543854; Local: 0.161675363779068; Global: 0.22475461661815643;\n",
      "Current experiment parameters: ('41_Waveform', 0.0, 2), model: Customized, metrics: {'aucroc': 0.993419740777667, 'aucpr': 0.8592178718314133}, fitting time: 102.63373303413391, inference time: 0.006948709487915039\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 20.403149\n",
      "epoch : 1000/5000, loss = 0.056357\n",
      "epoch : 2000/5000, loss = 0.042793\n",
      "epoch : 3000/5000, loss = 0.033713\n",
      "epoch : 4000/5000, loss = 0.031002\n",
      "epoch : 5000/5000, loss = 0.026255\n",
      "self.error_median=0.0163, self.error_range=0.0221, self.latent_error_median_=0.0009, self.latent_error_range_=0.0013\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 74.092525\n",
      "epoch : 1000/5000, loss = 0.006876\n",
      "epoch : 2000/5000, loss = 0.005942\n",
      "epoch : 3000/5000, loss = 0.005394\n",
      "epoch : 4000/5000, loss = 0.005126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [1:54:52, 161.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005010\n",
      "self.error_median=0.0258, self.error_range=0.0484, self.latent_error_median_=0.0003, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 10. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.302470326423645; loose: 0.16425924003124237\n",
      "Test data rec score: tight: 1.8790266513824463; loose: 0.6334120035171509\n",
      "Test data average rec_m: -0.0028519018087536097; average compct_m: -0.16618138551712036\n",
      "Dependency: 0.44630545377731323; Cluster: 0.19798901677131653; Local: 0.14774608612060547; Global: 0.20795945823192596;\n",
      "Current experiment parameters: ('41_Waveform', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9917248255234297, 'aucpr': 0.8223744569446515}, fitting time: 102.36896014213562, inference time: 0.005983829498291016\n",
      "{'Samples': 3686, 'Features': 50, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 40.198321\n",
      "epoch : 1000/5000, loss = 0.079435\n",
      "epoch : 2000/5000, loss = 0.071246\n",
      "epoch : 3000/5000, loss = 0.060005\n",
      "epoch : 4000/5000, loss = 0.055177\n",
      "epoch : 5000/5000, loss = 0.045744\n",
      "self.error_median=0.0199, self.error_range=0.0633, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 71.683008\n",
      "epoch : 1000/5000, loss = 0.007532\n",
      "epoch : 2000/5000, loss = 0.006980\n",
      "epoch : 3000/5000, loss = 0.005917\n",
      "epoch : 4000/5000, loss = 0.005457\n",
      "epoch : 5000/5000, loss = 0.005199\n",
      "self.error_median=0.0213, self.error_range=0.0678, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4339847266674042; loose: 0.4772399663925171\n",
      "Test data rec score: tight: 0.9218422174453735; loose: 0.7928420305252075\n",
      "Test data average rec_m: -10.378022193908691; average compct_m: 0.020131882280111313\n",
      "Dependency: 0.27679747343063354; Cluster: 0.2688601315021515; Local: 0.19060896337032318; Global: 0.26373347640037537;\n",
      "Current experiment parameters: ('36_speech', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9997446895424836, 'aucpr': 0.9853285173718611}, fitting time: 102.43759870529175, inference time: 0.005984067916870117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [1:56:34, 144.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 3686, 'Features': 50, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 39.883686\n",
      "epoch : 1000/5000, loss = 0.073238\n",
      "epoch : 2000/5000, loss = 0.067052\n",
      "epoch : 3000/5000, loss = 0.063908\n",
      "epoch : 4000/5000, loss = 0.053474\n",
      "epoch : 5000/5000, loss = 0.050566\n",
      "self.error_median=0.0226, self.error_range=0.0681, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.610944\n",
      "epoch : 1000/5000, loss = 0.007574\n",
      "epoch : 2000/5000, loss = 0.006407\n",
      "epoch : 3000/5000, loss = 0.005761\n",
      "epoch : 4000/5000, loss = 0.005339\n",
      "epoch : 5000/5000, loss = 0.005204\n",
      "self.error_median=0.0217, self.error_range=0.0670, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.47315943241119385; loose: 0.43002358078956604\n",
      "Test data rec score: tight: 0.6512769460678101; loose: 0.6364580392837524\n",
      "Test data average rec_m: 2.6148502826690674; average compct_m: -1.356300950050354\n",
      "Dependency: 0.1849747747182846; Cluster: 0.3333152234554291; Local: 0.1699257791042328; Global: 0.3117842376232147;\n",
      "Current experiment parameters: ('36_speech', 0.0, 2), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0}, fitting time: 103.14619421958923, inference time: 0.008998870849609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [1:58:18, 131.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 3686, 'Features': 50, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 38.976902\n",
      "epoch : 1000/5000, loss = 0.081803\n",
      "epoch : 2000/5000, loss = 0.079581\n",
      "epoch : 3000/5000, loss = 0.074668\n",
      "epoch : 4000/5000, loss = 0.070012\n",
      "epoch : 5000/5000, loss = 0.067811\n",
      "self.error_median=0.0273, self.error_range=0.0821, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 58.282660\n",
      "epoch : 1000/5000, loss = 0.007797\n",
      "epoch : 2000/5000, loss = 0.007277\n",
      "epoch : 3000/5000, loss = 0.006654\n",
      "epoch : 4000/5000, loss = 0.006137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [2:00:02, 123.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005789\n",
      "self.error_median=0.0253, self.error_range=0.0707, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.42463019490242004; loose: 0.44908425211906433\n",
      "Test data rec score: tight: 0.6070577502250671; loose: 0.671901285648346\n",
      "Test data average rec_m: 0.3795812726020813; average compct_m: -0.5045045018196106\n",
      "Dependency: 0.1222032830119133; Cluster: 0.4074186682701111; Local: 0.08591236919164658; Global: 0.38446569442749023;\n",
      "Current experiment parameters: ('36_speech', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9998978758169934, 'aucpr': 0.9939896036387265}, fitting time: 103.44624614715576, inference time: 0.0069811344146728516\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.642948\n",
      "epoch : 1000/5000, loss = 0.008194\n",
      "epoch : 2000/5000, loss = 0.004440\n",
      "epoch : 3000/5000, loss = 0.003510\n",
      "epoch : 4000/5000, loss = 0.002915\n",
      "epoch : 5000/5000, loss = 0.003254\n",
      "self.error_median=0.0013, self.error_range=0.0031, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.258678\n",
      "epoch : 1000/5000, loss = 0.003806\n",
      "epoch : 2000/5000, loss = 0.001069\n",
      "epoch : 3000/5000, loss = 0.009480\n",
      "epoch : 4000/5000, loss = 0.005532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [2:01:46, 117.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.000713\n",
      "self.error_median=0.0019, self.error_range=0.0053, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.40822339057922363; loose: 0.46330153942108154\n",
      "Test data rec score: tight: 1.4513158798217773; loose: 0.8990597724914551\n",
      "Test data average rec_m: 17.31056022644043; average compct_m: 0.20231227576732635\n",
      "Dependency: 0.3818649649620056; Cluster: 0.18466968834400177; Local: 0.23049290478229523; Global: 0.2029724419116974;\n",
      "Current experiment parameters: ('38_thyroid', 0.0, 1), model: Customized, metrics: {'aucroc': 0.822366718426501, 'aucpr': 0.1518001605681381}, fitting time: 103.93541288375854, inference time: 0.0069811344146728516\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 6.811134\n",
      "epoch : 1000/5000, loss = 0.007527\n",
      "epoch : 2000/5000, loss = 0.005858\n",
      "epoch : 3000/5000, loss = 0.003802\n",
      "epoch : 4000/5000, loss = 0.002991\n",
      "epoch : 5000/5000, loss = 0.002880\n",
      "self.error_median=0.0010, self.error_range=0.0022, self.latent_error_median_=0.0002, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 62.406797\n",
      "epoch : 1000/5000, loss = 0.001579\n",
      "epoch : 2000/5000, loss = 0.002030\n",
      "epoch : 3000/5000, loss = 0.002421\n",
      "epoch : 4000/5000, loss = 0.002864\n",
      "epoch : 5000/5000, loss = 0.001268\n",
      "self.error_median=0.0019, self.error_range=0.0052, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3988827168941498; loose: 0.37836897373199463\n",
      "Test data rec score: tight: 1.7502732276916504; loose: 0.8408181667327881\n",
      "Test data average rec_m: -0.41328632831573486; average compct_m: -1.177917718887329\n",
      "Dependency: 0.3796244263648987; Cluster: 0.1994210034608841; Local: 0.2098960429430008; Global: 0.21105852723121643;\n",
      "Current experiment parameters: ('38_thyroid', 0.0, 2), model: Customized, metrics: {'aucroc': 0.7998188405797102, 'aucpr': 0.1057891990389468}, fitting time: 103.94648742675781, inference time: 0.006981372833251953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [2:03:30, 113.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 12.269516\n",
      "epoch : 1000/5000, loss = 0.007947\n",
      "epoch : 2000/5000, loss = 0.006370\n",
      "epoch : 3000/5000, loss = 0.003904\n",
      "epoch : 4000/5000, loss = 0.003536\n",
      "epoch : 5000/5000, loss = 0.003563\n",
      "self.error_median=0.0007, self.error_range=0.0018, self.latent_error_median_=0.0002, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 86.986199\n",
      "epoch : 1000/5000, loss = 0.001354\n",
      "epoch : 2000/5000, loss = 0.004594\n",
      "epoch : 3000/5000, loss = 0.001135\n",
      "epoch : 4000/5000, loss = 0.001894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [2:05:14, 110.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001087\n",
      "self.error_median=0.0027, self.error_range=0.0067, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7813664674758911; loose: 0.32732370495796204\n",
      "Test data rec score: tight: 2.1588222980499268; loose: 0.8283358216285706\n",
      "Test data average rec_m: 2.5136334896087646; average compct_m: -0.37522774934768677\n",
      "Dependency: 0.3532581627368927; Cluster: 0.23246827721595764; Local: 0.16645772755146027; Global: 0.2478158175945282;\n",
      "Current experiment parameters: ('38_thyroid', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8203610248447205, 'aucpr': 0.11647390514034021}, fitting time: 103.72546100616455, inference time: 0.00698089599609375\n",
      "{'Samples': 4207, 'Features': 50, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 24.491681\n",
      "epoch : 1000/5000, loss = 0.057099\n",
      "epoch : 2000/5000, loss = 0.033641\n",
      "epoch : 3000/5000, loss = 0.022844\n",
      "epoch : 4000/5000, loss = 0.021121\n",
      "epoch : 5000/5000, loss = 0.017686\n",
      "self.error_median=0.0079, self.error_range=0.0181, self.latent_error_median_=0.0002, self.latent_error_range_=0.0010\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 87.859934\n",
      "epoch : 1000/5000, loss = 0.011813\n",
      "epoch : 2000/5000, loss = 0.011850\n",
      "epoch : 3000/5000, loss = 0.008316\n",
      "epoch : 4000/5000, loss = 0.006431\n",
      "epoch : 5000/5000, loss = 0.005989\n",
      "self.error_median=0.0181, self.error_range=0.0575, self.latent_error_median_=0.0003, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.4821114540100098; loose: 0.9294885993003845\n",
      "Test data rec score: tight: 3.0536890029907227; loose: 1.069966435432434\n",
      "Test data average rec_m: -0.35179436206817627; average compct_m: -0.41916128993034363\n",
      "Dependency: 0.4105996787548065; Cluster: 0.20262576639652252; Local: 0.1934594064950943; Global: 0.19331516325473785;\n",
      "Current experiment parameters: ('35_SpamBase', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8718238283455675, 'aucpr': 0.7271202086099666}, fitting time: 104.85776829719543, inference time: 0.007978677749633789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [2:06:59, 109.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 4207, 'Features': 50, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 22.059434\n",
      "epoch : 1000/5000, loss = 0.076586\n",
      "epoch : 2000/5000, loss = 0.043705\n",
      "epoch : 3000/5000, loss = 0.033790\n",
      "epoch : 4000/5000, loss = 0.028233\n",
      "epoch : 5000/5000, loss = 0.025361\n",
      "self.error_median=0.0132, self.error_range=0.0195, self.latent_error_median_=0.0008, self.latent_error_range_=0.0016\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.989168\n",
      "epoch : 1000/5000, loss = 0.014596\n",
      "epoch : 2000/5000, loss = 0.013409\n",
      "epoch : 3000/5000, loss = 0.010831\n",
      "epoch : 4000/5000, loss = 0.007456\n",
      "epoch : 5000/5000, loss = 0.007513\n",
      "self.error_median=0.0321, self.error_range=0.0627, self.latent_error_median_=0.0006, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.979393720626831; loose: 0.3825478255748749\n",
      "Test data rec score: tight: 3.526611089706421; loose: 1.218901515007019\n",
      "Test data average rec_m: -0.8396499752998352; average compct_m: 2.307471752166748\n",
      "Dependency: 0.4847278594970703; Cluster: 0.16766124963760376; Local: 0.1539791375398636; Global: 0.19363169372081757;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [2:08:44, 107.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('35_SpamBase', 0.0, 2), model: Customized, metrics: {'aucroc': 0.7768915866741953, 'aucpr': 0.6525931258816966}, fitting time: 104.44388365745544, inference time: 0.007977724075317383\n",
      "{'Samples': 4207, 'Features': 50, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 25.701384\n",
      "epoch : 1000/5000, loss = 0.068913\n",
      "epoch : 2000/5000, loss = 0.046075\n",
      "epoch : 3000/5000, loss = 0.035085\n",
      "epoch : 4000/5000, loss = 0.028450\n",
      "epoch : 5000/5000, loss = 0.023654\n",
      "self.error_median=0.0080, self.error_range=0.0282, self.latent_error_median_=0.0003, self.latent_error_range_=0.0010\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 99.346751\n",
      "epoch : 1000/5000, loss = 0.010734\n",
      "epoch : 2000/5000, loss = 0.008132\n",
      "epoch : 3000/5000, loss = 0.021459\n",
      "epoch : 4000/5000, loss = 0.006289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [2:10:31, 107.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.005469\n",
      "self.error_median=0.0185, self.error_range=0.0565, self.latent_error_median_=0.0002, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7675051689147949; loose: 0.9599241018295288\n",
      "Test data rec score: tight: 2.3760766983032227; loose: 1.7263529300689697\n",
      "Test data average rec_m: -1.6684473752975464; average compct_m: -0.05947647616267204\n",
      "Dependency: 0.36488527059555054; Cluster: 0.23973311483860016; Local: 0.17663535475730896; Global: 0.21874625980854034;\n",
      "Current experiment parameters: ('35_SpamBase', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8556397306397306, 'aucpr': 0.6940530641781432}, fitting time: 106.830491065979, inference time: 0.00698089599609375\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 7.078909\n",
      "epoch : 1000/5000, loss = 0.010610\n",
      "epoch : 2000/5000, loss = 0.009283\n",
      "epoch : 3000/5000, loss = 0.008217\n",
      "epoch : 4000/5000, loss = 0.008133\n",
      "epoch : 5000/5000, loss = 0.006918\n",
      "self.error_median=0.0032, self.error_range=0.0062, self.latent_error_median_=0.0002, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 51.073654\n",
      "epoch : 1000/5000, loss = 0.003722\n",
      "epoch : 2000/5000, loss = 0.003018\n",
      "epoch : 3000/5000, loss = 0.002182\n",
      "epoch : 4000/5000, loss = 0.001857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [2:12:18, 107.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.004470\n",
      "self.error_median=0.0065, self.error_range=0.0137, self.latent_error_median_=0.0007, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 2. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.702488899230957; loose: 0.5100762248039246\n",
      "Test data rec score: tight: 1.2040995359420776; loose: 0.5896095633506775\n",
      "Test data average rec_m: 0.15376321971416473; average compct_m: 0.5052540302276611\n",
      "Dependency: 0.35123348236083984; Cluster: 0.23422548174858093; Local: 0.20460335910320282; Global: 0.2099377065896988;\n",
      "Current experiment parameters: ('44_Wilt', 0.0, 1), model: Customized, metrics: {'aucroc': 0.629485926783224, 'aucpr': 0.08465131537547599}, fitting time: 106.58976650238037, inference time: 0.006970405578613281\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.570770\n",
      "epoch : 1000/5000, loss = 0.011161\n",
      "epoch : 2000/5000, loss = 0.007714\n",
      "epoch : 3000/5000, loss = 0.008844\n",
      "epoch : 4000/5000, loss = 0.006342\n",
      "epoch : 5000/5000, loss = 0.006366\n",
      "self.error_median=0.0027, self.error_range=0.0054, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 74.964149\n",
      "epoch : 1000/5000, loss = 0.002494\n",
      "epoch : 2000/5000, loss = 0.007775\n",
      "epoch : 3000/5000, loss = 0.001603\n",
      "epoch : 4000/5000, loss = 0.003977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [2:14:04, 107.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.000991\n",
      "self.error_median=0.0067, self.error_range=0.0145, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 2. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7269133925437927; loose: 0.5503152012825012\n",
      "Test data rec score: tight: 1.205364465713501; loose: 0.5095781683921814\n",
      "Test data average rec_m: 0.8316304683685303; average compct_m: 1.2981337308883667\n",
      "Dependency: 0.29821890592575073; Cluster: 0.2765023410320282; Local: 0.1826123744249344; Global: 0.2426663488149643;\n",
      "Current experiment parameters: ('44_Wilt', 0.0, 2), model: Customized, metrics: {'aucroc': 0.5592858565831539, 'aucpr': 0.06690441314048791}, fitting time: 106.50186252593994, inference time: 0.00698089599609375\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 7.524284\n",
      "epoch : 1000/5000, loss = 0.007618\n",
      "epoch : 2000/5000, loss = 0.006537\n",
      "epoch : 3000/5000, loss = 0.005653\n",
      "epoch : 4000/5000, loss = 0.005904\n",
      "epoch : 5000/5000, loss = 0.005282\n",
      "self.error_median=0.0026, self.error_range=0.0065, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 63.534311\n",
      "epoch : 1000/5000, loss = 0.002566\n",
      "epoch : 2000/5000, loss = 0.002297\n",
      "epoch : 3000/5000, loss = 0.003330\n",
      "epoch : 4000/5000, loss = 0.003018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [2:15:51, 107.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002306\n",
      "self.error_median=0.0048, self.error_range=0.0131, self.latent_error_median_=0.0002, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 2. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.4586997032165527; loose: 1.3513648509979248\n",
      "Test data rec score: tight: 0.8489867448806763; loose: 0.503678560256958\n",
      "Test data average rec_m: 5.158740043640137; average compct_m: 0.547311544418335\n",
      "Dependency: 0.248416468501091; Cluster: 0.3108944594860077; Local: 0.18768148124217987; Global: 0.2530076205730438;\n",
      "Current experiment parameters: ('44_Wilt', 0.0, 3), model: Customized, metrics: {'aucroc': 0.6286795746255205, 'aucpr': 0.07174477605877104}, fitting time: 106.46531629562378, inference time: 0.00597381591796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "76it [2:16:43, 90.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [2:17:36, 79.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [2:18:28, 70.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [2:24:20, 155.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [2:26:14, 143.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.956929\n",
      "epoch : 1000/5000, loss = 0.010369\n",
      "epoch : 2000/5000, loss = 0.008044\n",
      "epoch : 3000/5000, loss = 0.007433\n",
      "epoch : 4000/5000, loss = 0.006357\n",
      "epoch : 5000/5000, loss = 0.007015\n",
      "self.error_median=0.0029, self.error_range=0.0064, self.latent_error_median_=0.0003, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 94.962359\n",
      "epoch : 1000/5000, loss = 0.003154\n",
      "epoch : 2000/5000, loss = 0.002212\n",
      "epoch : 3000/5000, loss = 0.001593\n",
      "epoch : 4000/5000, loss = 0.001687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [2:28:03, 132.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.001324\n",
      "self.error_median=0.0042, self.error_range=0.0125, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4287889301776886; loose: 0.843761146068573\n",
      "Test data rec score: tight: 1.3190315961837769; loose: 0.7388314008712769\n",
      "Test data average rec_m: -2.661884307861328; average compct_m: -0.9776216149330139\n",
      "Dependency: 0.319137841463089; Cluster: 0.23679162561893463; Local: 0.1958964616060257; Global: 0.2481740564107895;\n",
      "Current experiment parameters: ('27_PageBlocks', 0.0, 3), model: Customized, metrics: {'aucroc': 0.7000736130629726, 'aucpr': 0.17108743563569187}, fitting time: 108.56045341491699, inference time: 0.00698089599609375\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 31.183845\n",
      "epoch : 1000/5000, loss = 0.059543\n",
      "epoch : 2000/5000, loss = 0.049614\n",
      "epoch : 3000/5000, loss = 0.041985\n",
      "epoch : 4000/5000, loss = 0.033236\n",
      "epoch : 5000/5000, loss = 0.027153\n",
      "self.error_median=0.0101, self.error_range=0.0312, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 55.368065\n",
      "epoch : 1000/5000, loss = 0.005578\n",
      "epoch : 2000/5000, loss = 0.004937\n",
      "epoch : 3000/5000, loss = 0.004487\n",
      "epoch : 4000/5000, loss = 0.003993\n",
      "epoch : 5000/5000, loss = 0.003542\n",
      "self.error_median=0.0135, self.error_range=0.0456, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3178246021270752; loose: 0.24840062856674194\n",
      "Test data rec score: tight: 1.2266225814819336; loose: 0.6511783599853516\n",
      "Test data average rec_m: -8.874570846557617; average compct_m: 0.27163422107696533\n",
      "Dependency: 0.37047794461250305; Cluster: 0.2142719179391861; Local: 0.22050327062606812; Global: 0.19474689662456512;\n",
      "Current experiment parameters: ('31_satimage-2', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9996677740863787, 'aucpr': 0.9660743865893091}, fitting time: 112.4675886631012, inference time: 0.009973526000976562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [2:29:56, 126.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 32.856961\n",
      "epoch : 1000/5000, loss = 0.091361\n",
      "epoch : 2000/5000, loss = 0.063736\n",
      "epoch : 3000/5000, loss = 0.059083\n",
      "epoch : 4000/5000, loss = 0.049006\n",
      "epoch : 5000/5000, loss = 0.045487\n",
      "self.error_median=0.0159, self.error_range=0.0506, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 58.571089\n",
      "epoch : 1000/5000, loss = 0.006642\n",
      "epoch : 2000/5000, loss = 0.005552\n",
      "epoch : 3000/5000, loss = 0.004871\n",
      "epoch : 4000/5000, loss = 0.004614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [2:31:49, 122.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003942\n",
      "self.error_median=0.0153, self.error_range=0.0501, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.24608707427978516; loose: 0.2956398129463196\n",
      "Test data rec score: tight: 0.9006035327911377; loose: 0.6902150511741638\n",
      "Test data average rec_m: 0.25135526061058044; average compct_m: 0.4080890417098999\n",
      "Dependency: 0.17032559216022491; Cluster: 0.35470661520957947; Local: 0.1149575412273407; Global: 0.3600102365016937;\n",
      "Current experiment parameters: ('31_satimage-2', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9979789590254706, 'aucpr': 0.7050555830926675}, fitting time: 113.11393594741821, inference time: 0.010970830917358398\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 34.698665\n",
      "epoch : 1000/5000, loss = 0.060587\n",
      "epoch : 2000/5000, loss = 0.049292\n",
      "epoch : 3000/5000, loss = 0.040041\n",
      "epoch : 4000/5000, loss = 0.029966\n",
      "epoch : 5000/5000, loss = 0.023306\n",
      "self.error_median=0.0095, self.error_range=0.0289, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 58.177726\n",
      "epoch : 1000/5000, loss = 0.006561\n",
      "epoch : 2000/5000, loss = 0.005755\n",
      "epoch : 3000/5000, loss = 0.004748\n",
      "epoch : 4000/5000, loss = 0.004015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [2:33:43, 119.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003805\n",
      "self.error_median=0.0144, self.error_range=0.0497, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.330021470785141; loose: 0.2624606490135193\n",
      "Test data rec score: tight: 1.4562451839447021; loose: 0.6091659069061279\n",
      "Test data average rec_m: 7.577391624450684; average compct_m: 0.0609014555811882\n",
      "Dependency: 0.2831544280052185; Cluster: 0.2888921797275543; Local: 0.16797758638858795; Global: 0.259975790977478;\n",
      "Current experiment parameters: ('31_satimage-2', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9999169435215948, 'aucpr': 0.9931869717584005}, fitting time: 113.13895845413208, inference time: 0.006981611251831055\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 30.750890\n",
      "epoch : 1000/5000, loss = 0.334330\n",
      "epoch : 2000/5000, loss = 0.256491\n",
      "epoch : 3000/5000, loss = 0.195031\n",
      "epoch : 4000/5000, loss = 0.176669\n",
      "epoch : 5000/5000, loss = 0.160874\n",
      "self.error_median=0.0389, self.error_range=0.3600, self.latent_error_median_=0.0004, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 64.617675\n",
      "epoch : 1000/5000, loss = 0.023433\n",
      "epoch : 2000/5000, loss = 0.021424\n",
      "epoch : 3000/5000, loss = 0.021204\n",
      "epoch : 4000/5000, loss = 0.020207\n",
      "epoch : 5000/5000, loss = 0.019644\n",
      "self.error_median=0.0416, self.error_range=0.4584, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.33167654275894165; loose: 0.10371749103069305\n",
      "Test data rec score: tight: 0.7390878796577454; loose: 0.4171954393386841\n",
      "Test data average rec_m: -1.599597692489624; average compct_m: -0.5972831845283508\n",
      "Dependency: 0.2225271612405777; Cluster: 0.38782593607902527; Local: 0.054727084934711456; Global: 0.334919810295105;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [2:35:42, 119.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('30_satellite', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9299447006893816, 'aucpr': 0.7745069876920709}, fitting time: 119.35586404800415, inference time: 0.0069904327392578125\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 27.866023\n",
      "epoch : 1000/5000, loss = 0.380730\n",
      "epoch : 2000/5000, loss = 0.260786\n",
      "epoch : 3000/5000, loss = 0.212548\n",
      "epoch : 4000/5000, loss = 0.187060\n",
      "epoch : 5000/5000, loss = 0.164700\n",
      "self.error_median=0.0422, self.error_range=0.3754, self.latent_error_median_=0.0006, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 69.727122\n",
      "epoch : 1000/5000, loss = 0.030292\n",
      "epoch : 2000/5000, loss = 0.023141\n",
      "epoch : 3000/5000, loss = 0.021694\n",
      "epoch : 4000/5000, loss = 0.020393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [2:37:42, 119.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.019111\n",
      "self.error_median=0.0444, self.error_range=0.4375, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2863330841064453; loose: 0.10042832046747208\n",
      "Test data rec score: tight: 0.7518532276153564; loose: 0.48958858847618103\n",
      "Test data average rec_m: 0.0010009639663621783; average compct_m: -0.22703507542610168\n",
      "Dependency: 0.23699720203876495; Cluster: 0.3562937378883362; Local: 0.05984276533126831; Global: 0.34686633944511414;\n",
      "Current experiment parameters: ('30_satellite', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9195977781084166, 'aucpr': 0.7380199205902558}, fitting time: 119.2450180053711, inference time: 0.00698089599609375\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 32.028522\n",
      "epoch : 1000/5000, loss = 0.344732\n",
      "epoch : 2000/5000, loss = 0.250748\n",
      "epoch : 3000/5000, loss = 0.201535\n",
      "epoch : 4000/5000, loss = 0.191307\n",
      "epoch : 5000/5000, loss = 0.165938\n",
      "self.error_median=0.0347, self.error_range=0.3879, self.latent_error_median_=0.0004, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 64.642557\n",
      "epoch : 1000/5000, loss = 0.026915\n",
      "epoch : 2000/5000, loss = 0.021875\n",
      "epoch : 3000/5000, loss = 0.020882\n",
      "epoch : 4000/5000, loss = 0.019695\n",
      "epoch : 5000/5000, loss = 0.019052\n",
      "self.error_median=0.0455, self.error_range=0.4359, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.39812806248664856; loose: 0.14651143550872803\n",
      "Test data rec score: tight: 0.6703584790229797; loose: 0.4881962537765503\n",
      "Test data average rec_m: -0.17183101177215576; average compct_m: -4.242564678192139\n",
      "Dependency: 0.17889147996902466; Cluster: 0.40315908193588257; Local: 0.052236925810575485; Global: 0.36571255326271057;\n",
      "Current experiment parameters: ('30_satellite', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9234067351088626, 'aucpr': 0.7250290415854799}, fitting time: 119.59927248954773, inference time: 0.005983829498291016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [2:39:42, 119.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 30.209744\n",
      "epoch : 1000/5000, loss = 0.244960\n",
      "epoch : 2000/5000, loss = 0.174290\n",
      "epoch : 3000/5000, loss = 0.138024\n",
      "epoch : 4000/5000, loss = 0.117139\n",
      "epoch : 5000/5000, loss = 0.098868\n",
      "self.error_median=0.0222, self.error_range=0.1926, self.latent_error_median_=0.0003, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 65.137983\n",
      "epoch : 1000/5000, loss = 0.019132\n",
      "epoch : 2000/5000, loss = 0.016522\n",
      "epoch : 3000/5000, loss = 0.016841\n",
      "epoch : 4000/5000, loss = 0.013931\n",
      "epoch : 5000/5000, loss = 0.013121\n",
      "self.error_median=0.0231, self.error_range=0.2727, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.36835694313049316; loose: 0.24021364748477936\n",
      "Test data rec score: tight: 1.5296189785003662; loose: 0.6882319450378418\n",
      "Test data average rec_m: -0.34778496623039246; average compct_m: -1.757736086845398\n",
      "Dependency: 0.2626749277114868; Cluster: 0.3606410622596741; Local: 0.07500337064266205; Global: 0.30168062448501587;\n",
      "Current experiment parameters: ('19_landsat', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9888047028086219, 'aucpr': 0.9389340901447031}, fitting time: 119.35972166061401, inference time: 0.006959199905395508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [2:41:41, 119.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 30.364247\n",
      "epoch : 1000/5000, loss = 0.265619\n",
      "epoch : 2000/5000, loss = 0.186966\n",
      "epoch : 3000/5000, loss = 0.133116\n",
      "epoch : 4000/5000, loss = 0.105443\n",
      "epoch : 5000/5000, loss = 0.081618\n",
      "self.error_median=0.0194, self.error_range=0.1447, self.latent_error_median_=0.0005, self.latent_error_range_=0.0014\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 71.236038\n",
      "epoch : 1000/5000, loss = 0.023631\n",
      "epoch : 2000/5000, loss = 0.018620\n",
      "epoch : 3000/5000, loss = 0.017512\n",
      "epoch : 4000/5000, loss = 0.016030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [2:43:41, 119.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.015030\n",
      "self.error_median=0.0229, self.error_range=0.3268, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3355874717235565; loose: 0.24823257327079773\n",
      "Test data rec score: tight: 2.2203314304351807; loose: 0.560612678527832\n",
      "Test data average rec_m: -2.6203176975250244; average compct_m: -0.46360522508621216\n",
      "Dependency: 0.2697741985321045; Cluster: 0.3547496199607849; Local: 0.06430019438266754; Global: 0.31117600202560425;\n",
      "Current experiment parameters: ('19_landsat', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9960385369039844, 'aucpr': 0.9816000706472663}, fitting time: 119.29405760765076, inference time: 0.006974697113037109\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 31.530027\n",
      "epoch : 1000/5000, loss = 0.224966\n",
      "epoch : 2000/5000, loss = 0.136956\n",
      "epoch : 3000/5000, loss = 0.104848\n",
      "epoch : 4000/5000, loss = 0.090947\n",
      "epoch : 5000/5000, loss = 0.080141\n",
      "self.error_median=0.0237, self.error_range=0.1472, self.latent_error_median_=0.0004, self.latent_error_range_=0.0012\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 66.888438\n",
      "epoch : 1000/5000, loss = 0.023267\n",
      "epoch : 2000/5000, loss = 0.018408\n",
      "epoch : 3000/5000, loss = 0.017056\n",
      "epoch : 4000/5000, loss = 0.015880\n",
      "epoch : 5000/5000, loss = 0.014646\n",
      "self.error_median=0.0242, self.error_range=0.2979, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4266475737094879; loose: 0.30779844522476196\n",
      "Test data rec score: tight: 2.6441197395324707; loose: 0.6440432071685791\n",
      "Test data average rec_m: -3.3477606773376465; average compct_m: -0.33476242423057556\n",
      "Dependency: 0.3590891361236572; Cluster: 0.29140713810920715; Local: 0.13875694572925568; Global: 0.21074682474136353;\n",
      "Current experiment parameters: ('19_landsat', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9975473546701502, 'aucpr': 0.9899638109220918}, fitting time: 119.24805545806885, inference time: 0.010969877243041992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [2:45:40, 119.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 15.804272\n",
      "epoch : 1000/5000, loss = 0.038326\n",
      "epoch : 2000/5000, loss = 0.017630\n",
      "epoch : 3000/5000, loss = 0.014295\n",
      "epoch : 4000/5000, loss = 0.011238\n",
      "epoch : 5000/5000, loss = 0.008227\n",
      "self.error_median=0.0042, self.error_range=0.0069, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 61.215951\n",
      "epoch : 1000/5000, loss = 0.004891\n",
      "epoch : 2000/5000, loss = 0.003238\n",
      "epoch : 3000/5000, loss = 0.002533\n",
      "epoch : 4000/5000, loss = 0.002137\n",
      "epoch : 5000/5000, loss = 0.001925\n",
      "self.error_median=0.0091, self.error_range=0.0189, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 8. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.2882953882217407; loose: 0.26410719752311707\n",
      "Test data rec score: tight: 3.1107993125915527; loose: 1.2331122159957886\n",
      "Test data average rec_m: -0.7670994400978088; average compct_m: -0.12343694269657135\n",
      "Dependency: 0.4474159777164459; Cluster: 0.16209916770458221; Local: 0.22792017459869385; Global: 0.1625646948814392;\n",
      "Current experiment parameters: ('28_pendigits', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9998309704409558, 'aucpr': 0.9922690949151621}, fitting time: 121.32220888137817, inference time: 0.006969928741455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [2:47:42, 120.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 17.697326\n",
      "epoch : 1000/5000, loss = 0.028760\n",
      "epoch : 2000/5000, loss = 0.014834\n",
      "epoch : 3000/5000, loss = 0.013189\n",
      "epoch : 4000/5000, loss = 0.010302\n",
      "epoch : 5000/5000, loss = 0.007653\n",
      "self.error_median=0.0047, self.error_range=0.0060, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.464120\n",
      "epoch : 1000/5000, loss = 0.005708\n",
      "epoch : 2000/5000, loss = 0.004243\n",
      "epoch : 3000/5000, loss = 0.003068\n",
      "epoch : 4000/5000, loss = 0.002563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [2:49:43, 120.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.002021\n",
      "self.error_median=0.0102, self.error_range=0.0183, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 8. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.24790959060192108; loose: 0.3322148025035858\n",
      "Test data rec score: tight: 3.729412794113159; loose: 1.2733625173568726\n",
      "Test data average rec_m: 0.20614057779312134; average compct_m: 1.027037501335144\n",
      "Dependency: 0.35765665769577026; Cluster: 0.23655416071414948; Local: 0.18319207429885864; Global: 0.2225971221923828;\n",
      "Current experiment parameters: ('28_pendigits', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9999894356525597, 'aucpr': 0.9995567375886526}, fitting time: 121.17910575866699, inference time: 0.00598454475402832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "93it [3:03:37, 334.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.401521\n",
      "epoch : 1000/5000, loss = 0.005273\n",
      "epoch : 2000/5000, loss = 0.002915\n",
      "epoch : 3000/5000, loss = 0.002584\n",
      "epoch : 4000/5000, loss = 0.002676\n",
      "epoch : 5000/5000, loss = 0.002234\n",
      "self.error_median=0.0005, self.error_range=0.0013, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 62.621022\n",
      "epoch : 1000/5000, loss = 0.006232\n",
      "epoch : 2000/5000, loss = 0.000603\n",
      "epoch : 3000/5000, loss = 0.000439\n",
      "epoch : 4000/5000, loss = 0.000410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [3:05:36, 269.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.000340\n",
      "self.error_median=0.0010, self.error_range=0.0025, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4163484275341034; loose: 0.36167213320732117\n",
      "Test data rec score: tight: 2.9568424224853516; loose: 1.5518900156021118\n",
      "Test data average rec_m: -0.7411339282989502; average compct_m: -1.1392822265625\n",
      "Dependency: 0.34564101696014404; Cluster: 0.22056548297405243; Local: 0.20329518616199493; Global: 0.2304983139038086;\n",
      "Current experiment parameters: ('2_annthyroid', 0.0, 1), model: Customized, metrics: {'aucroc': 0.854328125, 'aucpr': 0.3399109053004386}, fitting time: 119.28808212280273, inference time: 0.006989002227783203\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 6.487876\n",
      "epoch : 1000/5000, loss = 0.003498\n",
      "epoch : 2000/5000, loss = 0.003009\n",
      "epoch : 3000/5000, loss = 0.001982\n",
      "epoch : 4000/5000, loss = 0.002320\n",
      "epoch : 5000/5000, loss = 0.001669\n",
      "self.error_median=0.0004, self.error_range=0.0013, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 59.039967\n",
      "epoch : 1000/5000, loss = 0.001793\n",
      "epoch : 2000/5000, loss = 0.000633\n",
      "epoch : 3000/5000, loss = 0.000658\n",
      "epoch : 4000/5000, loss = 0.000475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [3:07:38, 225.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.000653\n",
      "self.error_median=0.0011, self.error_range=0.0029, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.32438215613365173; loose: 0.5030764937400818\n",
      "Test data rec score: tight: 1.9637138843536377; loose: 1.0669513940811157\n",
      "Test data average rec_m: 0.004188389051705599; average compct_m: -8.986371994018555\n",
      "Dependency: 0.3173547089099884; Cluster: 0.23102465271949768; Local: 0.21734412014484406; Global: 0.23427648842334747;\n",
      "Current experiment parameters: ('2_annthyroid', 0.0, 2), model: Customized, metrics: {'aucroc': 0.85545625, 'aucpr': 0.3077108521851757}, fitting time: 122.1666042804718, inference time: 0.0069811344146728516\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.902265\n",
      "epoch : 1000/5000, loss = 0.002616\n",
      "epoch : 2000/5000, loss = 0.002258\n",
      "epoch : 3000/5000, loss = 0.001549\n",
      "epoch : 4000/5000, loss = 0.001379\n",
      "epoch : 5000/5000, loss = 0.001339\n",
      "self.error_median=0.0003, self.error_range=0.0008, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 80.341716\n",
      "epoch : 1000/5000, loss = 0.001024\n",
      "epoch : 2000/5000, loss = 0.000798\n",
      "epoch : 3000/5000, loss = 0.000493\n",
      "epoch : 4000/5000, loss = 0.000402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [3:09:43, 195.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.000467\n",
      "self.error_median=0.0008, self.error_range=0.0019, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.26504701375961304; loose: 0.3468482792377472\n",
      "Test data rec score: tight: 1.6647381782531738; loose: 1.2146884202957153\n",
      "Test data average rec_m: 2.0629074573516846; average compct_m: 1.395811915397644\n",
      "Dependency: 0.34260571002960205; Cluster: 0.21746239066123962; Local: 0.21010956168174744; Global: 0.2298223078250885;\n",
      "Current experiment parameters: ('2_annthyroid', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8377718749999999, 'aucpr': 0.3523807430523549}, fitting time: 124.18204283714294, inference time: 0.006981849670410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "97it [3:10:42, 154.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [3:11:32, 123.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [3:12:37, 105.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n",
      "{'Samples': 10000, 'Features': 50, 'Anomalies': 1134, 'Anomalies Ratio(%)': 11.34}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 28.221577\n",
      "epoch : 1000/5000, loss = 0.178781\n",
      "epoch : 2000/5000, loss = 0.114044\n",
      "epoch : 3000/5000, loss = 0.089998\n",
      "epoch : 4000/5000, loss = 0.069930\n",
      "epoch : 5000/5000, loss = 0.067004\n",
      "self.error_median=0.0281, self.error_range=0.0469, self.latent_error_median_=0.0005, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.697318\n",
      "epoch : 1000/5000, loss = 0.026249\n",
      "epoch : 2000/5000, loss = 0.017576\n",
      "epoch : 3000/5000, loss = 0.013847\n",
      "epoch : 4000/5000, loss = 0.011887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [3:14:48, 113.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.010628\n",
      "self.error_median=0.0384, self.error_range=0.0958, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4640395939350128; loose: 0.276826947927475\n",
      "Test data rec score: tight: 4.335773468017578; loose: 2.040600299835205\n",
      "Test data average rec_m: -4.2823662757873535; average compct_m: -0.5995486378669739\n",
      "Dependency: 0.4804975986480713; Cluster: 0.14616534113883972; Local: 0.1985422819852829; Global: 0.1747947633266449;\n",
      "Current experiment parameters: ('5_campaign', 0.0, 1), model: Customized, metrics: {'aucroc': 0.954514595311809, 'aucpr': 0.7392474660556151}, fitting time: 130.37209725379944, inference time: 0.007950305938720703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "101it [3:16:35, 111.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\frank.py:139: RuntimeWarning: overflow encountered in multiply\n",
      "  num = self._g(U) * self._g(V) + self._g(U)\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\frank.py:140: RuntimeWarning: overflow encountered in multiply\n",
      "  den = self._g(U) * self._g(V) + self._g(1)\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\frank.py:141: RuntimeWarning: invalid value encountered in divide\n",
      "  return num / den\n",
      "102it [3:18:31, 112.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Unable to compute tau.\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 97, 'Anomalies Ratio(%)': 0.97}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.257097\n",
      "epoch : 1000/5000, loss = 0.007693\n",
      "epoch : 2000/5000, loss = 0.005101\n",
      "epoch : 3000/5000, loss = 0.003368\n",
      "epoch : 4000/5000, loss = 0.002631\n",
      "epoch : 5000/5000, loss = 0.001571\n",
      "self.error_median=0.0007, self.error_range=0.0011, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 54.434704\n",
      "epoch : 1000/5000, loss = 0.001463\n",
      "epoch : 2000/5000, loss = 0.000871\n",
      "epoch : 3000/5000, loss = 0.000673\n",
      "epoch : 4000/5000, loss = 0.000571\n",
      "epoch : 5000/5000, loss = 0.000495\n",
      "self.error_median=0.0016, self.error_range=0.0033, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.26491278409957886; loose: 0.32526546716690063\n",
      "Test data rec score: tight: 2.4500720500946045; loose: 1.1697473526000977\n",
      "Test data average rec_m: -1.2842531204223633; average compct_m: -1.449739933013916\n",
      "Dependency: 0.42333483695983887; Cluster: 0.17179308831691742; Local: 0.2264442890882492; Global: 0.1784277707338333;\n",
      "Current experiment parameters: ('10_cover', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9985143745865204, 'aucpr': 0.8779872587578559}, fitting time: 136.72466039657593, inference time: 0.005039691925048828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [3:20:48, 119.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 116, 'Anomalies Ratio(%)': 1.16}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 12.265694\n",
      "epoch : 1000/5000, loss = 0.010284\n",
      "epoch : 2000/5000, loss = 0.004231\n",
      "epoch : 3000/5000, loss = 0.002299\n",
      "epoch : 4000/5000, loss = 0.005678\n",
      "epoch : 5000/5000, loss = 0.001968\n",
      "self.error_median=0.0013, self.error_range=0.0013, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 89.880446\n",
      "epoch : 1000/5000, loss = 0.001927\n",
      "epoch : 2000/5000, loss = 0.000965\n",
      "epoch : 3000/5000, loss = 0.000612\n",
      "epoch : 4000/5000, loss = 0.000485\n",
      "epoch : 5000/5000, loss = 0.000435\n",
      "self.error_median=0.0016, self.error_range=0.0028, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3198142945766449; loose: 0.3527171313762665\n",
      "Test data rec score: tight: 1.3820743560791016; loose: 1.2101869583129883\n",
      "Test data average rec_m: 1.8181731700897217; average compct_m: 0.4057359993457794\n",
      "Dependency: 0.3254459500312805; Cluster: 0.1813807189464569; Local: 0.37536856532096863; Global: 0.11780476570129395;\n",
      "Current experiment parameters: ('10_cover', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9986991086485184, 'aucpr': 0.909474140871145}, fitting time: 134.1903920173645, inference time: 0.006971597671508789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [3:23:02, 124.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 95, 'Anomalies Ratio(%)': 0.95}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 12.852583\n",
      "epoch : 1000/5000, loss = 0.007210\n",
      "epoch : 2000/5000, loss = 0.003443\n",
      "epoch : 3000/5000, loss = 0.002789\n",
      "epoch : 4000/5000, loss = 0.002311\n",
      "epoch : 5000/5000, loss = 0.001895\n",
      "self.error_median=0.0008, self.error_range=0.0013, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 78.139734\n",
      "epoch : 1000/5000, loss = 0.001430\n",
      "epoch : 2000/5000, loss = 0.001160\n",
      "epoch : 3000/5000, loss = 0.000743\n",
      "epoch : 4000/5000, loss = 0.000616\n",
      "epoch : 5000/5000, loss = 0.000527\n",
      "self.error_median=0.0018, self.error_range=0.0035, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.37627875804901123; loose: 0.44298574328422546\n",
      "Test data rec score: tight: 2.1922054290771484; loose: 1.3818552494049072\n",
      "Test data average rec_m: 0.3141677975654602; average compct_m: -1.1324535608291626\n",
      "Dependency: 0.4193902015686035; Cluster: 0.1749075949192047; Local: 0.2182016521692276; Global: 0.1875005066394806;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [3:25:20, 128.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('10_cover', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9972481253605077, 'aucpr': 0.6439909087697447}, fitting time: 138.08806157112122, inference time: 0.007977008819580078\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 2.747536\n",
      "epoch : 1000/5000, loss = 0.008743\n",
      "epoch : 2000/5000, loss = 0.007444\n",
      "epoch : 3000/5000, loss = 0.006277\n",
      "epoch : 4000/5000, loss = 0.006421\n",
      "epoch : 5000/5000, loss = 0.005981\n",
      "self.error_median=0.0019, self.error_range=0.0061, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 48.940393\n",
      "epoch : 1000/5000, loss = 0.001370\n",
      "epoch : 2000/5000, loss = 0.001461\n",
      "epoch : 3000/5000, loss = 0.001932\n",
      "epoch : 4000/5000, loss = 0.000993\n",
      "epoch : 5000/5000, loss = 0.001645\n",
      "self.error_median=0.0028, self.error_range=0.0099, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.2252955436706543; loose: 0.3689458668231964\n",
      "Test data rec score: tight: 0.8798267841339111; loose: 0.9758180975914001\n",
      "Test data average rec_m: 11.178601264953613; average compct_m: -2.29923152923584\n",
      "Dependency: 0.2661578357219696; Cluster: 0.301069974899292; Local: 0.137045219540596; Global: 0.2957269251346588;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [3:30:01, 174.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('34_smtp', 0.0, 1), model: Customized, metrics: {'aucroc': 0.34378126042014, 'aucpr': 0.0005078720162519045}, fitting time: 279.9228711128235, inference time: 0.010186433792114258\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 4.025780\n",
      "epoch : 1000/5000, loss = 0.003736\n",
      "epoch : 2000/5000, loss = 0.003911\n",
      "epoch : 3000/5000, loss = 0.003994\n",
      "epoch : 4000/5000, loss = 0.005530\n",
      "epoch : 5000/5000, loss = 0.004014\n",
      "self.error_median=0.0008, self.error_range=0.0032, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 41.448927\n",
      "epoch : 1000/5000, loss = 0.000526\n",
      "epoch : 2000/5000, loss = 0.000453\n",
      "epoch : 3000/5000, loss = 0.000414\n",
      "epoch : 4000/5000, loss = 0.000407\n",
      "epoch : 5000/5000, loss = 0.000384\n",
      "self.error_median=0.0009, self.error_range=0.0032, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5796226263046265; loose: 0.4841960370540619\n",
      "Test data rec score: tight: 0.9969092607498169; loose: 0.876287579536438\n",
      "Test data average rec_m: 1.0646185874938965; average compct_m: 4.844438552856445\n",
      "Dependency: 0.23897290229797363; Cluster: 0.35037532448768616; Local: 0.12242680042982101; Global: 0.288224995136261;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107it [3:33:30, 184.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('34_smtp', 0.0, 2), model: Customized, metrics: {'aucroc': 0.6108702900966989, 'aucpr': 0.0008561643835616438}, fitting time: 208.6822226047516, inference time: 0.01639580726623535\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 5, 'Anomalies Ratio(%)': 0.05}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 4.862693\n",
      "epoch : 1000/5000, loss = 0.014239\n",
      "epoch : 2000/5000, loss = 0.006678\n",
      "epoch : 3000/5000, loss = 0.007094\n",
      "epoch : 4000/5000, loss = 0.004644\n",
      "epoch : 5000/5000, loss = 0.004449\n",
      "self.error_median=0.0013, self.error_range=0.0040, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 14.045906\n",
      "epoch : 1000/5000, loss = 0.002586\n",
      "epoch : 2000/5000, loss = 0.001730\n",
      "epoch : 3000/5000, loss = 0.001355\n",
      "epoch : 4000/5000, loss = 0.001314\n",
      "epoch : 5000/5000, loss = 0.001073\n",
      "self.error_median=0.0020, self.error_range=0.0065, self.latent_error_median_=0.0003, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5571414232254028; loose: 1.0250169038772583\n",
      "Test data rec score: tight: 0.685210108757019; loose: 0.8912571668624878\n",
      "Test data average rec_m: 1.6251863241195679; average compct_m: 1.279541015625\n",
      "Dependency: 0.17116758227348328; Cluster: 0.3439585864543915; Local: 0.12499991804361343; Global: 0.3598739504814148;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [3:37:00, 192.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('34_smtp', 0.0, 3), model: Customized, metrics: {'aucroc': 0.5383589059372915, 'aucpr': 0.0010777759830765464}, fitting time: 209.77690196037292, inference time: 0.007981300354003906\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2081, 'Anomalies Ratio(%)': 20.81}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 2.390542\n",
      "epoch : 1000/5000, loss = 0.040767\n",
      "epoch : 2000/5000, loss = 0.039586\n",
      "epoch : 3000/5000, loss = 0.025032\n",
      "epoch : 4000/5000, loss = 0.028940\n",
      "epoch : 5000/5000, loss = 0.020991\n",
      "self.error_median=0.0152, self.error_range=0.0428, self.latent_error_median_=0.0000, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 45.282295\n",
      "epoch : 1000/5000, loss = 0.003564\n",
      "epoch : 2000/5000, loss = 0.002973\n",
      "epoch : 3000/5000, loss = 0.002669\n",
      "epoch : 4000/5000, loss = 0.002733\n",
      "epoch : 5000/5000, loss = 0.002298\n",
      "self.error_median=0.0111, self.error_range=0.0290, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7053735256195068; loose: 0.7466831207275391\n",
      "Test data rec score: tight: 0.520352840423584; loose: 0.5276973843574524\n",
      "Test data average rec_m: 3.4839980602264404; average compct_m: 39.138118743896484\n",
      "Dependency: 0.22191543877124786; Cluster: 0.31818321347236633; Local: 0.16074366867542267; Global: 0.29915761947631836;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [3:40:28, 197.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('33_skin', 0.0, 1), model: Customized, metrics: {'aucroc': 0.5550631852715187, 'aucpr': 0.2365857364284384}, fitting time: 208.4365861415863, inference time: 0.007192373275756836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "110it [3:40:39, 141.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Marginal value out of bounds.\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2066, 'Anomalies Ratio(%)': 20.66}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 5.380370\n",
      "epoch : 1000/5000, loss = 0.031356\n",
      "epoch : 2000/5000, loss = 0.027482\n",
      "epoch : 3000/5000, loss = 0.025514\n",
      "epoch : 4000/5000, loss = 0.023731\n",
      "epoch : 5000/5000, loss = 0.027242\n",
      "self.error_median=0.0110, self.error_range=0.0263, self.latent_error_median_=0.0001, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 6.901512\n",
      "epoch : 1000/5000, loss = 0.004422\n",
      "epoch : 2000/5000, loss = 0.003480\n",
      "epoch : 3000/5000, loss = 0.004185\n",
      "epoch : 4000/5000, loss = 0.002800\n",
      "epoch : 5000/5000, loss = 0.003173\n",
      "self.error_median=0.0213, self.error_range=0.0724, self.latent_error_median_=0.0004, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.33875012397766113; loose: 0.8902039527893066\n",
      "Test data rec score: tight: 0.49607881903648376; loose: 0.4166756272315979\n",
      "Test data average rec_m: -3.4766576290130615; average compct_m: -0.2511095404624939\n",
      "Dependency: 0.2927422523498535; Cluster: 0.2806433141231537; Local: 0.20999063551425934; Global: 0.21662379801273346;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [3:44:07, 161.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('33_skin', 0.0, 3), model: Customized, metrics: {'aucroc': 0.5436208999728924, 'aucpr': 0.2287949002677315}, fitting time: 207.83306980133057, inference time: 0.01707935333251953\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 669, 'Anomalies Ratio(%)': 6.69}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 12.012930\n",
      "epoch : 1000/5000, loss = 0.006853\n",
      "epoch : 2000/5000, loss = 0.005341\n",
      "epoch : 3000/5000, loss = 0.004671\n",
      "epoch : 4000/5000, loss = 0.004082\n",
      "epoch : 5000/5000, loss = 0.003536\n",
      "self.error_median=0.0011, self.error_range=0.0031, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 66.241138\n",
      "epoch : 1000/5000, loss = 0.001492\n",
      "epoch : 2000/5000, loss = 0.000895\n",
      "epoch : 3000/5000, loss = 0.000784\n",
      "epoch : 4000/5000, loss = 0.000914\n",
      "epoch : 5000/5000, loss = 0.000622\n",
      "self.error_median=0.0019, self.error_range=0.0061, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7434004545211792; loose: 1.9297890663146973\n",
      "Test data rec score: tight: 17.913936614990234; loose: 8.987079620361328\n",
      "Test data average rec_m: -7.104475021362305; average compct_m: -1.0471076965332031\n",
      "Dependency: 0.7226671576499939; Cluster: 0.038912083953619; Local: 0.13497857749462128; Global: 0.1034422293305397;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [3:47:13, 168.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('32_shuttle', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8646193825442279, 'aucpr': 0.31537490605980206}, fitting time: 185.16367840766907, inference time: 0.010970592498779297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\frank.py:98: RuntimeWarning: divide by zero encountered in log\n",
      "  return -1.0 / self.theta * np.log(1 + num / den)\n",
      "113it [3:47:48, 128.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Marginal value out of bounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [3:48:22, 100.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Marginal value out of bounds.\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 222, 'Anomalies Ratio(%)': 2.22}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 39.867809\n",
      "epoch : 1000/5000, loss = 0.183444\n",
      "epoch : 2000/5000, loss = 0.094494\n",
      "epoch : 3000/5000, loss = 0.073004\n",
      "epoch : 4000/5000, loss = 0.064502\n",
      "epoch : 5000/5000, loss = 0.046174\n",
      "self.error_median=0.0158, self.error_range=0.0305, self.latent_error_median_=0.0003, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 80.289155\n",
      "epoch : 1000/5000, loss = 0.020915\n",
      "epoch : 2000/5000, loss = 0.013850\n",
      "epoch : 3000/5000, loss = 0.010487\n",
      "epoch : 4000/5000, loss = 0.009196\n",
      "epoch : 5000/5000, loss = 0.007898\n",
      "self.error_median=0.0154, self.error_range=0.0464, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.117157220840454; loose: 0.6366950869560242\n",
      "Test data rec score: tight: 3.3081448078155518; loose: 2.2994956970214844\n",
      "Test data average rec_m: -0.6491077542304993; average compct_m: 2.2631571292877197\n",
      "Dependency: 0.38807159662246704; Cluster: 0.18832078576087952; Local: 0.2350202202796936; Global: 0.18858742713928223;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [3:50:46, 113.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('8_celeba', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9674420261461191, 'aucpr': 0.6316211422104939}, fitting time: 143.1918182373047, inference time: 0.0073010921478271484\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 238, 'Anomalies Ratio(%)': 2.38}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 41.351710\n",
      "epoch : 1000/5000, loss = 0.240568\n",
      "epoch : 2000/5000, loss = 0.100452\n",
      "epoch : 3000/5000, loss = 0.080692\n",
      "epoch : 4000/5000, loss = 0.058344\n",
      "epoch : 5000/5000, loss = 0.053529\n",
      "self.error_median=0.0241, self.error_range=0.0361, self.latent_error_median_=0.0003, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.111989\n",
      "epoch : 1000/5000, loss = 0.020538\n",
      "epoch : 2000/5000, loss = 0.013103\n",
      "epoch : 3000/5000, loss = 0.009974\n",
      "epoch : 4000/5000, loss = 0.008541\n",
      "epoch : 5000/5000, loss = 0.007469\n",
      "self.error_median=0.0174, self.error_range=0.0450, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.8666561841964722; loose: 0.42932289838790894\n",
      "Test data rec score: tight: 2.9146597385406494; loose: 2.7674829959869385\n",
      "Test data average rec_m: -0.2449202835559845; average compct_m: 0.37266093492507935\n",
      "Dependency: 0.3898354470729828; Cluster: 0.18688713014125824; Local: 0.22643700242042542; Global: 0.19684037566184998;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [3:53:10, 122.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('8_celeba', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9779187243639371, 'aucpr': 0.7489451933727604}, fitting time: 143.29823303222656, inference time: 0.00790858268737793\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 216, 'Anomalies Ratio(%)': 2.16}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 35.358964\n",
      "epoch : 1000/5000, loss = 0.206114\n",
      "epoch : 2000/5000, loss = 0.110519\n",
      "epoch : 3000/5000, loss = 0.077153\n",
      "epoch : 4000/5000, loss = 0.076752\n",
      "epoch : 5000/5000, loss = 0.060726\n",
      "self.error_median=0.0193, self.error_range=0.0356, self.latent_error_median_=0.0003, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 78.433540\n",
      "epoch : 1000/5000, loss = 0.021512\n",
      "epoch : 2000/5000, loss = 0.014324\n",
      "epoch : 3000/5000, loss = 0.010822\n",
      "epoch : 4000/5000, loss = 0.009294\n",
      "epoch : 5000/5000, loss = 0.007814\n",
      "self.error_median=0.0166, self.error_range=0.0553, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.8986812829971313; loose: 0.4507092237472534\n",
      "Test data rec score: tight: 2.6435182094573975; loose: 1.9710530042648315\n",
      "Test data average rec_m: -0.051381491124629974; average compct_m: -0.19486191868782043\n",
      "Dependency: 0.34777480363845825; Cluster: 0.21599341928958893; Local: 0.23028238117694855; Global: 0.20594938099384308;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [3:55:33, 128.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('8_celeba', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9745406892936705, 'aucpr': 0.719764186632624}, fitting time: 143.1906111240387, inference time: 0.007990121841430664\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 230, 'Anomalies Ratio(%)': 2.3}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.944682\n",
      "epoch : 1000/5000, loss = 0.003215\n",
      "epoch : 2000/5000, loss = 0.002354\n",
      "epoch : 3000/5000, loss = 0.001985\n",
      "epoch : 4000/5000, loss = 0.002083\n",
      "epoch : 5000/5000, loss = 0.001969\n",
      "self.error_median=0.0006, self.error_range=0.0010, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 57.282781\n",
      "epoch : 1000/5000, loss = 0.000529\n",
      "epoch : 2000/5000, loss = 0.000323\n",
      "epoch : 3000/5000, loss = 0.000289\n",
      "epoch : 4000/5000, loss = 0.000275\n",
      "epoch : 5000/5000, loss = 0.000223\n",
      "self.error_median=0.0004, self.error_range=0.0014, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5803877711296082; loose: 0.7158204913139343\n",
      "Test data rec score: tight: 2.1706297397613525; loose: 1.5829490423202515\n",
      "Test data average rec_m: 0.3742295205593109; average compct_m: 0.8804415464401245\n",
      "Dependency: 0.42836135625839233; Cluster: 0.14890451729297638; Local: 0.22386369109153748; Global: 0.19887040555477142;\n",
      "Current experiment parameters: ('23_mammography', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8781293420161294, 'aucpr': 0.20873392221045145}, fitting time: 142.75451731681824, inference time: 0.008111953735351562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [3:57:56, 133.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 241, 'Anomalies Ratio(%)': 2.41}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 5.661174\n",
      "epoch : 1000/5000, loss = 0.003237\n",
      "epoch : 2000/5000, loss = 0.002550\n",
      "epoch : 3000/5000, loss = 0.001959\n",
      "epoch : 4000/5000, loss = 0.001880\n",
      "epoch : 5000/5000, loss = 0.001541\n",
      "self.error_median=0.0003, self.error_range=0.0009, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 51.170855\n",
      "epoch : 1000/5000, loss = 0.000392\n",
      "epoch : 2000/5000, loss = 0.000312\n",
      "epoch : 3000/5000, loss = 0.000284\n",
      "epoch : 4000/5000, loss = 0.000314\n",
      "epoch : 5000/5000, loss = 0.000222\n",
      "self.error_median=0.0004, self.error_range=0.0013, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7525319457054138; loose: 0.7547531723976135\n",
      "Test data rec score: tight: 4.129446029663086; loose: 3.4126224517822266\n",
      "Test data average rec_m: 0.29805290699005127; average compct_m: 0.7976536154747009\n",
      "Dependency: 0.32469531893730164; Cluster: 0.2274588942527771; Local: 0.19224226474761963; Global: 0.25560352206230164;\n",
      "Current experiment parameters: ('23_mammography', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8466767228293868, 'aucpr': 0.12563042138423783}, fitting time: 143.46340370178223, inference time: 0.007989645004272461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [4:00:20, 136.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 227, 'Anomalies Ratio(%)': 2.27}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.502247\n",
      "epoch : 1000/5000, loss = 0.003338\n",
      "epoch : 2000/5000, loss = 0.002993\n",
      "epoch : 3000/5000, loss = 0.002343\n",
      "epoch : 4000/5000, loss = 0.001574\n",
      "epoch : 5000/5000, loss = 0.001674\n",
      "self.error_median=0.0003, self.error_range=0.0010, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 90.313153\n",
      "epoch : 1000/5000, loss = 0.000551\n",
      "epoch : 2000/5000, loss = 0.000430\n",
      "epoch : 3000/5000, loss = 0.000459\n",
      "epoch : 4000/5000, loss = 0.000387\n",
      "epoch : 5000/5000, loss = 0.000311\n",
      "self.error_median=0.0006, self.error_range=0.0021, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.567520260810852; loose: 0.653394341468811\n",
      "Test data rec score: tight: 2.5509963035583496; loose: 1.6136845350265503\n",
      "Test data average rec_m: 0.10702788829803467; average compct_m: 1.0737590789794922\n",
      "Dependency: 0.2940618395805359; Cluster: 0.24585412442684174; Local: 0.1720171719789505; Global: 0.28806689381599426;\n",
      "Current experiment parameters: ('23_mammography', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8869522911483829, 'aucpr': 0.2655382720003496}, fitting time: 143.09771466255188, inference time: 0.009193897247314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [4:02:43, 138.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3548, 'Anomalies Ratio(%)': 35.48}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.664698\n",
      "epoch : 1000/5000, loss = 0.018642\n",
      "epoch : 2000/5000, loss = 0.015755\n",
      "epoch : 3000/5000, loss = 0.012335\n",
      "epoch : 4000/5000, loss = 0.012299\n",
      "epoch : 5000/5000, loss = 0.010687\n",
      "self.error_median=0.0047, self.error_range=0.0144, self.latent_error_median_=0.0003, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 48.067132\n",
      "epoch : 1000/5000, loss = 0.003188\n",
      "epoch : 2000/5000, loss = 0.002144\n",
      "epoch : 3000/5000, loss = 0.001956\n",
      "epoch : 4000/5000, loss = 0.001765\n",
      "epoch : 5000/5000, loss = 0.001732\n",
      "self.error_median=0.0083, self.error_range=0.0233, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.47014957666397095; loose: 0.3363927900791168\n",
      "Test data rec score: tight: 1.0761306285858154; loose: 0.4784387946128845\n",
      "Test data average rec_m: -0.10447452962398529; average compct_m: 0.19378790259361267\n",
      "Dependency: 0.3535500168800354; Cluster: 0.26585453748703003; Local: 0.1600414365530014; Global: 0.22055399417877197;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [4:05:12, 141.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('22_magic.gamma', 0.0, 1), model: Customized, metrics: {'aucroc': 0.856802064562232, 'aucpr': 0.6735127715048131}, fitting time: 148.64034485816956, inference time: 0.013962745666503906\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3533, 'Anomalies Ratio(%)': 35.33}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.368306\n",
      "epoch : 1000/5000, loss = 0.017512\n",
      "epoch : 2000/5000, loss = 0.015066\n",
      "epoch : 3000/5000, loss = 0.015575\n",
      "epoch : 4000/5000, loss = 0.012344\n",
      "epoch : 5000/5000, loss = 0.011907\n",
      "self.error_median=0.0063, self.error_range=0.0163, self.latent_error_median_=0.0003, self.latent_error_range_=0.0011\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 93.011890\n",
      "epoch : 1000/5000, loss = 0.002400\n",
      "epoch : 2000/5000, loss = 0.002206\n",
      "epoch : 3000/5000, loss = 0.001990\n",
      "epoch : 4000/5000, loss = 0.001987\n",
      "epoch : 5000/5000, loss = 0.001777\n",
      "self.error_median=0.0077, self.error_range=0.0251, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4597427546977997; loose: 0.6072953939437866\n",
      "Test data rec score: tight: 1.0930190086364746; loose: 0.541954517364502\n",
      "Test data average rec_m: -0.19140897691249847; average compct_m: -17.767345428466797\n",
      "Dependency: 0.36318179965019226; Cluster: 0.2538416385650635; Local: 0.18466438353061676; Global: 0.1983121931552887;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122it [4:10:15, 190.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('22_magic.gamma', 0.0, 2), model: Customized, metrics: {'aucroc': 0.8350851001750632, 'aucpr': 0.633905572138428}, fitting time: 302.8342504501343, inference time: 0.021942138671875\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3500, 'Anomalies Ratio(%)': 35.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 12.993463\n",
      "epoch : 1000/5000, loss = 0.016198\n",
      "epoch : 2000/5000, loss = 0.013818\n",
      "epoch : 3000/5000, loss = 0.012127\n",
      "epoch : 4000/5000, loss = 0.011351\n",
      "epoch : 5000/5000, loss = 0.010936\n",
      "self.error_median=0.0062, self.error_range=0.0152, self.latent_error_median_=0.0004, self.latent_error_range_=0.0011\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 86.045657\n",
      "epoch : 1000/5000, loss = 0.002387\n",
      "epoch : 2000/5000, loss = 0.001990\n",
      "epoch : 3000/5000, loss = 0.001747\n",
      "epoch : 4000/5000, loss = 0.001673\n",
      "epoch : 5000/5000, loss = 0.001554\n",
      "self.error_median=0.0063, self.error_range=0.0223, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3070397973060608; loose: 0.5657271146774292\n",
      "Test data rec score: tight: 0.922500491142273; loose: 0.5376499891281128\n",
      "Test data average rec_m: 3.702355146408081; average compct_m: -1.5239200592041016\n",
      "Dependency: 0.32561612129211426; Cluster: 0.26421210169792175; Local: 0.1694762110710144; Global: 0.2406955361366272;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [4:15:57, 235.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('22_magic.gamma', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8407833943833943, 'aucpr': 0.6555245623206285}, fitting time: 340.97939705848694, inference time: 0.03390622138977051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "124it [4:28:20, 387.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [5:02:53, 893.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [8:14:52, 4081.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: f(a) and f(b) must have different signs\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 46, 'Anomalies Ratio(%)': 0.46}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 1.878369\n",
      "epoch : 1000/5000, loss = 0.008338\n",
      "epoch : 2000/5000, loss = 0.003459\n",
      "epoch : 3000/5000, loss = 0.002960\n",
      "epoch : 4000/5000, loss = 0.004259\n",
      "epoch : 5000/5000, loss = 0.002816\n",
      "self.error_median=0.0008, self.error_range=0.0021, self.latent_error_median_=0.0000, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 50.035872\n",
      "epoch : 1000/5000, loss = 0.000995\n",
      "epoch : 2000/5000, loss = 0.000781\n",
      "epoch : 3000/5000, loss = 0.001185\n",
      "epoch : 4000/5000, loss = 0.000673\n",
      "epoch : 5000/5000, loss = 0.000478\n",
      "self.error_median=0.0013, self.error_range=0.0032, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.1427525281906128; loose: 0.44644230604171753\n",
      "Test data rec score: tight: 2.4016761779785156; loose: 1.7479887008666992\n",
      "Test data average rec_m: 0.44986188411712646; average compct_m: 1.1455100774765015\n",
      "Dependency: 0.2999558746814728; Cluster: 0.24918672442436218; Local: 0.18568241596221924; Global: 0.2651750147342682;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [8:17:13, 2899.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('16_http', 0.0, 1), model: Customized, metrics: {'aucroc': 0.648191560616209, 'aucpr': 0.014717428173591398}, fitting time: 140.11064505577087, inference time: 0.009973287582397461\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 35, 'Anomalies Ratio(%)': 0.35}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 4.899408\n",
      "epoch : 1000/5000, loss = 0.002152\n",
      "epoch : 2000/5000, loss = 0.001644\n",
      "epoch : 3000/5000, loss = 0.002233\n",
      "epoch : 4000/5000, loss = 0.001415\n",
      "epoch : 5000/5000, loss = 0.001245\n",
      "self.error_median=0.0005, self.error_range=0.0010, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 59.689772\n",
      "epoch : 1000/5000, loss = 0.000432\n",
      "epoch : 2000/5000, loss = 0.000391\n",
      "epoch : 3000/5000, loss = 0.001356\n",
      "epoch : 4000/5000, loss = 0.000358\n",
      "epoch : 5000/5000, loss = 0.000245\n",
      "self.error_median=0.0006, self.error_range=0.0010, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.49238309264183044; loose: 0.4065724313259125\n",
      "Test data rec score: tight: 1.8611652851104736; loose: 2.088527202606201\n",
      "Test data average rec_m: 0.847649335861206; average compct_m: -0.42338719964027405\n",
      "Dependency: 0.2506134510040283; Cluster: 0.292477011680603; Local: 0.21852119266986847; Global: 0.23838834464550018;\n",
      "Current experiment parameters: ('16_http', 0.0, 2), model: Customized, metrics: {'aucroc': 0.5564548494983278, 'aucpr': 0.009298725403743152}, fitting time: 143.61114931106567, inference time: 0.006921529769897461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [8:19:38, 2072.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 34, 'Anomalies Ratio(%)': 0.34}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 7.503238\n",
      "epoch : 1000/5000, loss = 0.003871\n",
      "epoch : 2000/5000, loss = 0.001800\n",
      "epoch : 3000/5000, loss = 0.002344\n",
      "epoch : 4000/5000, loss = 0.001480\n",
      "epoch : 5000/5000, loss = 0.004230\n",
      "self.error_median=0.0012, self.error_range=0.0014, self.latent_error_median_=0.0002, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 29.755884\n",
      "epoch : 1000/5000, loss = 0.000511\n",
      "epoch : 2000/5000, loss = 0.000256\n",
      "epoch : 3000/5000, loss = 0.000628\n",
      "epoch : 4000/5000, loss = 0.000294\n",
      "epoch : 5000/5000, loss = 0.000632\n",
      "self.error_median=0.0005, self.error_range=0.0009, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 1. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.43005919456481934; loose: 5.54243803024292\n",
      "Test data rec score: tight: 1.2836509943008423; loose: 1.2729631662368774\n",
      "Test data average rec_m: -3.275341033935547; average compct_m: 0.5845878720283508\n",
      "Dependency: 0.3876122236251831; Cluster: 0.13652989268302917; Local: 0.3484143912792206; Global: 0.12744350731372833;\n",
      "Current experiment parameters: ('16_http', 0.0, 3), model: Customized, metrics: {'aucroc': 0.5906688963210702, 'aucpr': 0.004428129748564027}, fitting time: 143.28319001197815, inference time: 0.00698089599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [8:22:02, 1494.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 33.668984\n",
      "epoch : 1000/5000, loss = 0.022613\n",
      "epoch : 2000/5000, loss = 0.008600\n",
      "epoch : 3000/5000, loss = 0.006462\n",
      "epoch : 4000/5000, loss = 0.005142\n",
      "epoch : 5000/5000, loss = 0.004051\n",
      "self.error_median=0.0009, self.error_range=0.0031, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 57.534054\n",
      "epoch : 1000/5000, loss = 0.001745\n",
      "epoch : 2000/5000, loss = 0.001424\n",
      "epoch : 3000/5000, loss = 0.001486\n",
      "epoch : 4000/5000, loss = 0.001002\n",
      "epoch : 5000/5000, loss = 0.000919\n",
      "self.error_median=0.0024, self.error_range=0.0092, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 14. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7357929348945618; loose: 0.5818703770637512\n",
      "Test data rec score: tight: 1.7178412675857544; loose: 0.922453761100769\n",
      "Test data average rec_m: -1.5867087841033936; average compct_m: -0.3102060556411743\n",
      "Dependency: 0.30103716254234314; Cluster: 0.2797796130180359; Local: 0.19035959243774414; Global: 0.22882358729839325;\n",
      "Current experiment parameters: ('13_fraud', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9960781041388519, 'aucpr': 0.24553571428571427}, fitting time: 143.02899742126465, inference time: 0.011036157608032227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [8:24:25, 1088.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 27.250529\n",
      "epoch : 1000/5000, loss = 0.013656\n",
      "epoch : 2000/5000, loss = 0.010513\n",
      "epoch : 3000/5000, loss = 0.007458\n",
      "epoch : 4000/5000, loss = 0.005835\n",
      "epoch : 5000/5000, loss = 0.005409\n",
      "self.error_median=0.0016, self.error_range=0.0041, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.804306\n",
      "epoch : 1000/5000, loss = 0.001546\n",
      "epoch : 2000/5000, loss = 0.001215\n",
      "epoch : 3000/5000, loss = 0.001027\n",
      "epoch : 4000/5000, loss = 0.000883\n",
      "epoch : 5000/5000, loss = 0.000766\n",
      "self.error_median=0.0021, self.error_range=0.0083, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 14. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.704172670841217; loose: 0.6157756447792053\n",
      "Test data rec score: tight: 0.8682633638381958; loose: 0.7877402305603027\n",
      "Test data average rec_m: -0.39493265748023987; average compct_m: 0.041317474097013474\n",
      "Dependency: 0.3043604791164398; Cluster: 0.2503802180290222; Local: 0.26081857085227966; Global: 0.1844407320022583;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [8:26:49, 805.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('13_fraud', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9960601001669449, 'aucpr': 0.19586979927537201}, fitting time: 142.82608699798584, inference time: 0.01021885871887207\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 13, 'Anomalies Ratio(%)': 0.13}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 30.119854\n",
      "epoch : 1000/5000, loss = 0.021112\n",
      "epoch : 2000/5000, loss = 0.014082\n",
      "epoch : 3000/5000, loss = 0.008821\n",
      "epoch : 4000/5000, loss = 0.006419\n",
      "epoch : 5000/5000, loss = 0.004918\n",
      "self.error_median=0.0012, self.error_range=0.0037, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 68.380087\n",
      "epoch : 1000/5000, loss = 0.002013\n",
      "epoch : 2000/5000, loss = 0.001540\n",
      "epoch : 3000/5000, loss = 0.001295\n",
      "epoch : 4000/5000, loss = 0.001117\n",
      "epoch : 5000/5000, loss = 0.000972\n",
      "self.error_median=0.0029, self.error_range=0.0106, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 14. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6438977718353271; loose: 0.46130892634391785\n",
      "Test data rec score: tight: 1.6934058666229248; loose: 1.0954091548919678\n",
      "Test data average rec_m: -0.45003849267959595; average compct_m: 7.954939842224121\n",
      "Dependency: 0.31745103001594543; Cluster: 0.26469552516937256; Local: 0.17557409405708313; Global: 0.24227938055992126;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [8:29:12, 606.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('13_fraud', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9941588785046729, 'aucpr': 0.21366995073891623}, fitting time: 142.72491192817688, inference time: 0.009286642074584961\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.612281\n",
      "epoch : 1000/5000, loss = 0.011641\n",
      "epoch : 2000/5000, loss = 0.010796\n",
      "epoch : 3000/5000, loss = 0.008838\n",
      "epoch : 4000/5000, loss = 0.007943\n",
      "epoch : 5000/5000, loss = 0.032476\n",
      "self.error_median=0.0075, self.error_range=0.0136, self.latent_error_median_=0.0013, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 55.088508\n",
      "epoch : 1000/5000, loss = 0.002731\n",
      "epoch : 2000/5000, loss = 0.001662\n",
      "epoch : 3000/5000, loss = 0.001031\n",
      "epoch : 4000/5000, loss = 0.001075\n",
      "epoch : 5000/5000, loss = 0.000806\n",
      "self.error_median=0.0025, self.error_range=0.0059, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3059254586696625; loose: 0.3300560712814331\n",
      "Test data rec score: tight: 0.8092483878135681; loose: 2.012345552444458\n",
      "Test data average rec_m: 0.30276915431022644; average compct_m: 1.234228491783142\n",
      "Dependency: 0.2549746036529541; Cluster: 0.29500851035118103; Local: 0.18834041059017181; Global: 0.26167649030685425;\n",
      "Current experiment parameters: ('11_donors', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8733138111468485, 'aucpr': 0.339802760560109}, fitting time: 142.73489451408386, inference time: 0.007336139678955078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [8:31:35, 467.63s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\frank.py:98: RuntimeWarning: divide by zero encountered in log\n",
      "  return -1.0 / self.theta * np.log(1 + num / den)\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\clayton.py:86: RuntimeWarning: overflow encountered in power\n",
      "  np.power(U[i], -self.theta) + np.power(V[i], -self.theta) - 1,\n",
      "134it [8:32:11, 337.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Marginal value out of bounds.\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 597, 'Anomalies Ratio(%)': 5.97}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.140190\n",
      "epoch : 1000/5000, loss = 0.011669\n",
      "epoch : 2000/5000, loss = 0.009081\n",
      "epoch : 3000/5000, loss = 0.007074\n",
      "epoch : 4000/5000, loss = 0.005363\n",
      "epoch : 5000/5000, loss = 0.005504\n",
      "self.error_median=0.0017, self.error_range=0.0031, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 70.493513\n",
      "epoch : 1000/5000, loss = 0.001659\n",
      "epoch : 2000/5000, loss = 0.001281\n",
      "epoch : 3000/5000, loss = 0.000984\n",
      "epoch : 4000/5000, loss = 0.000763\n",
      "epoch : 5000/5000, loss = 0.000735\n",
      "self.error_median=0.0025, self.error_range=0.0047, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 5. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.3167004883289337; loose: 0.3920663893222809\n",
      "Test data rec score: tight: 2.106307029724121; loose: 1.6808815002441406\n",
      "Test data average rec_m: 4.5482378005981445; average compct_m: 0.4805176556110382\n",
      "Dependency: 0.38754963874816895; Cluster: 0.1899355798959732; Local: 0.223120778799057; Global: 0.19939395785331726;\n",
      "Current experiment parameters: ('11_donors', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8796634974324649, 'aucpr': 0.42838246027768945}, fitting time: 142.30519104003906, inference time: 0.006701231002807617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [8:34:34, 279.40s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "136it [8:35:16, 208.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [8:35:44, 154.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [8:36:21, 119.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [8:37:55, 111.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n",
      "{'Samples': 10000, 'Features': 50, 'Anomalies': 628, 'Anomalies Ratio(%)': 6.28}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 28.523502\n",
      "epoch : 1000/5000, loss = 0.055236\n",
      "epoch : 2000/5000, loss = 0.040540\n",
      "epoch : 3000/5000, loss = 0.029128\n",
      "epoch : 4000/5000, loss = 0.021542\n",
      "epoch : 5000/5000, loss = 0.016877\n",
      "self.error_median=0.0027, self.error_range=0.0068, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 85.709500\n",
      "epoch : 1000/5000, loss = 0.010460\n",
      "epoch : 2000/5000, loss = 0.004793\n",
      "epoch : 3000/5000, loss = 0.004053\n",
      "epoch : 4000/5000, loss = 0.003201\n",
      "epoch : 5000/5000, loss = 0.002792\n",
      "self.error_median=0.0014, self.error_range=0.0120, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.804791808128357; loose: 1.432116150856018\n",
      "Test data rec score: tight: 6.1729207038879395; loose: 3.3234028816223145\n",
      "Test data average rec_m: -3.5082056522369385; average compct_m: -1.2128335237503052\n",
      "Dependency: 0.2569526731967926; Cluster: 0.2921867370605469; Local: 0.15422950685024261; Global: 0.2966310381889343;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [8:40:25, 122.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('9_census', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9600912502648224, 'aucpr': 0.5459169977851488}, fitting time: 147.39847660064697, inference time: 0.0069811344146728516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\multivariate\\vine.py:73: UserWarning: Vines have not been fully tested on Python >= 3.8 and might produce wrong results.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\copulas\\bivariate\\base.py:162: RuntimeWarning: Data does not appear to be uniform.\n",
      "  warnings.warn('Data does not appear to be uniform.', category=RuntimeWarning)\n",
      "141it [8:41:50, 222.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when generating data: Constant column.\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 97, 'Anomalies Ratio(%)': 0.97}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 116, 'Anomalies Ratio(%)': 1.16}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 95, 'Anomalies Ratio(%)': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 578, 'Anomalies Ratio(%)': 5.78}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 597, 'Anomalies Ratio(%)': 5.97}\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 13, 'Anomalies Ratio(%)': 0.13}\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 34, 'Anomalies Ratio(%)': 3.4}\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 170, 'Anomalies Ratio(%)': 17.0}\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 139, 'Anomalies Ratio(%)': 13.9}\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 46, 'Anomalies Ratio(%)': 0.46}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 35, 'Anomalies Ratio(%)': 0.35}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 34, 'Anomalies Ratio(%)': 0.34}\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 369, 'Anomalies Ratio(%)': 36.9}\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 374, 'Anomalies Ratio(%)': 37.4}\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 349, 'Anomalies Ratio(%)': 34.9}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 315, 'Anomalies Ratio(%)': 3.15}\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 334, 'Anomalies Ratio(%)': 3.34}\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 332, 'Anomalies Ratio(%)': 3.32}\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 38, 'Anomalies Ratio(%)': 3.8}\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 43, 'Anomalies Ratio(%)': 4.3}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3548, 'Anomalies Ratio(%)': 35.48}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3533, 'Anomalies Ratio(%)': 35.33}\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3500, 'Anomalies Ratio(%)': 35.0}\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 230, 'Anomalies Ratio(%)': 2.3}\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 241, 'Anomalies Ratio(%)': 2.41}\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 227, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 367, 'Anomalies Ratio(%)': 36.7}\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 346, 'Anomalies Ratio(%)': 34.6}\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 669, 'Anomalies Ratio(%)': 6.69}\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 697, 'Anomalies Ratio(%)': 6.97}\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 714, 'Anomalies Ratio(%)': 7.14}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2081, 'Anomalies Ratio(%)': 20.81}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2082, 'Anomalies Ratio(%)': 20.82}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2066, 'Anomalies Ratio(%)': 20.66}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 5, 'Anomalies Ratio(%)': 0.05}\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 90, 'Anomalies Ratio(%)': 9.0}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 95, 'Anomalies Ratio(%)': 9.5}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 100, 'Anomalies Ratio(%)': 10.0}\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 123, 'Anomalies Ratio(%)': 12.3}\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 133, 'Anomalies Ratio(%)': 13.3}\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 252, 'Anomalies Ratio(%)': 2.52}\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 246, 'Anomalies Ratio(%)': 2.46}\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 256, 'Anomalies Ratio(%)': 2.56}\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 35, 'Anomalies Ratio(%)': 3.5}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 45, 'Anomalies Ratio(%)': 4.5}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 48, 'Anomalies Ratio(%)': 4.8}\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 26, 'Anomalies Ratio(%)': 2.6}\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 29, 'Anomalies Ratio(%)': 2.9}\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 27, 'Anomalies Ratio(%)': 2.7}\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 75, 'Anomalies Ratio(%)': 7.5}\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 80, 'Anomalies Ratio(%)': 8.0}\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 88, 'Anomalies Ratio(%)': 8.8}\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 239, 'Anomalies Ratio(%)': 23.9}\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 224, 'Anomalies Ratio(%)': 22.4}\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 225, 'Anomalies Ratio(%)': 22.5}\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 348, 'Anomalies Ratio(%)': 34.8}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 336, 'Anomalies Ratio(%)': 33.6}\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 364, 'Anomalies Ratio(%)': 36.4}\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1134, 'Anomalies Ratio(%)': 11.34}\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1129, 'Anomalies Ratio(%)': 11.29}\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1129, 'Anomalies Ratio(%)': 11.29}\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 222, 'Anomalies Ratio(%)': 2.22}\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 238, 'Anomalies Ratio(%)': 2.38}\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 216, 'Anomalies Ratio(%)': 2.16}\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 628, 'Anomalies Ratio(%)': 6.28}\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 652, 'Anomalies Ratio(%)': 6.52}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 215, 'Anomalies Ratio(%)': 21.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 215, 'Anomalies Ratio(%)': 21.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 198, 'Anomalies Ratio(%)': 19.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 251, 'Anomalies Ratio(%)': 25.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 242, 'Anomalies Ratio(%)': 24.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 242, 'Anomalies Ratio(%)': 24.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 304, 'Anomalies Ratio(%)': 30.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 29.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 254, 'Anomalies Ratio(%)': 25.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 227, 'Anomalies Ratio(%)': 22.7}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 213, 'Anomalies Ratio(%)': 21.3}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 180, 'Anomalies Ratio(%)': 18.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 174, 'Anomalies Ratio(%)': 17.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 160, 'Anomalies Ratio(%)': 16.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 142, 'Anomalies Ratio(%)': 14.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 259, 'Anomalies Ratio(%)': 25.9}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 273, 'Anomalies Ratio(%)': 27.3}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 246, 'Anomalies Ratio(%)': 24.6}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 284, 'Anomalies Ratio(%)': 28.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 283, 'Anomalies Ratio(%)': 28.3}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 281, 'Anomalies Ratio(%)': 28.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 314, 'Anomalies Ratio(%)': 31.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 300, 'Anomalies Ratio(%)': 30.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 326, 'Anomalies Ratio(%)': 32.6}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 240, 'Anomalies Ratio(%)': 24.0}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 244, 'Anomalies Ratio(%)': 24.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 266, 'Anomalies Ratio(%)': 26.6}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 227, 'Anomalies Ratio(%)': 22.7}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 231, 'Anomalies Ratio(%)': 23.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 252, 'Anomalies Ratio(%)': 25.2}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 271, 'Anomalies Ratio(%)': 27.1}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 285, 'Anomalies Ratio(%)': 28.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 288, 'Anomalies Ratio(%)': 28.8}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 115, 'Anomalies Ratio(%)': 11.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 135, 'Anomalies Ratio(%)': 13.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 114, 'Anomalies Ratio(%)': 11.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 184, 'Anomalies Ratio(%)': 18.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 189, 'Anomalies Ratio(%)': 18.9}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 175, 'Anomalies Ratio(%)': 17.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 305, 'Anomalies Ratio(%)': 30.5}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 304, 'Anomalies Ratio(%)': 30.4}\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 320, 'Anomalies Ratio(%)': 32.0}\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 40, 'Anomalies Ratio(%)': 4.0}\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 90, 'Anomalies Ratio(%)': 9.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.673000\n",
      "epoch : 1000/5000, loss = 0.013260\n",
      "epoch : 2000/5000, loss = 0.004519\n",
      "epoch : 3000/5000, loss = 0.003594\n",
      "epoch : 4000/5000, loss = 0.002376\n",
      "epoch : 5000/5000, loss = 0.002345\n",
      "self.error_median=0.0013, self.error_range=0.0018, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.902712\n",
      "epoch : 1000/5000, loss = 0.016729\n",
      "epoch : 2000/5000, loss = 0.001178\n",
      "epoch : 3000/5000, loss = 0.001357\n",
      "epoch : 4000/5000, loss = 0.000764\n",
      "epoch : 5000/5000, loss = 0.000537\n",
      "self.error_median=0.0023, self.error_range=0.0039, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 3.49497652053833; loose: 1.287795901298523\n",
      "Test data rec score: tight: 1.6091089248657227; loose: 1.248356580734253\n",
      "Test data average rec_m: -0.1700720638036728; average compct_m: -1.821292757987976\n",
      "Dependency: 0.3479670286178589; Cluster: 0.2315456122159958; Local: 0.2648199796676636; Global: 0.15566742420196533;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:53, 113.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('37_Stamps', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9910459910459911, 'aucpr': 0.8007873257980092}, fitting time: 112.80152797698975, inference time: 0.008655548095703125\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 95, 'Anomalies Ratio(%)': 9.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.103413\n",
      "epoch : 1000/5000, loss = 0.008040\n",
      "epoch : 2000/5000, loss = 0.003240\n",
      "epoch : 3000/5000, loss = 0.002466\n",
      "epoch : 4000/5000, loss = 0.003299\n",
      "epoch : 5000/5000, loss = 0.002920\n",
      "self.error_median=0.0010, self.error_range=0.0015, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 121.641888\n",
      "epoch : 1000/5000, loss = 0.003333\n",
      "epoch : 2000/5000, loss = 0.001249\n",
      "epoch : 3000/5000, loss = 0.000831\n",
      "epoch : 4000/5000, loss = 0.001843\n",
      "epoch : 5000/5000, loss = 0.000712\n",
      "self.error_median=0.0037, self.error_range=0.0067, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 45.114479064941406; loose: 1.8545182943344116\n",
      "Test data rec score: tight: 1.525342583656311; loose: 0.8497314453125\n",
      "Test data average rec_m: -2.9819538593292236; average compct_m: -7.113178253173828\n",
      "Dependency: 0.3666441738605499; Cluster: 0.2605569064617157; Local: 0.18094108998775482; Global: 0.19185784459114075;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [03:47, 113.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('37_Stamps', 0.0, 2), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0}, fitting time: 113.54667973518372, inference time: 0.007982730865478516\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 100, 'Anomalies Ratio(%)': 10.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.533402\n",
      "epoch : 1000/5000, loss = 0.004373\n",
      "epoch : 2000/5000, loss = 0.002596\n",
      "epoch : 3000/5000, loss = 0.002790\n",
      "epoch : 4000/5000, loss = 0.002736\n",
      "epoch : 5000/5000, loss = 0.002003\n",
      "self.error_median=0.0009, self.error_range=0.0014, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 58.268312\n",
      "epoch : 1000/5000, loss = 0.001204\n",
      "epoch : 2000/5000, loss = 0.001109\n",
      "epoch : 3000/5000, loss = 0.009870\n",
      "epoch : 4000/5000, loss = 0.003336\n",
      "epoch : 5000/5000, loss = 0.000513\n",
      "self.error_median=0.0021, self.error_range=0.0034, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 5.122436046600342; loose: 1.1200543642044067\n",
      "Test data rec score: tight: 1.3730545043945312; loose: 1.599599838256836\n",
      "Test data average rec_m: -2.326575517654419; average compct_m: -0.818095326423645\n",
      "Dependency: 0.38797301054000854; Cluster: 0.22852101922035217; Local: 0.24629618227481842; Global: 0.13720978796482086;\n",
      "Current experiment parameters: ('37_Stamps', 0.0, 3), model: Customized, metrics: {'aucroc': 0.99, 'aucpr': 0.8447127946412466}, fitting time: 110.88381934165955, inference time: 0.0069806575775146484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [05:39, 113.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 26, 'Anomalies Ratio(%)': 2.6}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 25.810942\n",
      "epoch : 1000/5000, loss = 0.014660\n",
      "epoch : 2000/5000, loss = 0.013625\n",
      "epoch : 3000/5000, loss = 0.008738\n",
      "epoch : 4000/5000, loss = 0.006020\n",
      "epoch : 5000/5000, loss = 0.005564\n",
      "self.error_median=0.0029, self.error_range=0.0027, self.latent_error_median_=0.0005, self.latent_error_range_=0.0006\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 63.679232\n",
      "epoch : 1000/5000, loss = 0.004601\n",
      "epoch : 2000/5000, loss = 0.002551\n",
      "epoch : 3000/5000, loss = 0.005737\n",
      "epoch : 4000/5000, loss = 0.001668\n",
      "epoch : 5000/5000, loss = 0.001833\n",
      "self.error_median=0.0074, self.error_range=0.0086, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 15. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 2.2873246669769287; loose: 0.6280405521392822\n",
      "Test data rec score: tight: 2.6381616592407227; loose: 1.311910629272461\n",
      "Test data average rec_m: -1.9744616746902466; average compct_m: -1.409665822982788\n",
      "Dependency: 0.5568784475326538; Cluster: 0.12401270866394043; Local: 0.17579348385334015; Global: 0.14331533014774323;\n",
      "Current experiment parameters: ('43_WDBC', 0.0, 1), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0}, fitting time: 113.72402834892273, inference time: 0.007978677749633789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [07:34, 113.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 29, 'Anomalies Ratio(%)': 2.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 22.979299\n",
      "epoch : 1000/5000, loss = 0.013140\n",
      "epoch : 2000/5000, loss = 0.008512\n",
      "epoch : 3000/5000, loss = 0.006397\n",
      "epoch : 4000/5000, loss = 0.005985\n",
      "epoch : 5000/5000, loss = 0.005098\n",
      "self.error_median=0.0032, self.error_range=0.0031, self.latent_error_median_=0.0003, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.703163\n",
      "epoch : 1000/5000, loss = 0.003065\n",
      "epoch : 2000/5000, loss = 0.014066\n",
      "epoch : 3000/5000, loss = 0.001971\n",
      "epoch : 4000/5000, loss = 0.003070\n",
      "epoch : 5000/5000, loss = 0.001373\n",
      "self.error_median=0.0071, self.error_range=0.0071, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 15. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 2.9268429279327393; loose: 0.625965416431427\n",
      "Test data rec score: tight: 3.0899784564971924; loose: 1.6042073965072632\n",
      "Test data average rec_m: 0.05498659983277321; average compct_m: -8.782567024230957\n",
      "Dependency: 0.473359078168869; Cluster: 0.14901384711265564; Local: 0.20438316464424133; Global: 0.1732439398765564;\n",
      "Current experiment parameters: ('43_WDBC', 0.0, 2), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0000000000000002}, fitting time: 112.55707788467407, inference time: 0.006308078765869141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [09:27, 113.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 27, 'Anomalies Ratio(%)': 2.7}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 23.171814\n",
      "epoch : 1000/5000, loss = 0.017217\n",
      "epoch : 2000/5000, loss = 0.011557\n",
      "epoch : 3000/5000, loss = 0.011545\n",
      "epoch : 4000/5000, loss = 0.008172\n",
      "epoch : 5000/5000, loss = 0.008419\n",
      "self.error_median=0.0048, self.error_range=0.0042, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 79.394227\n",
      "epoch : 1000/5000, loss = 0.003367\n",
      "epoch : 2000/5000, loss = 0.003106\n",
      "epoch : 3000/5000, loss = 0.003288\n",
      "epoch : 4000/5000, loss = 0.003652\n",
      "epoch : 5000/5000, loss = 0.002907\n",
      "self.error_median=0.0079, self.error_range=0.0080, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 15. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.6984986066818237; loose: 0.766069233417511\n",
      "Test data rec score: tight: 1.3225393295288086; loose: 0.9901115894317627\n",
      "Test data average rec_m: 0.3416867256164551; average compct_m: 0.37001174688339233\n",
      "Dependency: 0.47255122661590576; Cluster: 0.1259746253490448; Local: 0.23942388594150543; Global: 0.16205023229122162;\n",
      "Current experiment parameters: ('43_WDBC', 0.0, 3), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0}, fitting time: 107.0992066860199, inference time: 0.005983829498291016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [11:14, 111.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 35, 'Anomalies Ratio(%)': 3.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.156190\n",
      "epoch : 1000/5000, loss = 0.013917\n",
      "epoch : 2000/5000, loss = 0.011363\n",
      "epoch : 3000/5000, loss = 0.011251\n",
      "epoch : 4000/5000, loss = 0.006983\n",
      "epoch : 5000/5000, loss = 0.005831\n",
      "self.error_median=0.0033, self.error_range=0.0045, self.latent_error_median_=0.0004, self.latent_error_range_=0.0009\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 78.111742\n",
      "epoch : 1000/5000, loss = 0.030830\n",
      "epoch : 2000/5000, loss = 0.003146\n",
      "epoch : 3000/5000, loss = 0.003079\n",
      "epoch : 4000/5000, loss = 0.002317\n",
      "epoch : 5000/5000, loss = 0.002417\n",
      "self.error_median=0.0114, self.error_range=0.0201, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.5028594136238098; loose: 0.5470301508903503\n",
      "Test data rec score: tight: 9.464886665344238; loose: 1.8433525562286377\n",
      "Test data average rec_m: -2.964731454849243; average compct_m: -1.3238348960876465\n",
      "Dependency: 0.5607496500015259; Cluster: 0.10310623794794083; Local: 0.1571456640958786; Global: 0.1789984405040741;\n",
      "Current experiment parameters: ('42_WBC', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9951724137931034, 'aucpr': 0.8029292929292928}, fitting time: 111.92751145362854, inference time: 0.0075719356536865234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [13:07, 111.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 45, 'Anomalies Ratio(%)': 4.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.882211\n",
      "epoch : 1000/5000, loss = 0.009971\n",
      "epoch : 2000/5000, loss = 0.071326\n",
      "epoch : 3000/5000, loss = 0.006051\n",
      "epoch : 4000/5000, loss = 0.006252\n",
      "epoch : 5000/5000, loss = 0.005484\n",
      "self.error_median=0.0023, self.error_range=0.0033, self.latent_error_median_=0.0004, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 121.404736\n",
      "epoch : 1000/5000, loss = 0.004176\n",
      "epoch : 2000/5000, loss = 0.005618\n",
      "epoch : 3000/5000, loss = 0.004005\n",
      "epoch : 4000/5000, loss = 0.001817\n",
      "epoch : 5000/5000, loss = 0.002248\n",
      "self.error_median=0.0093, self.error_range=0.0148, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7891159057617188; loose: 0.7704846858978271\n",
      "Test data rec score: tight: 2.4644036293029785; loose: 1.1537034511566162\n",
      "Test data average rec_m: 2.227268934249878; average compct_m: 1.0243396759033203\n",
      "Dependency: 0.4747461676597595; Cluster: 0.15645642578601837; Local: 0.19535687565803528; Global: 0.1734405755996704;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [15:00, 112.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('42_WBC', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9912587412587412, 'aucpr': 0.7124529065705536}, fitting time: 112.77786707878113, inference time: 0.006997585296630859\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 48, 'Anomalies Ratio(%)': 4.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.213793\n",
      "epoch : 1000/5000, loss = 0.013380\n",
      "epoch : 2000/5000, loss = 0.006467\n",
      "epoch : 3000/5000, loss = 0.004698\n",
      "epoch : 4000/5000, loss = 0.003304\n",
      "epoch : 5000/5000, loss = 0.002985\n",
      "self.error_median=0.0017, self.error_range=0.0025, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 72.467509\n",
      "epoch : 1000/5000, loss = 0.002864\n",
      "epoch : 2000/5000, loss = 0.001564\n",
      "epoch : 3000/5000, loss = 0.003468\n",
      "epoch : 4000/5000, loss = 0.001426\n",
      "epoch : 5000/5000, loss = 0.001201\n",
      "self.error_median=0.0046, self.error_range=0.0099, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 3.38684344291687; loose: 1.0433992147445679\n",
      "Test data rec score: tight: 1.4305710792541504; loose: 0.8439154028892517\n",
      "Test data average rec_m: -1.162124514579773; average compct_m: -0.34073638916015625\n",
      "Dependency: 0.3925022482872009; Cluster: 0.1937016397714615; Local: 0.24664820730686188; Global: 0.1671479046344757;\n",
      "Current experiment parameters: ('42_WBC', 0.0, 3), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 0.9999999999999999}, fitting time: 112.42284893989563, inference time: 0.0069811344146728516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [16:54, 112.57s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 239, 'Anomalies Ratio(%)': 23.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 24.190896\n",
      "epoch : 1000/5000, loss = 0.022075\n",
      "epoch : 2000/5000, loss = 0.016055\n",
      "epoch : 3000/5000, loss = 0.011200\n",
      "epoch : 4000/5000, loss = 0.009612\n",
      "epoch : 5000/5000, loss = 0.008016\n",
      "self.error_median=0.0035, self.error_range=0.0058, self.latent_error_median_=0.0004, self.latent_error_range_=0.0018\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 65.764706\n",
      "epoch : 1000/5000, loss = 0.004972\n",
      "epoch : 2000/5000, loss = 0.003173\n",
      "epoch : 3000/5000, loss = 0.002559\n",
      "epoch : 4000/5000, loss = 0.010569\n",
      "epoch : 5000/5000, loss = 0.001993\n",
      "self.error_median=0.0103, self.error_range=0.0125, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7075077295303345; loose: 0.4164656400680542\n",
      "Test data rec score: tight: 1.7043931484222412; loose: 2.2724785804748535\n",
      "Test data average rec_m: 1.15444016456604; average compct_m: -2.119799852371216\n",
      "Dependency: 0.4195347726345062; Cluster: 0.13148276507854462; Local: 0.2319796234369278; Global: 0.21700285375118256;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [18:48, 113.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('46_WPBC', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9540692007797271, 'aucpr': 0.7935940584756539}, fitting time: 113.87530303001404, inference time: 0.016977548599243164\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 224, 'Anomalies Ratio(%)': 22.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 26.044325\n",
      "epoch : 1000/5000, loss = 0.021776\n",
      "epoch : 2000/5000, loss = 0.018621\n",
      "epoch : 3000/5000, loss = 0.014263\n",
      "epoch : 4000/5000, loss = 0.070930\n",
      "epoch : 5000/5000, loss = 0.010437\n",
      "self.error_median=0.0047, self.error_range=0.0075, self.latent_error_median_=0.0003, self.latent_error_range_=0.0017\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 74.105915\n",
      "epoch : 1000/5000, loss = 0.005649\n",
      "epoch : 2000/5000, loss = 0.003794\n",
      "epoch : 3000/5000, loss = 0.003818\n",
      "epoch : 4000/5000, loss = 0.003156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [20:38, 112.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5000/5000, loss = 0.003155\n",
      "self.error_median=0.0122, self.error_range=0.0162, self.latent_error_median_=0.0001, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.4788237512111664; loose: 0.3899359107017517\n",
      "Test data rec score: tight: 0.8696638941764832; loose: 0.6837180256843567\n",
      "Test data average rec_m: -0.3694051206111908; average compct_m: -1.272177815437317\n",
      "Dependency: 0.4682708978652954; Cluster: 0.08846165239810944; Local: 0.3350818455219269; Global: 0.10818563401699066;\n",
      "Current experiment parameters: ('46_WPBC', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9853949138428031, 'aucpr': 0.9056496236098489}, fitting time: 109.62314105033875, inference time: 0.007219552993774414\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 225, 'Anomalies Ratio(%)': 22.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 24.182006\n",
      "epoch : 1000/5000, loss = 0.025689\n",
      "epoch : 2000/5000, loss = 0.015960\n",
      "epoch : 3000/5000, loss = 0.011372\n",
      "epoch : 4000/5000, loss = 0.010114\n",
      "epoch : 5000/5000, loss = 0.009928\n",
      "self.error_median=0.0036, self.error_range=0.0058, self.latent_error_median_=0.0002, self.latent_error_range_=0.0015\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 75.904905\n",
      "epoch : 1000/5000, loss = 0.005136\n",
      "epoch : 2000/5000, loss = 0.009332\n",
      "epoch : 3000/5000, loss = 0.002662\n",
      "epoch : 4000/5000, loss = 0.002630\n",
      "epoch : 5000/5000, loss = 0.002076\n",
      "self.error_median=0.0105, self.error_range=0.0134, self.latent_error_median_=0.0001, self.latent_error_range_=0.0007\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6839134693145752; loose: 0.442226380109787\n",
      "Test data rec score: tight: 1.5166417360305786; loose: 1.195499300956726\n",
      "Test data average rec_m: 1.629380226135254; average compct_m: -4.222928047180176\n",
      "Dependency: 0.44204846024513245; Cluster: 0.11826629936695099; Local: 0.273034930229187; Global: 0.16665033996105194;\n",
      "Current experiment parameters: ('46_WPBC', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9600659229208925, 'aucpr': 0.7796174534870837}, fitting time: 109.67237281799316, inference time: 0.006982088088989258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [22:28, 111.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 14.238332\n",
      "epoch : 1000/5000, loss = 0.017724\n",
      "epoch : 2000/5000, loss = 0.006346\n",
      "epoch : 3000/5000, loss = 0.007383\n",
      "epoch : 4000/5000, loss = 0.004237\n",
      "epoch : 5000/5000, loss = 0.004666\n",
      "self.error_median=0.0016, self.error_range=0.0016, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 96.276011\n",
      "epoch : 1000/5000, loss = 0.002969\n",
      "epoch : 2000/5000, loss = 0.003020\n",
      "epoch : 3000/5000, loss = 0.007440\n",
      "epoch : 4000/5000, loss = 0.001346\n",
      "epoch : 5000/5000, loss = 0.001283\n",
      "self.error_median=0.0060, self.error_range=0.0083, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 2.644261598587036; loose: 1.1957731246948242\n",
      "Test data rec score: tight: 7.946865558624268; loose: 2.055039405822754\n",
      "Test data average rec_m: -6.022380352020264; average compct_m: -0.598729133605957\n",
      "Dependency: 0.6449186205863953; Cluster: 0.06867833435535431; Local: 0.1337779015302658; Global: 0.1526251584291458;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [24:18, 111.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('21_Lymphography', 0.0, 1), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 0.9999999999999998}, fitting time: 109.3671624660492, inference time: 0.0075261592864990234\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 38, 'Anomalies Ratio(%)': 3.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 15.733823\n",
      "epoch : 1000/5000, loss = 0.014261\n",
      "epoch : 2000/5000, loss = 0.006767\n",
      "epoch : 3000/5000, loss = 0.005240\n",
      "epoch : 4000/5000, loss = 0.005112\n",
      "epoch : 5000/5000, loss = 0.003982\n",
      "self.error_median=0.0027, self.error_range=0.0028, self.latent_error_median_=0.0002, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 96.893818\n",
      "epoch : 1000/5000, loss = 0.003703\n",
      "epoch : 2000/5000, loss = 0.004613\n",
      "epoch : 3000/5000, loss = 0.001367\n",
      "epoch : 4000/5000, loss = 0.001672\n",
      "epoch : 5000/5000, loss = 0.001118\n",
      "self.error_median=0.0050, self.error_range=0.0070, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 4.219090461730957; loose: 1.08308744430542\n",
      "Test data rec score: tight: 2.0316615104675293; loose: 1.0480104684829712\n",
      "Test data average rec_m: 0.7092751860618591; average compct_m: 7.21948766708374\n",
      "Dependency: 0.3877372443675995; Cluster: 0.2062043994665146; Local: 0.22444108128547668; Global: 0.18161731958389282;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [26:08, 110.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('21_Lymphography', 0.0, 2), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0}, fitting time: 109.73743414878845, inference time: 0.008975505828857422\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 43, 'Anomalies Ratio(%)': 4.3}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 17.216541\n",
      "epoch : 1000/5000, loss = 0.020222\n",
      "epoch : 2000/5000, loss = 0.011227\n",
      "epoch : 3000/5000, loss = 0.005418\n",
      "epoch : 4000/5000, loss = 0.010143\n",
      "epoch : 5000/5000, loss = 0.003929\n",
      "self.error_median=0.0016, self.error_range=0.0020, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 83.069046\n",
      "epoch : 1000/5000, loss = 0.003592\n",
      "epoch : 2000/5000, loss = 0.004596\n",
      "epoch : 3000/5000, loss = 0.009333\n",
      "epoch : 4000/5000, loss = 0.001684\n",
      "epoch : 5000/5000, loss = 0.002277\n",
      "self.error_median=0.0092, self.error_range=0.0112, self.latent_error_median_=0.0002, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.0637621879577637; loose: 1.0505828857421875\n",
      "Test data rec score: tight: 5.084590911865234; loose: 4.275394916534424\n",
      "Test data average rec_m: -1.7846457958221436; average compct_m: 1.8737573623657227\n",
      "Dependency: 0.6002535223960876; Cluster: 0.07385336607694626; Local: 0.15392696857452393; Global: 0.17196615040302277;\n",
      "Current experiment parameters: ('21_Lymphography', 0.0, 3), model: Customized, metrics: {'aucroc': 0.997587778075583, 'aucpr': 0.9537087912087909}, fitting time: 109.99054002761841, inference time: 0.007063150405883789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [27:59, 110.73s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 348, 'Anomalies Ratio(%)': 34.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.971949\n",
      "epoch : 1000/5000, loss = 0.010280\n",
      "epoch : 2000/5000, loss = 0.005692\n",
      "epoch : 3000/5000, loss = 0.002682\n",
      "epoch : 4000/5000, loss = 0.002968\n",
      "epoch : 5000/5000, loss = 0.004175\n",
      "self.error_median=0.0005, self.error_range=0.0027, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 67.482676\n",
      "epoch : 1000/5000, loss = 0.001910\n",
      "epoch : 2000/5000, loss = 0.000996\n",
      "epoch : 3000/5000, loss = 0.001939\n",
      "epoch : 4000/5000, loss = 0.003616\n",
      "epoch : 5000/5000, loss = 0.001289\n",
      "self.error_median=0.0028, self.error_range=0.0070, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.6886910796165466; loose: 1.1574240922927856\n",
      "Test data rec score: tight: 6.090386867523193; loose: 3.396825075149536\n",
      "Test data average rec_m: -1.9808933734893799; average compct_m: -6.5873122215271\n",
      "Dependency: 0.5216427445411682; Cluster: 0.09526664763689041; Local: 0.13797463476657867; Global: 0.2451159656047821;\n",
      "Current experiment parameters: ('4_breastw', 0.0, 1), model: Customized, metrics: {'aucroc': 0.810782967032967, 'aucpr': 0.6797215166270206}, fitting time: 112.24800252914429, inference time: 0.008553743362426758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [29:52, 111.34s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 336, 'Anomalies Ratio(%)': 33.6}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.678147\n",
      "epoch : 1000/5000, loss = 0.013107\n",
      "epoch : 2000/5000, loss = 0.002613\n",
      "epoch : 3000/5000, loss = 0.002451\n",
      "epoch : 4000/5000, loss = 0.004603\n",
      "epoch : 5000/5000, loss = 0.003190\n",
      "self.error_median=0.0006, self.error_range=0.0019, self.latent_error_median_=0.0002, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 90.553688\n",
      "epoch : 1000/5000, loss = 0.001362\n",
      "epoch : 2000/5000, loss = 0.000867\n",
      "epoch : 3000/5000, loss = 0.000947\n",
      "epoch : 4000/5000, loss = 0.001137\n",
      "epoch : 5000/5000, loss = 0.001904\n",
      "self.error_median=0.0024, self.error_range=0.0045, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.9306137561798096; loose: 0.7094486951828003\n",
      "Test data rec score: tight: 1.6534783840179443; loose: 1.0642108917236328\n",
      "Test data average rec_m: 0.0993976891040802; average compct_m: 2.1609349250793457\n",
      "Dependency: 0.32345449924468994; Cluster: 0.21751073002815247; Local: 0.21913674473762512; Global: 0.23989807069301605;\n",
      "Current experiment parameters: ('4_breastw', 0.0, 2), model: Customized, metrics: {'aucroc': 0.76511269217374, 'aucpr': 0.5942180561141955}, fitting time: 110.47987246513367, inference time: 0.007483959197998047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [31:43, 111.24s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 364, 'Anomalies Ratio(%)': 36.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.199850\n",
      "epoch : 1000/5000, loss = 0.006110\n",
      "epoch : 2000/5000, loss = 0.005467\n",
      "epoch : 3000/5000, loss = 0.002614\n",
      "epoch : 4000/5000, loss = 0.003303\n",
      "epoch : 5000/5000, loss = 0.001890\n",
      "self.error_median=0.0005, self.error_range=0.0015, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 44.662560\n",
      "epoch : 1000/5000, loss = 0.002500\n",
      "epoch : 2000/5000, loss = 0.004312\n",
      "epoch : 3000/5000, loss = 0.000671\n",
      "epoch : 4000/5000, loss = 0.000981\n",
      "epoch : 5000/5000, loss = 0.000569\n",
      "self.error_median=0.0014, self.error_range=0.0032, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 18.9488582611084; loose: 2.072235584259033\n",
      "Test data rec score: tight: 76.08845520019531; loose: 38.31244659423828\n",
      "Test data average rec_m: 0.16058123111724854; average compct_m: -12.284911155700684\n",
      "Dependency: 0.44075319170951843; Cluster: 0.16203239560127258; Local: 0.2031421810388565; Global: 0.19407227635383606;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [33:33, 111.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('4_breastw', 0.0, 3), model: Customized, metrics: {'aucroc': 0.8494644315288917, 'aucpr': 0.6564644743528396}, fitting time: 110.32634425163269, inference time: 0.007139444351196289\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 75, 'Anomalies Ratio(%)': 7.5}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.869139\n",
      "epoch : 1000/5000, loss = 0.005341\n",
      "epoch : 2000/5000, loss = 0.003921\n",
      "epoch : 3000/5000, loss = 0.004752\n",
      "epoch : 4000/5000, loss = 0.003113\n",
      "epoch : 5000/5000, loss = 0.003280\n",
      "self.error_median=0.0019, self.error_range=0.0024, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 71.617740\n",
      "epoch : 1000/5000, loss = 0.201765\n",
      "epoch : 2000/5000, loss = 0.001452\n",
      "epoch : 3000/5000, loss = 0.000958\n",
      "epoch : 4000/5000, loss = 0.000472\n",
      "epoch : 5000/5000, loss = 0.000792\n",
      "self.error_median=0.0031, self.error_range=0.0043, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 11.344422340393066; loose: 1.3810933828353882\n",
      "Test data rec score: tight: 2.094360589981079; loose: 1.0436087846755981\n",
      "Test data average rec_m: 0.09516911953687668; average compct_m: -0.06791960448026657\n",
      "Dependency: 0.36990925669670105; Cluster: 0.23513135313987732; Local: 0.19030196964740753; Global: 0.2046574205160141;\n",
      "Current experiment parameters: ('45_wine', 0.0, 1), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 0.9999999999999999}, fitting time: 110.94992542266846, inference time: 0.009015560150146484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [35:25, 111.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 80, 'Anomalies Ratio(%)': 8.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 13.157907\n",
      "epoch : 1000/5000, loss = 0.005391\n",
      "epoch : 2000/5000, loss = 0.004123\n",
      "epoch : 3000/5000, loss = 0.003879\n",
      "epoch : 4000/5000, loss = 0.003863\n",
      "epoch : 5000/5000, loss = 0.003867\n",
      "self.error_median=0.0025, self.error_range=0.0029, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 53.692932\n",
      "epoch : 1000/5000, loss = 0.000950\n",
      "epoch : 2000/5000, loss = 0.000813\n",
      "epoch : 3000/5000, loss = 0.000677\n",
      "epoch : 4000/5000, loss = 0.000521\n",
      "epoch : 5000/5000, loss = 0.000440\n",
      "self.error_median=0.0029, self.error_range=0.0035, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 8.487625122070312; loose: 1.3802518844604492\n",
      "Test data rec score: tight: 1.120023250579834; loose: 1.8779535293579102\n",
      "Test data average rec_m: 1.4151256084442139; average compct_m: -2.2803843021392822\n",
      "Dependency: 0.3361119031906128; Cluster: 0.22973433136940002; Local: 0.2333286553621292; Global: 0.20082515478134155;\n",
      "Current experiment parameters: ('45_wine', 0.0, 2), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 0.9999999999999998}, fitting time: 112.69203853607178, inference time: 0.007940292358398438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [37:18, 111.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 88, 'Anomalies Ratio(%)': 8.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.271674\n",
      "epoch : 1000/5000, loss = 0.006668\n",
      "epoch : 2000/5000, loss = 0.005932\n",
      "epoch : 3000/5000, loss = 0.005884\n",
      "epoch : 4000/5000, loss = 0.004680\n",
      "epoch : 5000/5000, loss = 0.004388\n",
      "self.error_median=0.0025, self.error_range=0.0031, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 71.966222\n",
      "epoch : 1000/5000, loss = 0.001845\n",
      "epoch : 2000/5000, loss = 0.001289\n",
      "epoch : 3000/5000, loss = 0.000595\n",
      "epoch : 4000/5000, loss = 0.000728\n",
      "epoch : 5000/5000, loss = 0.000743\n",
      "self.error_median=0.0035, self.error_range=0.0047, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 6. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 12.422383308410645; loose: 2.650031805038452\n",
      "Test data rec score: tight: 1.6231138706207275; loose: 0.9973790645599365\n",
      "Test data average rec_m: -1.5708093643188477; average compct_m: 0.32621699571609497\n",
      "Dependency: 0.29221266508102417; Cluster: 0.2734771966934204; Local: 0.17523521184921265; Global: 0.25907495617866516;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [39:10, 111.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('45_wine', 0.0, 3), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 0.9999999999999999}, fitting time: 111.65063834190369, inference time: 0.007001638412475586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 369, 'Anomalies Ratio(%)': 36.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 29.857171\n",
      "epoch : 1000/5000, loss = 0.020421\n",
      "epoch : 2000/5000, loss = 0.011194\n",
      "epoch : 3000/5000, loss = 0.007914\n",
      "epoch : 4000/5000, loss = 0.008666\n",
      "epoch : 5000/5000, loss = 0.006488\n",
      "self.error_median=0.0046, self.error_range=0.0047, self.latent_error_median_=0.0003, self.latent_error_range_=0.0005\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 61.909509\n",
      "epoch : 1000/5000, loss = 0.003222\n",
      "epoch : 2000/5000, loss = 0.001140\n",
      "epoch : 3000/5000, loss = 0.002682\n",
      "epoch : 4000/5000, loss = 0.000921\n",
      "epoch : 5000/5000, loss = 0.001492\n",
      "self.error_median=0.0056, self.error_range=0.0061, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.2285290956497192; loose: 0.3287433981895447\n",
      "Test data rec score: tight: 2.398632764816284; loose: 4.9044294357299805\n",
      "Test data average rec_m: 0.11557171493768692; average compct_m: -0.603282630443573\n",
      "Dependency: 0.49759188294410706; Cluster: 0.06971541792154312; Local: 0.2881101667881012; Global: 0.1445825695991516;\n",
      "Current experiment parameters: ('18_Ionosphere', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9965203298536632, 'aucpr': 0.991472450985744}, fitting time: 107.22308039665222, inference time: 0.0069806575775146484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [40:58, 110.67s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 374, 'Anomalies Ratio(%)': 37.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 29.437969\n",
      "epoch : 1000/5000, loss = 0.009497\n",
      "epoch : 2000/5000, loss = 0.006317\n",
      "epoch : 3000/5000, loss = 0.006592\n",
      "epoch : 4000/5000, loss = 0.007815\n",
      "epoch : 5000/5000, loss = 0.005700\n",
      "self.error_median=0.0038, self.error_range=0.0044, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 54.804566\n",
      "epoch : 1000/5000, loss = 0.004822\n",
      "epoch : 2000/5000, loss = 0.001653\n",
      "epoch : 3000/5000, loss = 0.001297\n",
      "epoch : 4000/5000, loss = 0.001169\n",
      "epoch : 5000/5000, loss = 0.015597\n",
      "self.error_median=0.0205, self.error_range=0.0286, self.latent_error_median_=0.0012, self.latent_error_range_=0.0123\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.7272186875343323; loose: 0.25696057081222534\n",
      "Test data rec score: tight: 0.5774487257003784; loose: 0.44641369581222534\n",
      "Test data average rec_m: 1.8544498682022095; average compct_m: 0.13804049789905548\n",
      "Dependency: 0.3941095173358917; Cluster: 0.16235220432281494; Local: 0.32415491342544556; Global: 0.11938339471817017;\n",
      "Current experiment parameters: ('18_Ionosphere', 0.0, 2), model: Customized, metrics: {'aucroc': 0.91893047112462, 'aucpr': 0.8826562709792852}, fitting time: 114.73268532752991, inference time: 0.0068836212158203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [42:53, 112.02s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 349, 'Anomalies Ratio(%)': 34.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 33.132640\n",
      "epoch : 1000/5000, loss = 0.010710\n",
      "epoch : 2000/5000, loss = 0.008077\n",
      "epoch : 3000/5000, loss = 0.006491\n",
      "epoch : 4000/5000, loss = 0.006278\n",
      "epoch : 5000/5000, loss = 0.005876\n",
      "self.error_median=0.0035, self.error_range=0.0036, self.latent_error_median_=0.0003, self.latent_error_range_=0.0008\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 60.977500\n",
      "epoch : 1000/5000, loss = 0.002615\n",
      "epoch : 2000/5000, loss = 6.574495\n",
      "epoch : 3000/5000, loss = 0.001071\n",
      "epoch : 4000/5000, loss = 0.001078\n",
      "epoch : 5000/5000, loss = 0.018501\n",
      "self.error_median=0.1127, self.error_range=0.2757, self.latent_error_median_=0.0012, self.latent_error_range_=0.0429\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 16. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.42378464341163635; loose: 0.24803376197814941\n",
      "Test data rec score: tight: 1.0569238662719727; loose: 0.6570083498954773\n",
      "Test data average rec_m: -15.402064323425293; average compct_m: 0.22553928196430206\n",
      "Dependency: 0.39634546637535095; Cluster: 0.16961486637592316; Local: 0.2454458475112915; Global: 0.18859383463859558;\n",
      "Current experiment parameters: ('18_Ionosphere', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9917460317460318, 'aucpr': 0.9809490737200623}, fitting time: 111.68799018859863, inference time: 0.007088899612426758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [44:45, 112.07s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 367, 'Anomalies Ratio(%)': 36.7}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.060501\n",
      "epoch : 1000/5000, loss = 0.006472\n",
      "epoch : 2000/5000, loss = 0.004055\n",
      "epoch : 3000/5000, loss = 0.004798\n",
      "epoch : 4000/5000, loss = 0.003410\n",
      "epoch : 5000/5000, loss = 0.003620\n",
      "self.error_median=0.0021, self.error_range=0.0029, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 54.136011\n",
      "epoch : 1000/5000, loss = 0.001127\n",
      "epoch : 2000/5000, loss = 0.001066\n",
      "epoch : 3000/5000, loss = 0.000845\n",
      "epoch : 4000/5000, loss = 0.001497\n",
      "epoch : 5000/5000, loss = 0.000647\n",
      "self.error_median=0.0038, self.error_range=0.0055, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.38463646173477173; loose: 0.38211914896965027\n",
      "Test data rec score: tight: 0.8292996883392334; loose: 1.3352030515670776\n",
      "Test data average rec_m: -0.2798122763633728; average compct_m: 0.21599967777729034\n",
      "Dependency: 0.4333520531654358; Cluster: 0.12915048003196716; Local: 0.3069690465927124; Global: 0.13052843511104584;\n",
      "Current experiment parameters: ('29_Pima', 0.0, 1), model: Customized, metrics: {'aucroc': 0.8664593301435407, 'aucpr': 0.712508415534657}, fitting time: 110.65514421463013, inference time: 0.0069732666015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [46:37, 111.81s/it]d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 346, 'Anomalies Ratio(%)': 34.6}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.807474\n",
      "epoch : 1000/5000, loss = 0.004430\n",
      "epoch : 2000/5000, loss = 0.003801\n",
      "epoch : 3000/5000, loss = 0.002837\n",
      "epoch : 4000/5000, loss = 0.002840\n",
      "epoch : 5000/5000, loss = 0.002420\n",
      "self.error_median=0.0014, self.error_range=0.0023, self.latent_error_median_=0.0001, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 75.834945\n",
      "epoch : 1000/5000, loss = 0.000945\n",
      "epoch : 2000/5000, loss = 0.000935\n",
      "epoch : 3000/5000, loss = 0.000738\n",
      "epoch : 4000/5000, loss = 0.000681\n",
      "epoch : 5000/5000, loss = 0.000796\n",
      "self.error_median=0.0032, self.error_range=0.0044, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.39234527945518494; loose: 0.4357967674732208\n",
      "Test data rec score: tight: 1.7026519775390625; loose: 1.7283408641815186\n",
      "Test data average rec_m: -0.6946002244949341; average compct_m: 0.16643904149532318\n",
      "Dependency: 0.4380258321762085; Cluster: 0.13666923344135284; Local: 0.2519766390323639; Global: 0.17332834005355835;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [48:26, 110.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('29_Pima', 0.0, 2), model: Customized, metrics: {'aucroc': 0.816081240188383, 'aucpr': 0.6444284432843506}, fitting time: 108.42286372184753, inference time: 0.006477832794189453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "d:\\conda\\venv\\adbench\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 8.486900\n",
      "epoch : 1000/5000, loss = 0.004481\n",
      "epoch : 2000/5000, loss = 0.003805\n",
      "epoch : 3000/5000, loss = 0.003589\n",
      "epoch : 4000/5000, loss = 0.003549\n",
      "epoch : 5000/5000, loss = 0.003224\n",
      "self.error_median=0.0017, self.error_range=0.0025, self.latent_error_median_=0.0001, self.latent_error_range_=0.0004\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 50.982374\n",
      "epoch : 1000/5000, loss = 0.001236\n",
      "epoch : 2000/5000, loss = 0.000723\n",
      "epoch : 3000/5000, loss = 0.000686\n",
      "epoch : 4000/5000, loss = 0.000579\n",
      "epoch : 5000/5000, loss = 0.001295\n",
      "self.error_median=0.0040, self.error_range=0.0081, self.latent_error_median_=0.0001, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 4. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 0.8836315274238586; loose: 0.4850563704967499\n",
      "Test data rec score: tight: 1.1867331266403198; loose: 0.8971837759017944\n",
      "Test data average rec_m: -1.7517547607421875; average compct_m: 0.2928008735179901\n",
      "Dependency: 0.441611647605896; Cluster: 0.14661970734596252; Local: 0.2871849238872528; Global: 0.12458376586437225;\n",
      "Current experiment parameters: ('29_Pima', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9743415551839465, 'aucpr': 0.9433796252657304}, fitting time: 114.38707733154297, inference time: 0.007106781005859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [50:21, 112.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 123, 'Anomalies Ratio(%)': 12.3}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.711007\n",
      "epoch : 1000/5000, loss = 0.004041\n",
      "epoch : 2000/5000, loss = 0.001545\n",
      "epoch : 3000/5000, loss = 0.001828\n",
      "epoch : 4000/5000, loss = 0.001510\n",
      "epoch : 5000/5000, loss = 0.001367\n",
      "self.error_median=0.0006, self.error_range=0.0011, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 99.683162\n",
      "epoch : 1000/5000, loss = 0.004255\n",
      "epoch : 2000/5000, loss = 0.000916\n",
      "epoch : 3000/5000, loss = 0.000566\n",
      "epoch : 4000/5000, loss = 0.000395\n",
      "epoch : 5000/5000, loss = 0.000537\n",
      "self.error_median=0.0011, self.error_range=0.0026, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 5.38210916519165; loose: 2.239030361175537\n",
      "Test data rec score: tight: 0.5450472831726074; loose: 0.48229092359542847\n",
      "Test data average rec_m: 0.10024453699588776; average compct_m: 0.036596592515707016\n",
      "Dependency: 0.2986414432525635; Cluster: 0.29763782024383545; Local: 0.20431768894195557; Global: 0.1994030624628067;\n",
      "Current experiment parameters: ('39_vertebral', 0.0, 1), model: Customized, metrics: {'aucroc': 0.9940396670434694, 'aucpr': 0.9284013629746959}, fitting time: 112.60517811775208, inference time: 0.0074498653411865234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [52:14, 112.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 9.267847\n",
      "epoch : 1000/5000, loss = 0.014028\n",
      "epoch : 2000/5000, loss = 0.002542\n",
      "epoch : 3000/5000, loss = 0.001756\n",
      "epoch : 4000/5000, loss = 0.002070\n",
      "epoch : 5000/5000, loss = 0.001639\n",
      "self.error_median=0.0006, self.error_range=0.0012, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 66.497339\n",
      "epoch : 1000/5000, loss = 0.000947\n",
      "epoch : 2000/5000, loss = 0.000936\n",
      "epoch : 3000/5000, loss = 0.000337\n",
      "epoch : 4000/5000, loss = 0.000273\n",
      "epoch : 5000/5000, loss = 0.000463\n",
      "self.error_median=0.0012, self.error_range=0.0023, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 4.089017868041992; loose: 2.5840322971343994\n",
      "Test data rec score: tight: 0.5917927026748657; loose: 0.5961071848869324\n",
      "Test data average rec_m: 0.12321047484874725; average compct_m: -1.769810676574707\n",
      "Dependency: 0.28523215651512146; Cluster: 0.28520527482032776; Local: 0.2227485179901123; Global: 0.20681411027908325;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [54:04, 111.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('39_vertebral', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9975515585271683, 'aucpr': 0.9769264923556258}, fitting time: 109.59713554382324, inference time: 0.007951974868774414\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 133, 'Anomalies Ratio(%)': 13.3}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 6.685866\n",
      "epoch : 1000/5000, loss = 0.004146\n",
      "epoch : 2000/5000, loss = 0.004072\n",
      "epoch : 3000/5000, loss = 0.001720\n",
      "epoch : 4000/5000, loss = 0.001489\n",
      "epoch : 5000/5000, loss = 0.001564\n",
      "self.error_median=0.0006, self.error_range=0.0014, self.latent_error_median_=0.0000, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 87.825808\n",
      "epoch : 1000/5000, loss = 0.006781\n",
      "epoch : 2000/5000, loss = 0.000847\n",
      "epoch : 3000/5000, loss = 0.000480\n",
      "epoch : 4000/5000, loss = 0.000345\n",
      "epoch : 5000/5000, loss = 0.000303\n",
      "self.error_median=0.0014, self.error_range=0.0021, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.861051082611084; loose: 1.4579651355743408\n",
      "Test data rec score: tight: 2.5920212268829346; loose: 5.551793575286865\n",
      "Test data average rec_m: -0.8568815588951111; average compct_m: 0.11233698576688766\n",
      "Dependency: 0.4300724267959595; Cluster: 0.1566573679447174; Local: 0.24061860144138336; Global: 0.17265164852142334;\n",
      "Current experiment parameters: ('39_vertebral', 0.0, 3), model: Customized, metrics: {'aucroc': 0.9770192307692308, 'aucpr': 0.7636572578248408}, fitting time: 110.77898836135864, inference time: 0.00698089599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [55:56, 111.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 170, 'Anomalies Ratio(%)': 17.0}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 16.233823\n",
      "epoch : 1000/5000, loss = 0.011541\n",
      "epoch : 2000/5000, loss = 0.008111\n",
      "epoch : 3000/5000, loss = 0.009486\n",
      "epoch : 4000/5000, loss = 0.007194\n",
      "epoch : 5000/5000, loss = 0.005408\n",
      "self.error_median=0.0034, self.error_range=0.0046, self.latent_error_median_=0.0001, self.latent_error_range_=0.0001\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 78.080875\n",
      "epoch : 1000/5000, loss = 0.009709\n",
      "epoch : 2000/5000, loss = 0.001738\n",
      "epoch : 3000/5000, loss = 0.001859\n",
      "epoch : 4000/5000, loss = 0.002716\n",
      "epoch : 5000/5000, loss = 0.001473\n",
      "self.error_median=0.0061, self.error_range=0.0087, self.latent_error_median_=0.0001, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 1.9318199157714844; loose: 2.5147509574890137\n",
      "Test data rec score: tight: 1.0463939905166626; loose: 0.8855245113372803\n",
      "Test data average rec_m: 9.264404296875; average compct_m: 5.733583927154541\n",
      "Dependency: 0.287482351064682; Cluster: 0.2714528441429138; Local: 0.22856061160564423; Global: 0.21250419318675995;\n",
      "Current experiment parameters: ('15_Hepatitis', 0.0, 1), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0000000000000002}, fitting time: 114.2975447177887, inference time: 0.006850242614746094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [57:50, 112.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 139, 'Anomalies Ratio(%)': 13.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 18.839216\n",
      "epoch : 1000/5000, loss = 0.011498\n",
      "epoch : 2000/5000, loss = 0.007991\n",
      "epoch : 3000/5000, loss = 0.006811\n",
      "epoch : 4000/5000, loss = 0.006054\n",
      "epoch : 5000/5000, loss = 0.004858\n",
      "self.error_median=0.0025, self.error_range=0.0035, self.latent_error_median_=0.0002, self.latent_error_range_=0.0003\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 85.527144\n",
      "epoch : 1000/5000, loss = 0.004997\n",
      "epoch : 2000/5000, loss = 0.002049\n",
      "epoch : 3000/5000, loss = 0.001846\n",
      "epoch : 4000/5000, loss = 0.001521\n",
      "epoch : 5000/5000, loss = 0.001590\n",
      "self.error_median=0.0079, self.error_range=0.0107, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 5.254735469818115; loose: 0.8480019569396973\n",
      "Test data rec score: tight: 1.2913535833358765; loose: 0.5306434035301208\n",
      "Test data average rec_m: -1.547621250152588; average compct_m: -0.32395967841148376\n",
      "Dependency: 0.36398255825042725; Cluster: 0.2588018774986267; Local: 0.21687805652618408; Global: 0.16033750772476196;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [59:43, 112.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('15_Hepatitis', 0.0, 2), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0}, fitting time: 112.34047961235046, inference time: 0.007349967956542969\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 16.563568\n",
      "epoch : 1000/5000, loss = 0.013889\n",
      "epoch : 2000/5000, loss = 0.008270\n",
      "epoch : 3000/5000, loss = 0.007215\n",
      "epoch : 4000/5000, loss = 0.006485\n",
      "epoch : 5000/5000, loss = 0.005345\n",
      "self.error_median=0.0039, self.error_range=0.0052, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 88.225410\n",
      "epoch : 1000/5000, loss = 0.002220\n",
      "epoch : 2000/5000, loss = 0.003334\n",
      "epoch : 3000/5000, loss = 0.003326\n",
      "epoch : 4000/5000, loss = 0.001432\n",
      "epoch : 5000/5000, loss = 0.001285\n",
      "self.error_median=0.0109, self.error_range=0.0136, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 9. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 14.50283432006836; loose: 4.6609344482421875\n",
      "Test data rec score: tight: 0.8103322386741638; loose: 0.4115385115146637\n",
      "Test data average rec_m: 6.763116359710693; average compct_m: -6.233142852783203\n",
      "Dependency: 0.3048742711544037; Cluster: 0.3094048798084259; Local: 0.22609136998653412; Global: 0.15962952375411987;\n",
      "Current experiment parameters: ('15_Hepatitis', 0.0, 3), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 1.0000000000000002}, fitting time: 113.92220449447632, inference time: 0.0069806575775146484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [1:01:38, 113.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 13.649970\n",
      "epoch : 1000/5000, loss = 0.005122\n",
      "epoch : 2000/5000, loss = 0.001637\n",
      "epoch : 3000/5000, loss = 0.003412\n",
      "epoch : 4000/5000, loss = 0.001211\n",
      "epoch : 5000/5000, loss = 0.000955\n",
      "self.error_median=0.0004, self.error_range=0.0006, self.latent_error_median_=0.0000, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 20.230012\n",
      "epoch : 1000/5000, loss = 0.011074\n",
      "epoch : 2000/5000, loss = 0.002529\n",
      "epoch : 3000/5000, loss = 0.001736\n",
      "epoch : 4000/5000, loss = 0.001811\n",
      "epoch : 5000/5000, loss = 0.000777\n",
      "self.error_median=0.0018, self.error_range=0.0053, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 42.19388198852539; loose: 0.8406901359558105\n",
      "Test data rec score: tight: 41.06072998046875; loose: 5.027163982391357\n",
      "Test data average rec_m: -5.044990539550781; average compct_m: -4.790168762207031\n",
      "Dependency: 0.4485433101654053; Cluster: 0.1703692376613617; Local: 0.11909952759742737; Global: 0.26198792457580566;\n",
      "Current experiment parameters: ('14_glass', 0.0, 1), model: Customized, metrics: {'aucroc': 1.0, 'aucpr': 0.9999999999999998}, fitting time: 112.46890044212341, inference time: 0.006981849670410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [1:03:30, 113.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 11.174250\n",
      "epoch : 1000/5000, loss = 0.002720\n",
      "epoch : 2000/5000, loss = 0.004353\n",
      "epoch : 3000/5000, loss = 0.001313\n",
      "epoch : 4000/5000, loss = 0.001677\n",
      "epoch : 5000/5000, loss = 0.001159\n",
      "self.error_median=0.0003, self.error_range=0.0006, self.latent_error_median_=0.0000, self.latent_error_range_=0.0002\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/5000, loss = 93.198975\n",
      "epoch : 1000/5000, loss = 0.057899\n",
      "epoch : 2000/5000, loss = 0.002161\n",
      "epoch : 3000/5000, loss = 0.007652\n",
      "epoch : 4000/5000, loss = 0.000970\n",
      "epoch : 5000/5000, loss = 0.000500\n",
      "self.error_median=0.0009, self.error_range=0.0020, self.latent_error_median_=0.0000, self.latent_error_range_=0.0000\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 3. Weight Decay = 0.000001\n",
      "Test data dev score: tight: 3.1504573822021484; loose: 0.8084218502044678\n",
      "Test data rec score: tight: 2.3622031211853027; loose: 1.4179075956344604\n",
      "Test data average rec_m: 0.6582807302474976; average compct_m: 0.2945477366447449\n",
      "Dependency: 0.3520849645137787; Cluster: 0.22402791678905487; Local: 0.26086094975471497; Global: 0.16302619874477386;\n",
      "Current experiment parameters: ('14_glass', 0.0, 2), model: Customized, metrics: {'aucroc': 0.9951755561511658, 'aucpr': 0.7994238667315591}, fitting time: 111.51424932479858, inference time: 0.009529590606689453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [1:05:22, 112.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 34, 'Anomalies Ratio(%)': 3.4}\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/5000, loss = 10.032185\n",
      "epoch : 1000/5000, loss = 0.002902\n",
      "epoch : 2000/5000, loss = 0.003379\n",
      "epoch : 3000/5000, loss = 0.002044\n",
      "epoch : 4000/5000, loss = 0.001494\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Params:\n",
    "\n",
    "parallel: running either 'unsupervise', 'semi-supervise', or 'supervise' (AD) algorithms,\n",
    "\n",
    "realistic_synthetic_mode: testing on 'local', 'global', 'dependency', and 'cluster' anomalies, \n",
    "\n",
    "noise type: evaluating algorithms on 'duplicated_anomalies', 'irrelevant_features' and 'label_contamination',\n",
    "'''\n",
    "model_to_test = CVStack\n",
    "for mode in [\n",
    "                #None,\n",
    "                'dependency',\n",
    "                'cluster',\n",
    "                None,\n",
    "                'local',\n",
    "                'global',\n",
    "                #'dependency',\n",
    "            ]:\n",
    "    pipeline = RunPipeline(suffix=model_to_test.name, parallel='unsupervise', realistic_synthetic_mode=mode, noise_type=None, num_seed=3)\n",
    "    # 'duplicated_anomalies', 'irrelevant_features' and 'label_contamination'\n",
    "    results = pipeline.run(clf=model_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\n",
    "                #None,\n",
    "                'dependency',\n",
    "                'cluster',\n",
    "                None,\n",
    "                'local',\n",
    "                'global',\n",
    "            ]:\n",
    "    pipeline = RunPipeline(suffix='ADBench', parallel='unsupervise', realistic_synthetic_mode=mode, noise_type=None, num_seed=3)\n",
    "    #results = pipeline.run(clf=VAE)\n",
    "    results = pipeline.run(clf=SimpleAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\n",
    "                'dependency',\n",
    "                'cluster',\n",
    "                None,\n",
    "                'local',\n",
    "                'global',\n",
    "            ]:\n",
    "    pipeline = RunPipeline(suffix='ADBench', parallel='unsupervise', realistic_synthetic_mode=mode, noise_type=None, num_seed=3)\n",
    "    #results = pipeline.run(clf=VAE)\n",
    "    results = pipeline.run(clf=DOCAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your customized algorithm on customized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adbench.baseline.Bagging.run import Bagging\n",
    "from adbench.baseline.AADOCAE.run import AADOCAE\n",
    "# customized model on customized dataset\n",
    "import numpy as np\n",
    "dataset = {}\n",
    "dataset['X'] = np.random.randn(1000, 20)\n",
    "dataset['y'] = np.random.choice([0, 1], 1000)\n",
    "#print(dataset['y'])\n",
    "RunPipeline(suffix='ADBench', parallel='unsupervise', realistic_synthetic_mode=None, noise_type=None, num_seed=1)\n",
    "results = pipeline.run(dataset=dataset, clf=CVStack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import AD algorithms from ADBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN normalization layer:  BatchNorm1d\n",
      "ANN normalization layer:  BatchNorm1d\n",
      "epoch : 1/50, loss = 7.901216\n",
      "self.error_median=0.0081, self.error_range=0.0116, self.latent_error_median_=0.0332, self.latent_error_range_=0.0502\n",
      "Model result scaling scheme:  True\n",
      "epoch : 1/50, loss = 4.630847\n",
      "self.error_median=0.1354, self.error_range=0.1726, self.latent_error_median_=0.0023, self.latent_error_range_=0.0062\n",
      "Model result scaling scheme:  True\n",
      "Latent dim= 2. Weight Decay = 0.000001\n",
      "torch.Size([100])\n",
      "torch.Size([100, 2])\n",
      "Test data dev score: tight: 2.1175153255462646; loose: 0.6049145460128784\n",
      "Test data rec score: tight: 325.9106750488281; loose: 23.763961791992188\n",
      "Test data average rec_m: -0.4002861976623535; average compct_m: 0.6744373440742493\n",
      "Dependency: 0.9693173766136169; Cluster: 0.001600173069164157; Local: 0.008372575975954533; Global: 0.02070985734462738;\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "from adbench.baseline.CVStack.run import CVStack\n",
    "from adbench.baseline.SS.run import SS\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.random.randn(1000, 1)\n",
    "X_train = np.concatenate((X_train, X_train, X_train, X_train), axis=-1)\n",
    "y_train = np.random.choice([0, 1], 1000)\n",
    "X_test = np.random.randn(100, 4)\n",
    "y_test = np.random.choice([0, 1], 100)\n",
    "\n",
    "# Directly import AD algorithms from the existing toolkits like PyOD\n",
    "#from adbench.baseline.PyOD import PYOD\n",
    "#model = PYOD(seed=42, model_name='XGBOD')  # initialization\n",
    "model = CVStack(seed=42)\n",
    "model.fit(X_train, y_train)  # fit\n",
    "score = model.predict_score(X_test)  # predict\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list pyod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
